{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DJIA stock project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDUQkKf_jq-p"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8X7Wvabnrfg"
      },
      "source": [
        "df_RedditNews = pd.read_csv('https://raw.githubusercontent.com/Eliot100/DJIA-stock-project/main/RedditNews.csv')\n",
        "df_DJIA = pd.read_csv('https://raw.githubusercontent.com/Eliot100/DJIA-stock-project/main/upload_DJIA_table.csv')\n",
        "df_Combined_News_DJIA = pd.read_csv('https://raw.githubusercontent.com/Eliot100/DJIA-stock-project/main/Combined_News_DJIA.csv')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "08KB3HKENk3T",
        "outputId": "629d37ad-e683-4e59-e874-3a3f76791b71"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "\n",
        "df_DJIA2 = df_DJIA.copy()\n",
        "df_DJIA2[['Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']] = \\\n",
        "  scaler.fit_transform(df_DJIA2[['Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']])\n",
        "  \n",
        "df_DJIA2.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Adj Close</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2016-07-01</td>\n",
              "      <td>0.933579</td>\n",
              "      <td>0.940047</td>\n",
              "      <td>0.939734</td>\n",
              "      <td>0.938290</td>\n",
              "      <td>-0.778698</td>\n",
              "      <td>0.938290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2016-06-30</td>\n",
              "      <td>0.897638</td>\n",
              "      <td>0.927717</td>\n",
              "      <td>0.904977</td>\n",
              "      <td>0.934995</td>\n",
              "      <td>-0.626052</td>\n",
              "      <td>0.934995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2016-06-29</td>\n",
              "      <td>0.854005</td>\n",
              "      <td>0.888874</td>\n",
              "      <td>0.861634</td>\n",
              "      <td>0.894995</td>\n",
              "      <td>-0.706021</td>\n",
              "      <td>0.894995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2016-06-28</td>\n",
              "      <td>0.808881</td>\n",
              "      <td>0.838231</td>\n",
              "      <td>0.816642</td>\n",
              "      <td>0.846554</td>\n",
              "      <td>-0.688587</td>\n",
              "      <td>0.846554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2016-06-27</td>\n",
              "      <td>0.836872</td>\n",
              "      <td>0.828866</td>\n",
              "      <td>0.795049</td>\n",
              "      <td>0.800745</td>\n",
              "      <td>-0.608918</td>\n",
              "      <td>0.800745</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Date      Open      High       Low     Close    Volume  Adj Close\n",
              "0  2016-07-01  0.933579  0.940047  0.939734  0.938290 -0.778698   0.938290\n",
              "1  2016-06-30  0.897638  0.927717  0.904977  0.934995 -0.626052   0.934995\n",
              "2  2016-06-29  0.854005  0.888874  0.861634  0.894995 -0.706021   0.894995\n",
              "3  2016-06-28  0.808881  0.838231  0.816642  0.846554 -0.688587   0.846554\n",
              "4  2016-06-27  0.836872  0.828866  0.795049  0.800745 -0.608918   0.800745"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXldyEiwMHFQ",
        "outputId": "26b9dbf2-644f-4b6a-c756-6b1ec851c5a9"
      },
      "source": [
        "df_Combined = df_DJIA2.copy()\n",
        "for i in range(0,59):\n",
        "  df_Combined[str(i+1)+\" day before Open\"] = \"\"\n",
        "  df_Combined[str(i+1)+\" day before High\"] = \"\"\n",
        "  df_Combined[str(i+1)+\" day before Low\"] = \"\"\n",
        "  df_Combined[str(i+1)+\" day before Close\"] = \"\"\n",
        "  df_Combined[str(i+1)+\" day before Volume\"] = \"\"\n",
        "  df_Combined[str(i+1)+\" day before Adj Close\"] = \"\"\n",
        "\n",
        "for i in range(0,25):\n",
        "  df_Combined[\"Top\"+str(i+1)] = \"\"\n",
        "\n",
        "for j in range(0, df_DJIA2.shape[0]-59):\n",
        "  for i in range(0, 59):\n",
        "    df_Combined[str(i+1)+\" day before Open\"][j] = df_Combined[\"Open\"][j+i+1]\n",
        "    df_Combined[str(i+1)+\" day before High\"][j] = df_Combined[\"High\"][j+i+1]\n",
        "    df_Combined[str(i+1)+\" day before Low\"][j] = df_Combined[\"Low\"][j+i+1]\n",
        "    df_Combined[str(i+1)+\" day before Close\"][j] = df_Combined[\"Close\"][j+i+1]\n",
        "    df_Combined[str(i+1)+\" day before Volume\"][j] = df_Combined[\"Volume\"][j+i+1]\n",
        "    df_Combined[str(i+1)+\" day before Adj Close\"][j] = df_Combined[\"Adj Close\"][j+i+1]\n",
        "\n",
        "for i in range(0,25):\n",
        "  df_Combined[\"Top\"+str(i+1)] = \"\"\n",
        "\n",
        "for i in range(0, df_DJIA2.shape[0]):\n",
        "  News_Date_array = df_RedditNews[df_RedditNews[\"Date\"] == df_Combined[\"Date\"][i]][\"News\"].to_numpy()\n",
        "  for j in range(0, News_Date_array.shape[0]):\n",
        "    df_Combined[\"Top\"+str(j+1)][i] = News_Date_array[j]\n",
        "\n",
        "df_Combined = df_Combined[:-59]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "Te1WmkTyv64J",
        "outputId": "cc8e8c46-ecbf-456d-ae60-7d3b2d990f6e"
      },
      "source": [
        "df_Combined.head(2)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>1 day before Open</th>\n",
              "      <th>1 day before High</th>\n",
              "      <th>1 day before Low</th>\n",
              "      <th>1 day before Close</th>\n",
              "      <th>1 day before Volume</th>\n",
              "      <th>1 day before Adj Close</th>\n",
              "      <th>2 day before Open</th>\n",
              "      <th>2 day before High</th>\n",
              "      <th>2 day before Low</th>\n",
              "      <th>2 day before Close</th>\n",
              "      <th>2 day before Volume</th>\n",
              "      <th>2 day before Adj Close</th>\n",
              "      <th>3 day before Open</th>\n",
              "      <th>3 day before High</th>\n",
              "      <th>3 day before Low</th>\n",
              "      <th>3 day before Close</th>\n",
              "      <th>3 day before Volume</th>\n",
              "      <th>3 day before Adj Close</th>\n",
              "      <th>4 day before Open</th>\n",
              "      <th>4 day before High</th>\n",
              "      <th>4 day before Low</th>\n",
              "      <th>4 day before Close</th>\n",
              "      <th>4 day before Volume</th>\n",
              "      <th>4 day before Adj Close</th>\n",
              "      <th>5 day before Open</th>\n",
              "      <th>5 day before High</th>\n",
              "      <th>5 day before Low</th>\n",
              "      <th>5 day before Close</th>\n",
              "      <th>5 day before Volume</th>\n",
              "      <th>5 day before Adj Close</th>\n",
              "      <th>6 day before Open</th>\n",
              "      <th>6 day before High</th>\n",
              "      <th>6 day before Low</th>\n",
              "      <th>...</th>\n",
              "      <th>57 day before Close</th>\n",
              "      <th>57 day before Volume</th>\n",
              "      <th>57 day before Adj Close</th>\n",
              "      <th>58 day before Open</th>\n",
              "      <th>58 day before High</th>\n",
              "      <th>58 day before Low</th>\n",
              "      <th>58 day before Close</th>\n",
              "      <th>58 day before Volume</th>\n",
              "      <th>58 day before Adj Close</th>\n",
              "      <th>59 day before Open</th>\n",
              "      <th>59 day before High</th>\n",
              "      <th>59 day before Low</th>\n",
              "      <th>59 day before Close</th>\n",
              "      <th>59 day before Volume</th>\n",
              "      <th>59 day before Adj Close</th>\n",
              "      <th>Top1</th>\n",
              "      <th>Top2</th>\n",
              "      <th>Top3</th>\n",
              "      <th>Top4</th>\n",
              "      <th>Top5</th>\n",
              "      <th>Top6</th>\n",
              "      <th>Top7</th>\n",
              "      <th>Top8</th>\n",
              "      <th>Top9</th>\n",
              "      <th>Top10</th>\n",
              "      <th>Top11</th>\n",
              "      <th>Top12</th>\n",
              "      <th>Top13</th>\n",
              "      <th>Top14</th>\n",
              "      <th>Top15</th>\n",
              "      <th>Top16</th>\n",
              "      <th>Top17</th>\n",
              "      <th>Top18</th>\n",
              "      <th>Top19</th>\n",
              "      <th>Top20</th>\n",
              "      <th>Top21</th>\n",
              "      <th>Top22</th>\n",
              "      <th>Top23</th>\n",
              "      <th>Top24</th>\n",
              "      <th>Top25</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2016-07-01</td>\n",
              "      <td>0.933579</td>\n",
              "      <td>0.940047</td>\n",
              "      <td>0.939734</td>\n",
              "      <td>0.938290</td>\n",
              "      <td>-0.778698</td>\n",
              "      <td>0.938290</td>\n",
              "      <td>0.897638</td>\n",
              "      <td>0.927717</td>\n",
              "      <td>0.904977</td>\n",
              "      <td>0.934995</td>\n",
              "      <td>-0.626052</td>\n",
              "      <td>0.934995</td>\n",
              "      <td>0.854005</td>\n",
              "      <td>0.888874</td>\n",
              "      <td>0.861634</td>\n",
              "      <td>0.894995</td>\n",
              "      <td>-0.706021</td>\n",
              "      <td>0.894995</td>\n",
              "      <td>0.808881</td>\n",
              "      <td>0.838231</td>\n",
              "      <td>0.816642</td>\n",
              "      <td>0.846554</td>\n",
              "      <td>-0.688587</td>\n",
              "      <td>0.846554</td>\n",
              "      <td>0.836872</td>\n",
              "      <td>0.828866</td>\n",
              "      <td>0.795049</td>\n",
              "      <td>0.800745</td>\n",
              "      <td>-0.608918</td>\n",
              "      <td>0.800745</td>\n",
              "      <td>0.937385</td>\n",
              "      <td>0.930469</td>\n",
              "      <td>0.844743</td>\n",
              "      <td>0.845029</td>\n",
              "      <td>-0.308067</td>\n",
              "      <td>0.845029</td>\n",
              "      <td>0.919961</td>\n",
              "      <td>0.94154</td>\n",
              "      <td>0.927397</td>\n",
              "      <td>...</td>\n",
              "      <td>0.899512</td>\n",
              "      <td>-0.782119</td>\n",
              "      <td>0.899512</td>\n",
              "      <td>0.876177</td>\n",
              "      <td>0.893533</td>\n",
              "      <td>0.878559</td>\n",
              "      <td>0.87149</td>\n",
              "      <td>-0.70386</td>\n",
              "      <td>0.87149</td>\n",
              "      <td>0.870893</td>\n",
              "      <td>0.887156</td>\n",
              "      <td>0.873858</td>\n",
              "      <td>0.874984</td>\n",
              "      <td>-0.78521</td>\n",
              "      <td>0.874984</td>\n",
              "      <td>A 117-year-old woman in Mexico City finally re...</td>\n",
              "      <td>IMF chief backs Athens as permanent Olympic host</td>\n",
              "      <td>The president of France says if Brexit won, so...</td>\n",
              "      <td>British Man Who Must Give Police 24 Hours' Not...</td>\n",
              "      <td>100+ Nobel laureates urge Greenpeace to stop o...</td>\n",
              "      <td>Brazil: Huge spike in number of police killing...</td>\n",
              "      <td>Austria's highest court annuls presidential el...</td>\n",
              "      <td>Facebook wins privacy case, can track any Belg...</td>\n",
              "      <td>Switzerland denies Muslim girls citizenship af...</td>\n",
              "      <td>China kills millions of innocent meditators fo...</td>\n",
              "      <td>France Cracks Down on Factory Farms - A viral ...</td>\n",
              "      <td>Abbas PLO Faction Calls Killer of 13-Year-Old ...</td>\n",
              "      <td>Taiwanese warship accidentally fires missile t...</td>\n",
              "      <td>Iran celebrates American Human Rights Week, mo...</td>\n",
              "      <td>U.N. panel moves to curb bias against L.G.B.T....</td>\n",
              "      <td>The United States has placed Myanmar, Uzbekist...</td>\n",
              "      <td>S&amp;amp;P revises European Union credit rating t...</td>\n",
              "      <td>India gets $1 billion loan from World Bank for...</td>\n",
              "      <td>U.S. sailors detained by Iran spoke too much u...</td>\n",
              "      <td>Mass fish kill in Vietnam solved as Taiwan ste...</td>\n",
              "      <td>Philippines president Rodrigo Duterte urges pe...</td>\n",
              "      <td>Spain arrests three Pakistanis accused of prom...</td>\n",
              "      <td>Venezuela, where anger over food shortages is ...</td>\n",
              "      <td>A Hindu temple worker has been killed by three...</td>\n",
              "      <td>Ozone layer hole seems to be healing - US &amp;amp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2016-06-30</td>\n",
              "      <td>0.897638</td>\n",
              "      <td>0.927717</td>\n",
              "      <td>0.904977</td>\n",
              "      <td>0.934995</td>\n",
              "      <td>-0.626052</td>\n",
              "      <td>0.934995</td>\n",
              "      <td>0.854005</td>\n",
              "      <td>0.888874</td>\n",
              "      <td>0.861634</td>\n",
              "      <td>0.894995</td>\n",
              "      <td>-0.706021</td>\n",
              "      <td>0.894995</td>\n",
              "      <td>0.808881</td>\n",
              "      <td>0.838231</td>\n",
              "      <td>0.816642</td>\n",
              "      <td>0.846554</td>\n",
              "      <td>-0.688587</td>\n",
              "      <td>0.846554</td>\n",
              "      <td>0.836872</td>\n",
              "      <td>0.828866</td>\n",
              "      <td>0.795049</td>\n",
              "      <td>0.800745</td>\n",
              "      <td>-0.608918</td>\n",
              "      <td>0.800745</td>\n",
              "      <td>0.937385</td>\n",
              "      <td>0.930469</td>\n",
              "      <td>0.844743</td>\n",
              "      <td>0.845029</td>\n",
              "      <td>-0.308067</td>\n",
              "      <td>0.845029</td>\n",
              "      <td>0.919961</td>\n",
              "      <td>0.94154</td>\n",
              "      <td>0.927397</td>\n",
              "      <td>0.948778</td>\n",
              "      <td>-0.730957</td>\n",
              "      <td>0.948778</td>\n",
              "      <td>0.918017</td>\n",
              "      <td>0.925922</td>\n",
              "      <td>0.9149</td>\n",
              "      <td>...</td>\n",
              "      <td>0.87149</td>\n",
              "      <td>-0.70386</td>\n",
              "      <td>0.87149</td>\n",
              "      <td>0.870893</td>\n",
              "      <td>0.887156</td>\n",
              "      <td>0.873858</td>\n",
              "      <td>0.874984</td>\n",
              "      <td>-0.78521</td>\n",
              "      <td>0.874984</td>\n",
              "      <td>0.893308</td>\n",
              "      <td>0.885914</td>\n",
              "      <td>0.866414</td>\n",
              "      <td>0.869034</td>\n",
              "      <td>-0.754812</td>\n",
              "      <td>0.869034</td>\n",
              "      <td>Jamaica proposes marijuana dispensers for tour...</td>\n",
              "      <td>Stephen Hawking says pollution and 'stupidity'...</td>\n",
              "      <td>Boris Johnson says he will not run for Tory pa...</td>\n",
              "      <td>Six gay men in Ivory Coast were abused and for...</td>\n",
              "      <td>Switzerland denies citizenship to Muslim immig...</td>\n",
              "      <td>Palestinian terrorist stabs israeli teen girl ...</td>\n",
              "      <td>Puerto Rico will default on $1 billion of debt...</td>\n",
              "      <td>Republic of Ireland fans to be awarded medal f...</td>\n",
              "      <td>Afghan suicide bomber 'kills up to 40' - BBC News</td>\n",
              "      <td>US airstrikes kill at least 250 ISIS fighters ...</td>\n",
              "      <td>Turkish Cop Who Took Down Istanbul Gunman Hail...</td>\n",
              "      <td>Cannabis compounds could treat Alzheimer's by ...</td>\n",
              "      <td>Japan's top court has approved blanket surveil...</td>\n",
              "      <td>CIA Gave Romania Millions to Host Secret Prisons</td>\n",
              "      <td>Groups urge U.N. to suspend Saudi Arabia from ...</td>\n",
              "      <td>Googles free wifi at Indian railway stations i...</td>\n",
              "      <td>Mounting evidence suggests 'hobbits' were wipe...</td>\n",
              "      <td>The men who carried out Tuesday's terror attac...</td>\n",
              "      <td>Calls to suspend Saudi Arabia from UN Human Ri...</td>\n",
              "      <td>More Than 100 Nobel Laureates Call Out Greenpe...</td>\n",
              "      <td>British pedophile sentenced to 85 years in US ...</td>\n",
              "      <td>US permitted 1,200 offshore fracks in Gulf of ...</td>\n",
              "      <td>We will be swimming in ridicule - French beach...</td>\n",
              "      <td>UEFA says no minutes of silence for Istanbul v...</td>\n",
              "      <td>Law Enforcement Sources: Gun Used in Paris Ter...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 386 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         Date  ...                                              Top25\n",
              "0  2016-07-01  ...  Ozone layer hole seems to be healing - US &amp...\n",
              "1  2016-06-30  ...  Law Enforcement Sources: Gun Used in Paris Ter...\n",
              "\n",
              "[2 rows x 386 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PPyrouheS8-",
        "outputId": "3c519f90-938d-4eac-a415-2046161d2003"
      },
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "sna = SentimentIntensityAnalyzer()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgTom4oVeTCb",
        "outputId": "407e5d9e-eb6a-4d3b-acf4-6b37f1fdb9e6"
      },
      "source": [
        "df_Combined2 = df_Combined.copy()\n",
        "for i in range(0, df_Combined2.shape[0]):\n",
        "  News_Date_array = df_RedditNews[df_RedditNews[\"Date\"] == df_Combined2[\"Date\"][i]][\"News\"].to_numpy()\n",
        "\n",
        "  for j in range(0, 25):\n",
        "    df_Combined2[\"Top\"+str(j+1)][i] = sna.polarity_scores(df_Combined2[\"Top\"+str(j+1)][i])[\"compound\"]\n",
        "    "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bs9Dxqnp3wAt"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_nos3er2mkS"
      },
      "source": [
        "df_final = df_Combined2.drop(['Date', 'High', 'Low', 'Volume', 'Adj Close'], axis=1)\n",
        "X_df = df_final.drop(['Close'], axis=1)\n",
        "Y_df = df_final['Close']"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmyHXqsBHDNM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "766e91f8-fde5-4334-f2e1-713293746a03"
      },
      "source": [
        "df_final.head(2)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>Close</th>\n",
              "      <th>1 day before Open</th>\n",
              "      <th>1 day before High</th>\n",
              "      <th>1 day before Low</th>\n",
              "      <th>1 day before Close</th>\n",
              "      <th>1 day before Volume</th>\n",
              "      <th>1 day before Adj Close</th>\n",
              "      <th>2 day before Open</th>\n",
              "      <th>2 day before High</th>\n",
              "      <th>2 day before Low</th>\n",
              "      <th>2 day before Close</th>\n",
              "      <th>2 day before Volume</th>\n",
              "      <th>2 day before Adj Close</th>\n",
              "      <th>3 day before Open</th>\n",
              "      <th>3 day before High</th>\n",
              "      <th>3 day before Low</th>\n",
              "      <th>3 day before Close</th>\n",
              "      <th>3 day before Volume</th>\n",
              "      <th>3 day before Adj Close</th>\n",
              "      <th>4 day before Open</th>\n",
              "      <th>4 day before High</th>\n",
              "      <th>4 day before Low</th>\n",
              "      <th>4 day before Close</th>\n",
              "      <th>4 day before Volume</th>\n",
              "      <th>4 day before Adj Close</th>\n",
              "      <th>5 day before Open</th>\n",
              "      <th>5 day before High</th>\n",
              "      <th>5 day before Low</th>\n",
              "      <th>5 day before Close</th>\n",
              "      <th>5 day before Volume</th>\n",
              "      <th>5 day before Adj Close</th>\n",
              "      <th>6 day before Open</th>\n",
              "      <th>6 day before High</th>\n",
              "      <th>6 day before Low</th>\n",
              "      <th>6 day before Close</th>\n",
              "      <th>6 day before Volume</th>\n",
              "      <th>6 day before Adj Close</th>\n",
              "      <th>7 day before Open</th>\n",
              "      <th>7 day before High</th>\n",
              "      <th>...</th>\n",
              "      <th>57 day before Close</th>\n",
              "      <th>57 day before Volume</th>\n",
              "      <th>57 day before Adj Close</th>\n",
              "      <th>58 day before Open</th>\n",
              "      <th>58 day before High</th>\n",
              "      <th>58 day before Low</th>\n",
              "      <th>58 day before Close</th>\n",
              "      <th>58 day before Volume</th>\n",
              "      <th>58 day before Adj Close</th>\n",
              "      <th>59 day before Open</th>\n",
              "      <th>59 day before High</th>\n",
              "      <th>59 day before Low</th>\n",
              "      <th>59 day before Close</th>\n",
              "      <th>59 day before Volume</th>\n",
              "      <th>59 day before Adj Close</th>\n",
              "      <th>Top1</th>\n",
              "      <th>Top2</th>\n",
              "      <th>Top3</th>\n",
              "      <th>Top4</th>\n",
              "      <th>Top5</th>\n",
              "      <th>Top6</th>\n",
              "      <th>Top7</th>\n",
              "      <th>Top8</th>\n",
              "      <th>Top9</th>\n",
              "      <th>Top10</th>\n",
              "      <th>Top11</th>\n",
              "      <th>Top12</th>\n",
              "      <th>Top13</th>\n",
              "      <th>Top14</th>\n",
              "      <th>Top15</th>\n",
              "      <th>Top16</th>\n",
              "      <th>Top17</th>\n",
              "      <th>Top18</th>\n",
              "      <th>Top19</th>\n",
              "      <th>Top20</th>\n",
              "      <th>Top21</th>\n",
              "      <th>Top22</th>\n",
              "      <th>Top23</th>\n",
              "      <th>Top24</th>\n",
              "      <th>Top25</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.933579</td>\n",
              "      <td>0.938290</td>\n",
              "      <td>0.897638</td>\n",
              "      <td>0.927717</td>\n",
              "      <td>0.904977</td>\n",
              "      <td>0.934995</td>\n",
              "      <td>-0.626052</td>\n",
              "      <td>0.934995</td>\n",
              "      <td>0.854005</td>\n",
              "      <td>0.888874</td>\n",
              "      <td>0.861634</td>\n",
              "      <td>0.894995</td>\n",
              "      <td>-0.706021</td>\n",
              "      <td>0.894995</td>\n",
              "      <td>0.808881</td>\n",
              "      <td>0.838231</td>\n",
              "      <td>0.816642</td>\n",
              "      <td>0.846554</td>\n",
              "      <td>-0.688587</td>\n",
              "      <td>0.846554</td>\n",
              "      <td>0.836872</td>\n",
              "      <td>0.828866</td>\n",
              "      <td>0.795049</td>\n",
              "      <td>0.800745</td>\n",
              "      <td>-0.608918</td>\n",
              "      <td>0.800745</td>\n",
              "      <td>0.937385</td>\n",
              "      <td>0.930469</td>\n",
              "      <td>0.844743</td>\n",
              "      <td>0.845029</td>\n",
              "      <td>-0.308067</td>\n",
              "      <td>0.845029</td>\n",
              "      <td>0.919961</td>\n",
              "      <td>0.94154</td>\n",
              "      <td>0.927397</td>\n",
              "      <td>0.948778</td>\n",
              "      <td>-0.730957</td>\n",
              "      <td>0.948778</td>\n",
              "      <td>0.918017</td>\n",
              "      <td>0.925922</td>\n",
              "      <td>...</td>\n",
              "      <td>0.899512</td>\n",
              "      <td>-0.782119</td>\n",
              "      <td>0.899512</td>\n",
              "      <td>0.876177</td>\n",
              "      <td>0.893533</td>\n",
              "      <td>0.878559</td>\n",
              "      <td>0.87149</td>\n",
              "      <td>-0.70386</td>\n",
              "      <td>0.87149</td>\n",
              "      <td>0.870893</td>\n",
              "      <td>0.887156</td>\n",
              "      <td>0.873858</td>\n",
              "      <td>0.874984</td>\n",
              "      <td>-0.78521</td>\n",
              "      <td>0.874984</td>\n",
              "      <td>-0.5574</td>\n",
              "      <td>-0.0516</td>\n",
              "      <td>0.5719</td>\n",
              "      <td>-0.8658</td>\n",
              "      <td>-0.296</td>\n",
              "      <td>-0.4404</td>\n",
              "      <td>-0.3182</td>\n",
              "      <td>0.5612</td>\n",
              "      <td>-0.7351</td>\n",
              "      <td>-0.2732</td>\n",
              "      <td>-0.8402</td>\n",
              "      <td>-0.6486</td>\n",
              "      <td>-0.4767</td>\n",
              "      <td>0.1779</td>\n",
              "      <td>-0.1027</td>\n",
              "      <td>-0.5859</td>\n",
              "      <td>0.3818</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.4019</td>\n",
              "      <td>-0.3182</td>\n",
              "      <td>-0.9509</td>\n",
              "      <td>-0.3818</td>\n",
              "      <td>-0.9618</td>\n",
              "      <td>-0.9432</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.897638</td>\n",
              "      <td>0.934995</td>\n",
              "      <td>0.854005</td>\n",
              "      <td>0.888874</td>\n",
              "      <td>0.861634</td>\n",
              "      <td>0.894995</td>\n",
              "      <td>-0.706021</td>\n",
              "      <td>0.894995</td>\n",
              "      <td>0.808881</td>\n",
              "      <td>0.838231</td>\n",
              "      <td>0.816642</td>\n",
              "      <td>0.846554</td>\n",
              "      <td>-0.688587</td>\n",
              "      <td>0.846554</td>\n",
              "      <td>0.836872</td>\n",
              "      <td>0.828866</td>\n",
              "      <td>0.795049</td>\n",
              "      <td>0.800745</td>\n",
              "      <td>-0.608918</td>\n",
              "      <td>0.800745</td>\n",
              "      <td>0.937385</td>\n",
              "      <td>0.930469</td>\n",
              "      <td>0.844743</td>\n",
              "      <td>0.845029</td>\n",
              "      <td>-0.308067</td>\n",
              "      <td>0.845029</td>\n",
              "      <td>0.919961</td>\n",
              "      <td>0.94154</td>\n",
              "      <td>0.927397</td>\n",
              "      <td>0.948778</td>\n",
              "      <td>-0.730957</td>\n",
              "      <td>0.948778</td>\n",
              "      <td>0.918017</td>\n",
              "      <td>0.925922</td>\n",
              "      <td>0.9149</td>\n",
              "      <td>0.90964</td>\n",
              "      <td>-0.756853</td>\n",
              "      <td>0.90964</td>\n",
              "      <td>0.917109</td>\n",
              "      <td>0.918651</td>\n",
              "      <td>...</td>\n",
              "      <td>0.87149</td>\n",
              "      <td>-0.70386</td>\n",
              "      <td>0.87149</td>\n",
              "      <td>0.870893</td>\n",
              "      <td>0.887156</td>\n",
              "      <td>0.873858</td>\n",
              "      <td>0.874984</td>\n",
              "      <td>-0.78521</td>\n",
              "      <td>0.874984</td>\n",
              "      <td>0.893308</td>\n",
              "      <td>0.885914</td>\n",
              "      <td>0.866414</td>\n",
              "      <td>0.869034</td>\n",
              "      <td>-0.754812</td>\n",
              "      <td>0.869034</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.4141</td>\n",
              "      <td>0.4019</td>\n",
              "      <td>-0.8934</td>\n",
              "      <td>-0.6124</td>\n",
              "      <td>-0.91</td>\n",
              "      <td>-0.3612</td>\n",
              "      <td>0.7003</td>\n",
              "      <td>-0.8402</td>\n",
              "      <td>-0.7096</td>\n",
              "      <td>0.6705</td>\n",
              "      <td>0.4019</td>\n",
              "      <td>-0.5423</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.3182</td>\n",
              "      <td>0.7351</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.7579</td>\n",
              "      <td>-0.3182</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.9578</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.872</td>\n",
              "      <td>-0.5423</td>\n",
              "      <td>-0.875</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 381 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Open     Close 1 day before Open  ...   Top23   Top24  Top25\n",
              "0  0.933579  0.938290          0.897638  ... -0.9618 -0.9432      0\n",
              "1  0.897638  0.934995          0.854005  ...  -0.872 -0.5423 -0.875\n",
              "\n",
              "[2 rows x 381 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoYUaMCRP_Lq"
      },
      "source": [
        "train_size = X_df.shape[0]*0.8\n",
        "test_size = X_df.shape[0]*0.2"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jjkCDh51cOd"
      },
      "source": [
        "x_train, x_test, y_train, y_test = (X_df.tail(int(train_size)), X_df.head(int(test_size)), Y_df.tail(int(train_size)), Y_df.head(int(test_size)))\n",
        "# x_train, x_test, y_train, y_test = train_test_split(X_df, Y_df, test_size=0.3, random_state=1)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GugGUO6sfyQZ"
      },
      "source": [
        "data_x_train = x_train\n",
        "data_y_train = np.array(y_train, ndmin=2).reshape((y_train.shape[0], 1))\n",
        "data_x_test = x_test\n",
        "data_y_test = np.array(y_test, ndmin=2).reshape((y_test.shape[0], 1))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQffrPhz3ZRU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2373dd1-e22e-400e-abbe-3e4593a1cb5e"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPvtswI8bTtd"
      },
      "source": [
        "features = x_train.shape[1]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogkI0cqtbTwV"
      },
      "source": [
        "x = tf.placeholder(tf.float32, [None, features])\n",
        "y_ = tf.placeholder(tf.float32, [None, 1])\n",
        "W = tf.Variable(tf.zeros([features,1]))\n",
        "b = tf.Variable(tf.zeros([1]))\n",
        "y = tf.matmul(x,W) + b\n",
        "loss = tf.reduce_mean(tf.pow(y - y_, 2))\n",
        "update = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BS2V83Y9bTy3",
        "outputId": "36820fa3-0d3e-4bef-efd0-ed136959ffd8"
      },
      "source": [
        "train_losses = []\n",
        "test_losses = []\n",
        "temp_loss = sess.run(loss, feed_dict = {x: data_x_test, y_: data_y_test})\n",
        "print('iter:', 0,', loss:', temp_loss)\n",
        "for i in range(50):\n",
        "    sess.run(update, feed_dict = {x:data_x_train, y_:data_y_train})\n",
        "  \n",
        "    temp_loss = sess.run(loss, feed_dict = {x: data_x_train, y_: data_y_train})\n",
        "    train_losses.append(temp_loss)\n",
        "    temp_loss = sess.run(loss, feed_dict = {x: data_x_test, y_: data_y_test})\n",
        "    test_losses.append(temp_loss)\n",
        "    print('iter:', i+1,', loss:', temp_loss)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter: 0 , loss: 0.7480454\n",
            "iter: 1 , loss: 0.566847\n",
            "iter: 2 , loss: 0.42938447\n",
            "iter: 3 , loss: 0.32520777\n",
            "iter: 4 , loss: 0.24635\n",
            "iter: 5 , loss: 0.1867388\n",
            "iter: 6 , loss: 0.14174764\n",
            "iter: 7 , loss: 0.10785278\n",
            "iter: 8 , loss: 0.082371786\n",
            "iter: 9 , loss: 0.063263655\n",
            "iter: 10 , loss: 0.04897628\n",
            "iter: 11 , loss: 0.038330216\n",
            "iter: 12 , loss: 0.030429933\n",
            "iter: 13 , loss: 0.024595965\n",
            "iter: 14 , loss: 0.020313397\n",
            "iter: 15 , loss: 0.017192416\n",
            "iter: 16 , loss: 0.014938354\n",
            "iter: 17 , loss: 0.013328791\n",
            "iter: 18 , loss: 0.012196177\n",
            "iter: 19 , loss: 0.011414538\n",
            "iter: 20 , loss: 0.010889405\n",
            "iter: 21 , loss: 0.010550158\n",
            "iter: 22 , loss: 0.010344171\n",
            "iter: 23 , loss: 0.010232399\n",
            "iter: 24 , loss: 0.010186022\n",
            "iter: 25 , loss: 0.010183896\n",
            "iter: 26 , loss: 0.010210619\n",
            "iter: 27 , loss: 0.010255086\n",
            "iter: 28 , loss: 0.010309382\n",
            "iter: 29 , loss: 0.010367961\n",
            "iter: 30 , loss: 0.0104270065\n",
            "iter: 31 , loss: 0.010483979\n",
            "iter: 32 , loss: 0.010537265\n",
            "iter: 33 , loss: 0.0105859125\n",
            "iter: 34 , loss: 0.010629428\n",
            "iter: 35 , loss: 0.010667661\n",
            "iter: 36 , loss: 0.010700672\n",
            "iter: 37 , loss: 0.010728667\n",
            "iter: 38 , loss: 0.010751947\n",
            "iter: 39 , loss: 0.010770848\n",
            "iter: 40 , loss: 0.010785754\n",
            "iter: 41 , loss: 0.010797025\n",
            "iter: 42 , loss: 0.01080503\n",
            "iter: 43 , loss: 0.010810115\n",
            "iter: 44 , loss: 0.010812604\n",
            "iter: 45 , loss: 0.010812791\n",
            "iter: 46 , loss: 0.010810951\n",
            "iter: 47 , loss: 0.010807333\n",
            "iter: 48 , loss: 0.010802154\n",
            "iter: 49 , loss: 0.010795613\n",
            "iter: 50 , loss: 0.010787891\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcK2U67JbT1e"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oX45HVuNSgIK"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "4tSr4Qj6vNyG",
        "outputId": "ae47878f-0d5e-42be-9b2c-4df620ff20c1"
      },
      "source": [
        "iter = np.arange(1,len(test_losses)+1)\n",
        "plt.title('Linear Regresion Mean Squerd Error minimization')\n",
        "plt.plot(iter, test_losses, label='Testing loss')\n",
        "plt.plot(iter, train_losses, label='Training loss')\n",
        "plt.legend()\n",
        "plt.xlabel('Number of iterations')\n",
        "plt.ylabel('Mean Squerd Error')\n",
        "plt.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cClVf6nHvN15",
        "outputId": "7448be02-40a0-43fd-b8e4-14c8bd382c8b"
      },
      "source": [
        "sess.run(loss, feed_dict = {x: data_x_test, y_: data_y_test})"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.010787891"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dat1eNqD38l6"
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jz5-xEvd38pQ"
      },
      "source": [
        "train_size = X_df.shape[0]*0.8\n",
        "test_size = X_df.shape[0]*0.2"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASRzbReOZBV3"
      },
      "source": [
        "x_train, x_test, y_train, y_test = (X_df.tail(int(train_size)), X_df.head(int(test_size)), Y_df.tail(int(train_size)), Y_df.head(int(test_size)))\n",
        "# x_train, x_test, y_train, y_test = train_test_split(X_df, Y_df, test_size=0.3, random_state=1)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y4STaNygmJQ"
      },
      "source": [
        "data_x_train = x_train\n",
        "data_y_train = np.array(y_train, ndmin=2).reshape((y_train.shape[0], 1))\n",
        "data_x_test = x_test\n",
        "data_y_test = np.array(y_test, ndmin=2).reshape((y_test.shape[0], 1))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJNEFUoZ_UoW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8b4cd59-e60b-4718-b7d6-5a144c0d4f53"
      },
      "source": [
        "train_size = x_train.shape[0]\n",
        "print(train_size)\n",
        "test_size = x_test.shape[0]\n",
        "print(test_size)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1544\n",
            "386\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAo33rgzRctw"
      },
      "source": [
        "step_size = 0.02\n",
        "# (hidden1_size, hidden2_size, hidden3_size, hidden4_size, hidden5_size) = (350, 1, 1, 1, 1 )\n",
        "(hidden1_size, hidden2_size, hidden3_size, hidden4_size, hidden5_size) = (300, 50, 1, 1, 1 )\n",
        "# (hidden1_size, hidden2_size, hidden3_size, hidden4_size, hidden5_size) = (1228, 1228, 612, 1, 1 )"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5vzlppZRKzV"
      },
      "source": [
        "for _ in [1,2]:\n",
        "  minVal=-0.0001\n",
        "  maxVal=0.0001\n",
        "\n",
        "  x = tf.placeholder(tf.float32, shape=[None, x_train.shape[1]])\n",
        "  y_ = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "\n",
        "  W1 = tf.Variable(tf.random.uniform(shape=[x_train.shape[1], hidden1_size], minval=minVal, maxval=maxVal))\n",
        "  b1 = tf.Variable(tf.random.uniform(shape=[hidden1_size], minval=minVal, maxval=maxVal))\n",
        "  z1 = tf.nn.relu(tf.matmul(x,W1)+b1)\n",
        "\n",
        "  W2 = tf.Variable(tf.random.uniform([hidden1_size, hidden2_size], minval=minVal, maxval=maxVal))\n",
        "  b2 = tf.Variable(tf.random.uniform(shape=[hidden2_size], minval=minVal, maxval=maxVal))\n",
        "  # z2 = tf.nn.relu(tf.matmul(z1,W2)+b2)\n",
        "\n",
        "  # W3 = tf.Variable(tf.random.uniform([hidden2_size, hidden3_size], minval=minVal, maxval=maxVal))\n",
        "  # b3 = tf.Variable(tf.random.uniform(shape=[hidden3_size], minval=minVal, maxval=maxVal))\n",
        "  # z3 = tf.nn.relu(tf.matmul(z2,W3)+b3)\n",
        "\n",
        "  # W4 = tf.Variable(tf.random.uniform([hidden3_size, hidden4_size], minval=minVal, maxval=maxVal))\n",
        "  # b4 = tf.Variable(tf.random.uniform(shape=[hidden4_size], minval=minVal, maxval=maxVal))\n",
        "  # z4 = tf.nn.relu(tf.matmul(z3,W4)+b4)\n",
        "\n",
        "  # W5 = tf.Variable(tf.random.uniform([hidden4_size, hidden5_size], minval=minVal, maxval=maxVal))\n",
        "  # b5 = tf.Variable(tf.random.uniform(shape=[hidden5_size], minval=minVal, maxval=maxVal))\n",
        "  # z5 = tf.nn.relu(tf.matmul(z4,W5)+b5)\n",
        "\n",
        "  # W6 = tf.Variable(tf.random.uniform([hidden5_size, 1], minval=minVal, maxval=maxVal))\n",
        "  # b6 = tf.Variable(tf.random.uniform(shape=[1], minval=minVal, maxval=maxVal))\n",
        "\n",
        "  predict = tf.matmul(z1,W2) + b2\n",
        "  # predict = tf.matmul(z5,W6) + b6\n",
        "  # predict = tf.matmul(z4,W5) + b5\n",
        "  # predict = tf.matmul(z5,W6) + b6\n",
        "\n",
        "  loss = tf.reduce_mean(tf.pow(predict - y_, 2))\n",
        "  train_step = tf.train.GradientDescentOptimizer(step_size).minimize(loss)\n",
        "\n",
        "  init = tf.global_variables_initializer()\n",
        "  sess = tf.Session()\n",
        "  train_losses = []\n",
        "  test_losses = []\n",
        "  sess.run(init)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h78yXovxc9VT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "990036cf-5fc1-443e-8dcf-cf868bde8bd5"
      },
      "source": [
        "for i in range (2000):\n",
        "    for _ in range(10):\n",
        "      sess.run(train_step, feed_dict={x:data_x_train, y_:data_y_train})\n",
        "  \n",
        "    temp_loss = sess.run(loss, feed_dict = {x: data_x_train, y_: data_y_train})\n",
        "    train_losses.append(temp_loss)\n",
        "    temp_loss = sess.run(loss, feed_dict = {x: data_x_test, y_: data_y_test})\n",
        "    test_losses.append(temp_loss)\n",
        "    print('iter:', i+1,', loss:', temp_loss)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter: 1 , loss: 0.7340909\n",
            "iter: 2 , loss: 0.72480744\n",
            "iter: 3 , loss: 0.717866\n",
            "iter: 4 , loss: 0.7091065\n",
            "iter: 5 , loss: 0.6823566\n",
            "iter: 6 , loss: 0.5728468\n",
            "iter: 7 , loss: 0.2790476\n",
            "iter: 8 , loss: 0.0502299\n",
            "iter: 9 , loss: 0.013083776\n",
            "iter: 10 , loss: 0.010259821\n",
            "iter: 11 , loss: 0.010195697\n",
            "iter: 12 , loss: 0.010183903\n",
            "iter: 13 , loss: 0.010109145\n",
            "iter: 14 , loss: 0.010029288\n",
            "iter: 15 , loss: 0.00996156\n",
            "iter: 16 , loss: 0.009905634\n",
            "iter: 17 , loss: 0.009858397\n",
            "iter: 18 , loss: 0.009817437\n",
            "iter: 19 , loss: 0.009781184\n",
            "iter: 20 , loss: 0.009748744\n",
            "iter: 21 , loss: 0.0097195245\n",
            "iter: 22 , loss: 0.009693523\n",
            "iter: 23 , loss: 0.009670089\n",
            "iter: 24 , loss: 0.009648918\n",
            "iter: 25 , loss: 0.009630041\n",
            "iter: 26 , loss: 0.00961337\n",
            "iter: 27 , loss: 0.009599086\n",
            "iter: 28 , loss: 0.009586315\n",
            "iter: 29 , loss: 0.009574987\n",
            "iter: 30 , loss: 0.009564889\n",
            "iter: 31 , loss: 0.0095550185\n",
            "iter: 32 , loss: 0.009544881\n",
            "iter: 33 , loss: 0.009534358\n",
            "iter: 34 , loss: 0.009523351\n",
            "iter: 35 , loss: 0.009511917\n",
            "iter: 36 , loss: 0.009500068\n",
            "iter: 37 , loss: 0.00948771\n",
            "iter: 38 , loss: 0.009474616\n",
            "iter: 39 , loss: 0.00946077\n",
            "iter: 40 , loss: 0.009446094\n",
            "iter: 41 , loss: 0.0094300825\n",
            "iter: 42 , loss: 0.009412865\n",
            "iter: 43 , loss: 0.009394723\n",
            "iter: 44 , loss: 0.009375702\n",
            "iter: 45 , loss: 0.009355845\n",
            "iter: 46 , loss: 0.009335227\n",
            "iter: 47 , loss: 0.009313813\n",
            "iter: 48 , loss: 0.009291628\n",
            "iter: 49 , loss: 0.009268662\n",
            "iter: 50 , loss: 0.009244894\n",
            "iter: 51 , loss: 0.009220337\n",
            "iter: 52 , loss: 0.009195052\n",
            "iter: 53 , loss: 0.00916908\n",
            "iter: 54 , loss: 0.0091424035\n",
            "iter: 55 , loss: 0.009114966\n",
            "iter: 56 , loss: 0.009086828\n",
            "iter: 57 , loss: 0.009058098\n",
            "iter: 58 , loss: 0.009028988\n",
            "iter: 59 , loss: 0.008999518\n",
            "iter: 60 , loss: 0.008969622\n",
            "iter: 61 , loss: 0.008939393\n",
            "iter: 62 , loss: 0.008908878\n",
            "iter: 63 , loss: 0.008878208\n",
            "iter: 64 , loss: 0.008847465\n",
            "iter: 65 , loss: 0.008816693\n",
            "iter: 66 , loss: 0.008785908\n",
            "iter: 67 , loss: 0.008755085\n",
            "iter: 68 , loss: 0.00872423\n",
            "iter: 69 , loss: 0.008693315\n",
            "iter: 70 , loss: 0.008662368\n",
            "iter: 71 , loss: 0.00863141\n",
            "iter: 72 , loss: 0.008600521\n",
            "iter: 73 , loss: 0.008569725\n",
            "iter: 74 , loss: 0.008539026\n",
            "iter: 75 , loss: 0.008508425\n",
            "iter: 76 , loss: 0.0084779225\n",
            "iter: 77 , loss: 0.008447515\n",
            "iter: 78 , loss: 0.008417206\n",
            "iter: 79 , loss: 0.008386995\n",
            "iter: 80 , loss: 0.008356895\n",
            "iter: 81 , loss: 0.008326872\n",
            "iter: 82 , loss: 0.008296941\n",
            "iter: 83 , loss: 0.008267067\n",
            "iter: 84 , loss: 0.008237211\n",
            "iter: 85 , loss: 0.008207407\n",
            "iter: 86 , loss: 0.008177628\n",
            "iter: 87 , loss: 0.008147954\n",
            "iter: 88 , loss: 0.008118394\n",
            "iter: 89 , loss: 0.008088941\n",
            "iter: 90 , loss: 0.008059637\n",
            "iter: 91 , loss: 0.008030492\n",
            "iter: 92 , loss: 0.008001493\n",
            "iter: 93 , loss: 0.007972642\n",
            "iter: 94 , loss: 0.007943917\n",
            "iter: 95 , loss: 0.007915327\n",
            "iter: 96 , loss: 0.007886878\n",
            "iter: 97 , loss: 0.007858581\n",
            "iter: 98 , loss: 0.007830401\n",
            "iter: 99 , loss: 0.0078023355\n",
            "iter: 100 , loss: 0.007774391\n",
            "iter: 101 , loss: 0.0077465614\n",
            "iter: 102 , loss: 0.007718864\n",
            "iter: 103 , loss: 0.007691306\n",
            "iter: 104 , loss: 0.0076638767\n",
            "iter: 105 , loss: 0.007636517\n",
            "iter: 106 , loss: 0.007609211\n",
            "iter: 107 , loss: 0.0075819707\n",
            "iter: 108 , loss: 0.0075548114\n",
            "iter: 109 , loss: 0.007527793\n",
            "iter: 110 , loss: 0.0075009507\n",
            "iter: 111 , loss: 0.0074742823\n",
            "iter: 112 , loss: 0.0074477857\n",
            "iter: 113 , loss: 0.00742145\n",
            "iter: 114 , loss: 0.007395264\n",
            "iter: 115 , loss: 0.0073692193\n",
            "iter: 116 , loss: 0.0073433323\n",
            "iter: 117 , loss: 0.0073175947\n",
            "iter: 118 , loss: 0.0072920034\n",
            "iter: 119 , loss: 0.007266538\n",
            "iter: 120 , loss: 0.007241192\n",
            "iter: 121 , loss: 0.0072159576\n",
            "iter: 122 , loss: 0.007190843\n",
            "iter: 123 , loss: 0.007165845\n",
            "iter: 124 , loss: 0.0071409694\n",
            "iter: 125 , loss: 0.0071162214\n",
            "iter: 126 , loss: 0.0070915804\n",
            "iter: 127 , loss: 0.0070670536\n",
            "iter: 128 , loss: 0.007042635\n",
            "iter: 129 , loss: 0.007018322\n",
            "iter: 130 , loss: 0.006994121\n",
            "iter: 131 , loss: 0.006970031\n",
            "iter: 132 , loss: 0.006946048\n",
            "iter: 133 , loss: 0.0069221766\n",
            "iter: 134 , loss: 0.0068984055\n",
            "iter: 135 , loss: 0.0068747406\n",
            "iter: 136 , loss: 0.0068511753\n",
            "iter: 137 , loss: 0.0068276823\n",
            "iter: 138 , loss: 0.0068042595\n",
            "iter: 139 , loss: 0.006780925\n",
            "iter: 140 , loss: 0.0067576645\n",
            "iter: 141 , loss: 0.006734468\n",
            "iter: 142 , loss: 0.006711294\n",
            "iter: 143 , loss: 0.0066881706\n",
            "iter: 144 , loss: 0.006665145\n",
            "iter: 145 , loss: 0.006642222\n",
            "iter: 146 , loss: 0.0066194255\n",
            "iter: 147 , loss: 0.0065967566\n",
            "iter: 148 , loss: 0.006574209\n",
            "iter: 149 , loss: 0.0065517705\n",
            "iter: 150 , loss: 0.0065294434\n",
            "iter: 151 , loss: 0.0065072263\n",
            "iter: 152 , loss: 0.006485105\n",
            "iter: 153 , loss: 0.0064630797\n",
            "iter: 154 , loss: 0.0064411447\n",
            "iter: 155 , loss: 0.006419311\n",
            "iter: 156 , loss: 0.00639751\n",
            "iter: 157 , loss: 0.0063757687\n",
            "iter: 158 , loss: 0.00635406\n",
            "iter: 159 , loss: 0.006332379\n",
            "iter: 160 , loss: 0.0063107717\n",
            "iter: 161 , loss: 0.0062892414\n",
            "iter: 162 , loss: 0.006267644\n",
            "iter: 163 , loss: 0.006246122\n",
            "iter: 164 , loss: 0.006224714\n",
            "iter: 165 , loss: 0.006203428\n",
            "iter: 166 , loss: 0.0061822585\n",
            "iter: 167 , loss: 0.0061612464\n",
            "iter: 168 , loss: 0.006140394\n",
            "iter: 169 , loss: 0.0061196852\n",
            "iter: 170 , loss: 0.006099107\n",
            "iter: 171 , loss: 0.0060786675\n",
            "iter: 172 , loss: 0.0060583767\n",
            "iter: 173 , loss: 0.0060382085\n",
            "iter: 174 , loss: 0.006018141\n",
            "iter: 175 , loss: 0.005998171\n",
            "iter: 176 , loss: 0.0059783063\n",
            "iter: 177 , loss: 0.0059585604\n",
            "iter: 178 , loss: 0.005938909\n",
            "iter: 179 , loss: 0.005919341\n",
            "iter: 180 , loss: 0.005899859\n",
            "iter: 181 , loss: 0.0058804536\n",
            "iter: 182 , loss: 0.005861122\n",
            "iter: 183 , loss: 0.005841871\n",
            "iter: 184 , loss: 0.0058227084\n",
            "iter: 185 , loss: 0.005803623\n",
            "iter: 186 , loss: 0.005784589\n",
            "iter: 187 , loss: 0.0057656155\n",
            "iter: 188 , loss: 0.0057467017\n",
            "iter: 189 , loss: 0.005727855\n",
            "iter: 190 , loss: 0.0057090865\n",
            "iter: 191 , loss: 0.0056903963\n",
            "iter: 192 , loss: 0.0056717843\n",
            "iter: 193 , loss: 0.005653258\n",
            "iter: 194 , loss: 0.0056348224\n",
            "iter: 195 , loss: 0.005616449\n",
            "iter: 196 , loss: 0.0055981684\n",
            "iter: 197 , loss: 0.0055799843\n",
            "iter: 198 , loss: 0.0055618887\n",
            "iter: 199 , loss: 0.0055438774\n",
            "iter: 200 , loss: 0.0055259485\n",
            "iter: 201 , loss: 0.0055081085\n",
            "iter: 202 , loss: 0.0054903175\n",
            "iter: 203 , loss: 0.005472601\n",
            "iter: 204 , loss: 0.005454945\n",
            "iter: 205 , loss: 0.005437369\n",
            "iter: 206 , loss: 0.0054198694\n",
            "iter: 207 , loss: 0.005402449\n",
            "iter: 208 , loss: 0.0053851185\n",
            "iter: 209 , loss: 0.0053678616\n",
            "iter: 210 , loss: 0.0053507034\n",
            "iter: 211 , loss: 0.005333632\n",
            "iter: 212 , loss: 0.005316676\n",
            "iter: 213 , loss: 0.0052998015\n",
            "iter: 214 , loss: 0.005283005\n",
            "iter: 215 , loss: 0.0052662827\n",
            "iter: 216 , loss: 0.0052496684\n",
            "iter: 217 , loss: 0.0052331532\n",
            "iter: 218 , loss: 0.005216727\n",
            "iter: 219 , loss: 0.0052003823\n",
            "iter: 220 , loss: 0.0051841303\n",
            "iter: 221 , loss: 0.005167954\n",
            "iter: 222 , loss: 0.005151851\n",
            "iter: 223 , loss: 0.00513583\n",
            "iter: 224 , loss: 0.0051198914\n",
            "iter: 225 , loss: 0.0051040435\n",
            "iter: 226 , loss: 0.00508828\n",
            "iter: 227 , loss: 0.0050725834\n",
            "iter: 228 , loss: 0.0050569624\n",
            "iter: 229 , loss: 0.005041403\n",
            "iter: 230 , loss: 0.005025916\n",
            "iter: 231 , loss: 0.0050105043\n",
            "iter: 232 , loss: 0.0049951696\n",
            "iter: 233 , loss: 0.0049799187\n",
            "iter: 234 , loss: 0.0049647624\n",
            "iter: 235 , loss: 0.004949697\n",
            "iter: 236 , loss: 0.0049347137\n",
            "iter: 237 , loss: 0.0049198046\n",
            "iter: 238 , loss: 0.0049049715\n",
            "iter: 239 , loss: 0.004890209\n",
            "iter: 240 , loss: 0.004875517\n",
            "iter: 241 , loss: 0.004860905\n",
            "iter: 242 , loss: 0.004846378\n",
            "iter: 243 , loss: 0.0048319246\n",
            "iter: 244 , loss: 0.004817547\n",
            "iter: 245 , loss: 0.0048032445\n",
            "iter: 246 , loss: 0.0047890167\n",
            "iter: 247 , loss: 0.0047748685\n",
            "iter: 248 , loss: 0.0047608013\n",
            "iter: 249 , loss: 0.0047468026\n",
            "iter: 250 , loss: 0.0047328705\n",
            "iter: 251 , loss: 0.004719004\n",
            "iter: 252 , loss: 0.0047052028\n",
            "iter: 253 , loss: 0.00469147\n",
            "iter: 254 , loss: 0.004677804\n",
            "iter: 255 , loss: 0.0046642013\n",
            "iter: 256 , loss: 0.004650653\n",
            "iter: 257 , loss: 0.0046371534\n",
            "iter: 258 , loss: 0.004623705\n",
            "iter: 259 , loss: 0.0046103317\n",
            "iter: 260 , loss: 0.0045970385\n",
            "iter: 261 , loss: 0.0045837853\n",
            "iter: 262 , loss: 0.0045705726\n",
            "iter: 263 , loss: 0.004557422\n",
            "iter: 264 , loss: 0.0045442986\n",
            "iter: 265 , loss: 0.0045312336\n",
            "iter: 266 , loss: 0.0045182225\n",
            "iter: 267 , loss: 0.004505264\n",
            "iter: 268 , loss: 0.004492361\n",
            "iter: 269 , loss: 0.0044795503\n",
            "iter: 270 , loss: 0.0044668405\n",
            "iter: 271 , loss: 0.004454225\n",
            "iter: 272 , loss: 0.004441678\n",
            "iter: 273 , loss: 0.004429218\n",
            "iter: 274 , loss: 0.004416838\n",
            "iter: 275 , loss: 0.004404517\n",
            "iter: 276 , loss: 0.0043922714\n",
            "iter: 277 , loss: 0.004380102\n",
            "iter: 278 , loss: 0.0043680156\n",
            "iter: 279 , loss: 0.0043560024\n",
            "iter: 280 , loss: 0.0043440624\n",
            "iter: 281 , loss: 0.0043321913\n",
            "iter: 282 , loss: 0.004320405\n",
            "iter: 283 , loss: 0.0043086717\n",
            "iter: 284 , loss: 0.0042970055\n",
            "iter: 285 , loss: 0.0042854156\n",
            "iter: 286 , loss: 0.004273904\n",
            "iter: 287 , loss: 0.0042624585\n",
            "iter: 288 , loss: 0.0042510815\n",
            "iter: 289 , loss: 0.004239743\n",
            "iter: 290 , loss: 0.0042284545\n",
            "iter: 291 , loss: 0.0042172344\n",
            "iter: 292 , loss: 0.004206092\n",
            "iter: 293 , loss: 0.004195027\n",
            "iter: 294 , loss: 0.004184033\n",
            "iter: 295 , loss: 0.004173095\n",
            "iter: 296 , loss: 0.0041622203\n",
            "iter: 297 , loss: 0.004151407\n",
            "iter: 298 , loss: 0.004140665\n",
            "iter: 299 , loss: 0.0041300063\n",
            "iter: 300 , loss: 0.0041194195\n",
            "iter: 301 , loss: 0.0041088993\n",
            "iter: 302 , loss: 0.0040984517\n",
            "iter: 303 , loss: 0.004088071\n",
            "iter: 304 , loss: 0.0040777633\n",
            "iter: 305 , loss: 0.0040675253\n",
            "iter: 306 , loss: 0.004057335\n",
            "iter: 307 , loss: 0.004047191\n",
            "iter: 308 , loss: 0.004037093\n",
            "iter: 309 , loss: 0.004027038\n",
            "iter: 310 , loss: 0.0040170625\n",
            "iter: 311 , loss: 0.004007153\n",
            "iter: 312 , loss: 0.0039973026\n",
            "iter: 313 , loss: 0.0039875233\n",
            "iter: 314 , loss: 0.003977806\n",
            "iter: 315 , loss: 0.0039681587\n",
            "iter: 316 , loss: 0.003958583\n",
            "iter: 317 , loss: 0.003949074\n",
            "iter: 318 , loss: 0.003939628\n",
            "iter: 319 , loss: 0.0039302437\n",
            "iter: 320 , loss: 0.0039209225\n",
            "iter: 321 , loss: 0.0039116642\n",
            "iter: 322 , loss: 0.00390247\n",
            "iter: 323 , loss: 0.0038933386\n",
            "iter: 324 , loss: 0.0038842675\n",
            "iter: 325 , loss: 0.003875254\n",
            "iter: 326 , loss: 0.0038662911\n",
            "iter: 327 , loss: 0.0038573847\n",
            "iter: 328 , loss: 0.003848526\n",
            "iter: 329 , loss: 0.003839713\n",
            "iter: 330 , loss: 0.003830939\n",
            "iter: 331 , loss: 0.0038222158\n",
            "iter: 332 , loss: 0.0038135278\n",
            "iter: 333 , loss: 0.0038048804\n",
            "iter: 334 , loss: 0.0037962776\n",
            "iter: 335 , loss: 0.003787737\n",
            "iter: 336 , loss: 0.0037792427\n",
            "iter: 337 , loss: 0.0037708066\n",
            "iter: 338 , loss: 0.0037624317\n",
            "iter: 339 , loss: 0.0037541133\n",
            "iter: 340 , loss: 0.0037458476\n",
            "iter: 341 , loss: 0.0037376415\n",
            "iter: 342 , loss: 0.0037294854\n",
            "iter: 343 , loss: 0.00372138\n",
            "iter: 344 , loss: 0.0037133275\n",
            "iter: 345 , loss: 0.0037053358\n",
            "iter: 346 , loss: 0.003697398\n",
            "iter: 347 , loss: 0.0036895075\n",
            "iter: 348 , loss: 0.0036816758\n",
            "iter: 349 , loss: 0.003673896\n",
            "iter: 350 , loss: 0.0036661662\n",
            "iter: 351 , loss: 0.0036584907\n",
            "iter: 352 , loss: 0.0036508706\n",
            "iter: 353 , loss: 0.003643297\n",
            "iter: 354 , loss: 0.0036357685\n",
            "iter: 355 , loss: 0.0036282877\n",
            "iter: 356 , loss: 0.0036208534\n",
            "iter: 357 , loss: 0.0036134685\n",
            "iter: 358 , loss: 0.0036061301\n",
            "iter: 359 , loss: 0.0035988223\n",
            "iter: 360 , loss: 0.0035915591\n",
            "iter: 361 , loss: 0.0035843395\n",
            "iter: 362 , loss: 0.003577157\n",
            "iter: 363 , loss: 0.0035700218\n",
            "iter: 364 , loss: 0.0035629312\n",
            "iter: 365 , loss: 0.0035558892\n",
            "iter: 366 , loss: 0.0035488917\n",
            "iter: 367 , loss: 0.0035419392\n",
            "iter: 368 , loss: 0.003535036\n",
            "iter: 369 , loss: 0.0035281698\n",
            "iter: 370 , loss: 0.0035213402\n",
            "iter: 371 , loss: 0.003514555\n",
            "iter: 372 , loss: 0.0035078197\n",
            "iter: 373 , loss: 0.0035011303\n",
            "iter: 374 , loss: 0.0034944853\n",
            "iter: 375 , loss: 0.003487886\n",
            "iter: 376 , loss: 0.0034813313\n",
            "iter: 377 , loss: 0.0034748218\n",
            "iter: 378 , loss: 0.003468356\n",
            "iter: 379 , loss: 0.0034619311\n",
            "iter: 380 , loss: 0.0034555465\n",
            "iter: 381 , loss: 0.0034491918\n",
            "iter: 382 , loss: 0.0034428702\n",
            "iter: 383 , loss: 0.0034365884\n",
            "iter: 384 , loss: 0.003430348\n",
            "iter: 385 , loss: 0.0034241495\n",
            "iter: 386 , loss: 0.0034179941\n",
            "iter: 387 , loss: 0.0034118805\n",
            "iter: 388 , loss: 0.0034058096\n",
            "iter: 389 , loss: 0.0033997796\n",
            "iter: 390 , loss: 0.0033937902\n",
            "iter: 391 , loss: 0.0033878356\n",
            "iter: 392 , loss: 0.0033819205\n",
            "iter: 393 , loss: 0.003376046\n",
            "iter: 394 , loss: 0.0033702026\n",
            "iter: 395 , loss: 0.003364387\n",
            "iter: 396 , loss: 0.0033586072\n",
            "iter: 397 , loss: 0.0033528665\n",
            "iter: 398 , loss: 0.003347166\n",
            "iter: 399 , loss: 0.0033414953\n",
            "iter: 400 , loss: 0.0033358585\n",
            "iter: 401 , loss: 0.0033302626\n",
            "iter: 402 , loss: 0.0033246914\n",
            "iter: 403 , loss: 0.0033191454\n",
            "iter: 404 , loss: 0.0033136283\n",
            "iter: 405 , loss: 0.0033081516\n",
            "iter: 406 , loss: 0.003302712\n",
            "iter: 407 , loss: 0.003297309\n",
            "iter: 408 , loss: 0.0032919399\n",
            "iter: 409 , loss: 0.0032866076\n",
            "iter: 410 , loss: 0.0032813123\n",
            "iter: 411 , loss: 0.0032760492\n",
            "iter: 412 , loss: 0.0032708184\n",
            "iter: 413 , loss: 0.0032656223\n",
            "iter: 414 , loss: 0.0032604572\n",
            "iter: 415 , loss: 0.0032553202\n",
            "iter: 416 , loss: 0.00325021\n",
            "iter: 417 , loss: 0.003245131\n",
            "iter: 418 , loss: 0.00324008\n",
            "iter: 419 , loss: 0.003235058\n",
            "iter: 420 , loss: 0.0032300649\n",
            "iter: 421 , loss: 0.0032251026\n",
            "iter: 422 , loss: 0.0032201684\n",
            "iter: 423 , loss: 0.003215264\n",
            "iter: 424 , loss: 0.0032103958\n",
            "iter: 425 , loss: 0.0032055597\n",
            "iter: 426 , loss: 0.003200748\n",
            "iter: 427 , loss: 0.0031959654\n",
            "iter: 428 , loss: 0.0031912082\n",
            "iter: 429 , loss: 0.0031864778\n",
            "iter: 430 , loss: 0.0031817795\n",
            "iter: 431 , loss: 0.0031771061\n",
            "iter: 432 , loss: 0.0031724535\n",
            "iter: 433 , loss: 0.0031678271\n",
            "iter: 434 , loss: 0.0031632308\n",
            "iter: 435 , loss: 0.0031586583\n",
            "iter: 436 , loss: 0.0031541148\n",
            "iter: 437 , loss: 0.0031495935\n",
            "iter: 438 , loss: 0.0031450985\n",
            "iter: 439 , loss: 0.0031406227\n",
            "iter: 440 , loss: 0.0031361778\n",
            "iter: 441 , loss: 0.003131759\n",
            "iter: 442 , loss: 0.00312736\n",
            "iter: 443 , loss: 0.0031229833\n",
            "iter: 444 , loss: 0.0031186289\n",
            "iter: 445 , loss: 0.0031143047\n",
            "iter: 446 , loss: 0.0031100032\n",
            "iter: 447 , loss: 0.0031057273\n",
            "iter: 448 , loss: 0.0031014774\n",
            "iter: 449 , loss: 0.0030972506\n",
            "iter: 450 , loss: 0.0030930543\n",
            "iter: 451 , loss: 0.0030888843\n",
            "iter: 452 , loss: 0.0030847408\n",
            "iter: 453 , loss: 0.0030806214\n",
            "iter: 454 , loss: 0.0030765252\n",
            "iter: 455 , loss: 0.0030724509\n",
            "iter: 456 , loss: 0.003068399\n",
            "iter: 457 , loss: 0.00306437\n",
            "iter: 458 , loss: 0.003060364\n",
            "iter: 459 , loss: 0.0030563765\n",
            "iter: 460 , loss: 0.0030524123\n",
            "iter: 461 , loss: 0.003048472\n",
            "iter: 462 , loss: 0.0030445545\n",
            "iter: 463 , loss: 0.0030406553\n",
            "iter: 464 , loss: 0.0030367775\n",
            "iter: 465 , loss: 0.003032921\n",
            "iter: 466 , loss: 0.0030290845\n",
            "iter: 467 , loss: 0.0030252698\n",
            "iter: 468 , loss: 0.0030214766\n",
            "iter: 469 , loss: 0.003017701\n",
            "iter: 470 , loss: 0.003013949\n",
            "iter: 471 , loss: 0.0030102152\n",
            "iter: 472 , loss: 0.003006503\n",
            "iter: 473 , loss: 0.0030028082\n",
            "iter: 474 , loss: 0.0029991306\n",
            "iter: 475 , loss: 0.0029954705\n",
            "iter: 476 , loss: 0.0029918242\n",
            "iter: 477 , loss: 0.0029881832\n",
            "iter: 478 , loss: 0.0029845505\n",
            "iter: 479 , loss: 0.0029809396\n",
            "iter: 480 , loss: 0.0029773524\n",
            "iter: 481 , loss: 0.0029737866\n",
            "iter: 482 , loss: 0.0029702357\n",
            "iter: 483 , loss: 0.0029667055\n",
            "iter: 484 , loss: 0.0029631997\n",
            "iter: 485 , loss: 0.0029597087\n",
            "iter: 486 , loss: 0.0029562325\n",
            "iter: 487 , loss: 0.0029527734\n",
            "iter: 488 , loss: 0.0029493382\n",
            "iter: 489 , loss: 0.0029459258\n",
            "iter: 490 , loss: 0.0029425356\n",
            "iter: 491 , loss: 0.0029391658\n",
            "iter: 492 , loss: 0.002935817\n",
            "iter: 493 , loss: 0.002932486\n",
            "iter: 494 , loss: 0.0029291678\n",
            "iter: 495 , loss: 0.0029258663\n",
            "iter: 496 , loss: 0.0029225878\n",
            "iter: 497 , loss: 0.0029193237\n",
            "iter: 498 , loss: 0.0029160776\n",
            "iter: 499 , loss: 0.0029128485\n",
            "iter: 500 , loss: 0.0029096326\n",
            "iter: 501 , loss: 0.0029064268\n",
            "iter: 502 , loss: 0.002903237\n",
            "iter: 503 , loss: 0.0029000603\n",
            "iter: 504 , loss: 0.002896902\n",
            "iter: 505 , loss: 0.002893761\n",
            "iter: 506 , loss: 0.0028906367\n",
            "iter: 507 , loss: 0.0028875312\n",
            "iter: 508 , loss: 0.00288444\n",
            "iter: 509 , loss: 0.0028813614\n",
            "iter: 510 , loss: 0.0028782967\n",
            "iter: 511 , loss: 0.002875245\n",
            "iter: 512 , loss: 0.0028722081\n",
            "iter: 513 , loss: 0.002869181\n",
            "iter: 514 , loss: 0.0028661622\n",
            "iter: 515 , loss: 0.0028631585\n",
            "iter: 516 , loss: 0.0028601687\n",
            "iter: 517 , loss: 0.0028571957\n",
            "iter: 518 , loss: 0.0028542413\n",
            "iter: 519 , loss: 0.002851301\n",
            "iter: 520 , loss: 0.0028483728\n",
            "iter: 521 , loss: 0.0028454582\n",
            "iter: 522 , loss: 0.002842555\n",
            "iter: 523 , loss: 0.0028396675\n",
            "iter: 524 , loss: 0.0028367934\n",
            "iter: 525 , loss: 0.0028339338\n",
            "iter: 526 , loss: 0.0028310851\n",
            "iter: 527 , loss: 0.002828252\n",
            "iter: 528 , loss: 0.0028254336\n",
            "iter: 529 , loss: 0.0028226273\n",
            "iter: 530 , loss: 0.0028198324\n",
            "iter: 531 , loss: 0.0028170496\n",
            "iter: 532 , loss: 0.0028142768\n",
            "iter: 533 , loss: 0.0028115164\n",
            "iter: 534 , loss: 0.0028087734\n",
            "iter: 535 , loss: 0.0028060398\n",
            "iter: 536 , loss: 0.0028033184\n",
            "iter: 537 , loss: 0.0028006132\n",
            "iter: 538 , loss: 0.0027979168\n",
            "iter: 539 , loss: 0.002795233\n",
            "iter: 540 , loss: 0.002792562\n",
            "iter: 541 , loss: 0.0027899016\n",
            "iter: 542 , loss: 0.0027872524\n",
            "iter: 543 , loss: 0.0027846145\n",
            "iter: 544 , loss: 0.0027819907\n",
            "iter: 545 , loss: 0.0027793776\n",
            "iter: 546 , loss: 0.0027767753\n",
            "iter: 547 , loss: 0.0027741827\n",
            "iter: 548 , loss: 0.0027716025\n",
            "iter: 549 , loss: 0.0027690325\n",
            "iter: 550 , loss: 0.0027664728\n",
            "iter: 551 , loss: 0.002763922\n",
            "iter: 552 , loss: 0.002761379\n",
            "iter: 553 , loss: 0.0027588448\n",
            "iter: 554 , loss: 0.00275632\n",
            "iter: 555 , loss: 0.0027538047\n",
            "iter: 556 , loss: 0.0027512996\n",
            "iter: 557 , loss: 0.0027488023\n",
            "iter: 558 , loss: 0.0027463168\n",
            "iter: 559 , loss: 0.0027438367\n",
            "iter: 560 , loss: 0.0027413631\n",
            "iter: 561 , loss: 0.0027389028\n",
            "iter: 562 , loss: 0.0027364541\n",
            "iter: 563 , loss: 0.0027340157\n",
            "iter: 564 , loss: 0.0027315898\n",
            "iter: 565 , loss: 0.0027291754\n",
            "iter: 566 , loss: 0.0027267707\n",
            "iter: 567 , loss: 0.002724373\n",
            "iter: 568 , loss: 0.0027219844\n",
            "iter: 569 , loss: 0.002719604\n",
            "iter: 570 , loss: 0.0027172368\n",
            "iter: 571 , loss: 0.0027148789\n",
            "iter: 572 , loss: 0.0027125308\n",
            "iter: 573 , loss: 0.0027101927\n",
            "iter: 574 , loss: 0.0027078653\n",
            "iter: 575 , loss: 0.0027055459\n",
            "iter: 576 , loss: 0.0027032364\n",
            "iter: 577 , loss: 0.0027009312\n",
            "iter: 578 , loss: 0.0026986352\n",
            "iter: 579 , loss: 0.002696344\n",
            "iter: 580 , loss: 0.0026940617\n",
            "iter: 581 , loss: 0.0026917881\n",
            "iter: 582 , loss: 0.0026895222\n",
            "iter: 583 , loss: 0.0026872652\n",
            "iter: 584 , loss: 0.0026850218\n",
            "iter: 585 , loss: 0.0026827883\n",
            "iter: 586 , loss: 0.002680559\n",
            "iter: 587 , loss: 0.0026783366\n",
            "iter: 588 , loss: 0.0026761235\n",
            "iter: 589 , loss: 0.0026739177\n",
            "iter: 590 , loss: 0.002671713\n",
            "iter: 591 , loss: 0.0026695172\n",
            "iter: 592 , loss: 0.0026673307\n",
            "iter: 593 , loss: 0.0026651507\n",
            "iter: 594 , loss: 0.0026629793\n",
            "iter: 595 , loss: 0.0026608182\n",
            "iter: 596 , loss: 0.0026586652\n",
            "iter: 597 , loss: 0.0026565185\n",
            "iter: 598 , loss: 0.0026543771\n",
            "iter: 599 , loss: 0.002652243\n",
            "iter: 600 , loss: 0.0026501212\n",
            "iter: 601 , loss: 0.0026480092\n",
            "iter: 602 , loss: 0.0026459054\n",
            "iter: 603 , loss: 0.0026438115\n",
            "iter: 604 , loss: 0.0026417258\n",
            "iter: 605 , loss: 0.0026396469\n",
            "iter: 606 , loss: 0.0026375735\n",
            "iter: 607 , loss: 0.0026355109\n",
            "iter: 608 , loss: 0.0026334536\n",
            "iter: 609 , loss: 0.0026314033\n",
            "iter: 610 , loss: 0.0026293588\n",
            "iter: 611 , loss: 0.0026273227\n",
            "iter: 612 , loss: 0.002625297\n",
            "iter: 613 , loss: 0.0026232768\n",
            "iter: 614 , loss: 0.0026212628\n",
            "iter: 615 , loss: 0.0026192567\n",
            "iter: 616 , loss: 0.0026172572\n",
            "iter: 617 , loss: 0.0026152644\n",
            "iter: 618 , loss: 0.0026132793\n",
            "iter: 619 , loss: 0.0026112993\n",
            "iter: 620 , loss: 0.0026093256\n",
            "iter: 621 , loss: 0.0026073589\n",
            "iter: 622 , loss: 0.0026053987\n",
            "iter: 623 , loss: 0.0026034475\n",
            "iter: 624 , loss: 0.0026015032\n",
            "iter: 625 , loss: 0.0025995655\n",
            "iter: 626 , loss: 0.0025976335\n",
            "iter: 627 , loss: 0.0025957087\n",
            "iter: 628 , loss: 0.0025937897\n",
            "iter: 629 , loss: 0.002591877\n",
            "iter: 630 , loss: 0.0025899715\n",
            "iter: 631 , loss: 0.002588074\n",
            "iter: 632 , loss: 0.002586183\n",
            "iter: 633 , loss: 0.0025842986\n",
            "iter: 634 , loss: 0.0025824197\n",
            "iter: 635 , loss: 0.0025805407\n",
            "iter: 636 , loss: 0.0025786704\n",
            "iter: 637 , loss: 0.002576803\n",
            "iter: 638 , loss: 0.0025749453\n",
            "iter: 639 , loss: 0.0025730964\n",
            "iter: 640 , loss: 0.002571254\n",
            "iter: 641 , loss: 0.0025694198\n",
            "iter: 642 , loss: 0.0025675932\n",
            "iter: 643 , loss: 0.002565772\n",
            "iter: 644 , loss: 0.002563957\n",
            "iter: 645 , loss: 0.0025621483\n",
            "iter: 646 , loss: 0.0025603469\n",
            "iter: 647 , loss: 0.0025585494\n",
            "iter: 648 , loss: 0.0025567585\n",
            "iter: 649 , loss: 0.002554973\n",
            "iter: 650 , loss: 0.0025531931\n",
            "iter: 651 , loss: 0.0025514185\n",
            "iter: 652 , loss: 0.00254965\n",
            "iter: 653 , loss: 0.0025478837\n",
            "iter: 654 , loss: 0.0025461256\n",
            "iter: 655 , loss: 0.0025443723\n",
            "iter: 656 , loss: 0.0025426256\n",
            "iter: 657 , loss: 0.0025408843\n",
            "iter: 658 , loss: 0.0025391467\n",
            "iter: 659 , loss: 0.0025374165\n",
            "iter: 660 , loss: 0.002535691\n",
            "iter: 661 , loss: 0.0025339718\n",
            "iter: 662 , loss: 0.0025322577\n",
            "iter: 663 , loss: 0.0025305501\n",
            "iter: 664 , loss: 0.002528849\n",
            "iter: 665 , loss: 0.0025271534\n",
            "iter: 666 , loss: 0.0025254618\n",
            "iter: 667 , loss: 0.002523776\n",
            "iter: 668 , loss: 0.0025220949\n",
            "iter: 669 , loss: 0.0025204187\n",
            "iter: 670 , loss: 0.002518748\n",
            "iter: 671 , loss: 0.0025170827\n",
            "iter: 672 , loss: 0.0025154222\n",
            "iter: 673 , loss: 0.0025137684\n",
            "iter: 674 , loss: 0.0025121183\n",
            "iter: 675 , loss: 0.0025104745\n",
            "iter: 676 , loss: 0.002508835\n",
            "iter: 677 , loss: 0.0025072\n",
            "iter: 678 , loss: 0.002505569\n",
            "iter: 679 , loss: 0.002503943\n",
            "iter: 680 , loss: 0.0025023227\n",
            "iter: 681 , loss: 0.0025007075\n",
            "iter: 682 , loss: 0.0024990968\n",
            "iter: 683 , loss: 0.0024974903\n",
            "iter: 684 , loss: 0.0024958886\n",
            "iter: 685 , loss: 0.0024942923\n",
            "iter: 686 , loss: 0.0024927002\n",
            "iter: 687 , loss: 0.0024911095\n",
            "iter: 688 , loss: 0.002489524\n",
            "iter: 689 , loss: 0.002487942\n",
            "iter: 690 , loss: 0.0024863663\n",
            "iter: 691 , loss: 0.002484792\n",
            "iter: 692 , loss: 0.002483222\n",
            "iter: 693 , loss: 0.0024816561\n",
            "iter: 694 , loss: 0.002480096\n",
            "iter: 695 , loss: 0.0024785392\n",
            "iter: 696 , loss: 0.002476984\n",
            "iter: 697 , loss: 0.0024754326\n",
            "iter: 698 , loss: 0.0024738845\n",
            "iter: 699 , loss: 0.0024723415\n",
            "iter: 700 , loss: 0.0024708025\n",
            "iter: 701 , loss: 0.0024692642\n",
            "iter: 702 , loss: 0.0024677329\n",
            "iter: 703 , loss: 0.0024662018\n",
            "iter: 704 , loss: 0.0024646746\n",
            "iter: 705 , loss: 0.0024631508\n",
            "iter: 706 , loss: 0.0024616309\n",
            "iter: 707 , loss: 0.0024601158\n",
            "iter: 708 , loss: 0.0024586043\n",
            "iter: 709 , loss: 0.002457099\n",
            "iter: 710 , loss: 0.002455597\n",
            "iter: 711 , loss: 0.0024541\n",
            "iter: 712 , loss: 0.0024526073\n",
            "iter: 713 , loss: 0.0024511185\n",
            "iter: 714 , loss: 0.0024496333\n",
            "iter: 715 , loss: 0.0024481516\n",
            "iter: 716 , loss: 0.0024466736\n",
            "iter: 717 , loss: 0.0024451974\n",
            "iter: 718 , loss: 0.0024437262\n",
            "iter: 719 , loss: 0.0024422593\n",
            "iter: 720 , loss: 0.0024407955\n",
            "iter: 721 , loss: 0.0024393352\n",
            "iter: 722 , loss: 0.0024378793\n",
            "iter: 723 , loss: 0.002436428\n",
            "iter: 724 , loss: 0.0024349818\n",
            "iter: 725 , loss: 0.002433538\n",
            "iter: 726 , loss: 0.0024320967\n",
            "iter: 727 , loss: 0.0024306609\n",
            "iter: 728 , loss: 0.002429227\n",
            "iter: 729 , loss: 0.0024277968\n",
            "iter: 730 , loss: 0.002426368\n",
            "iter: 731 , loss: 0.0024249437\n",
            "iter: 732 , loss: 0.002423522\n",
            "iter: 733 , loss: 0.0024221041\n",
            "iter: 734 , loss: 0.0024206867\n",
            "iter: 735 , loss: 0.0024192734\n",
            "iter: 736 , loss: 0.0024178645\n",
            "iter: 737 , loss: 0.0024164577\n",
            "iter: 738 , loss: 0.002415056\n",
            "iter: 739 , loss: 0.0024136582\n",
            "iter: 740 , loss: 0.0024122645\n",
            "iter: 741 , loss: 0.002410873\n",
            "iter: 742 , loss: 0.0024094835\n",
            "iter: 743 , loss: 0.002408097\n",
            "iter: 744 , loss: 0.0024067133\n",
            "iter: 745 , loss: 0.0024053338\n",
            "iter: 746 , loss: 0.0024039596\n",
            "iter: 747 , loss: 0.002402588\n",
            "iter: 748 , loss: 0.0024012204\n",
            "iter: 749 , loss: 0.002399855\n",
            "iter: 750 , loss: 0.0023984907\n",
            "iter: 751 , loss: 0.0023971302\n",
            "iter: 752 , loss: 0.0023957759\n",
            "iter: 753 , loss: 0.0023944254\n",
            "iter: 754 , loss: 0.0023930795\n",
            "iter: 755 , loss: 0.0023917344\n",
            "iter: 756 , loss: 0.0023903898\n",
            "iter: 757 , loss: 0.0023890515\n",
            "iter: 758 , loss: 0.0023877178\n",
            "iter: 759 , loss: 0.0023863837\n",
            "iter: 760 , loss: 0.0023850568\n",
            "iter: 761 , loss: 0.0023837353\n",
            "iter: 762 , loss: 0.0023824177\n",
            "iter: 763 , loss: 0.002381103\n",
            "iter: 764 , loss: 0.0023797925\n",
            "iter: 765 , loss: 0.0023784863\n",
            "iter: 766 , loss: 0.002377183\n",
            "iter: 767 , loss: 0.0023758847\n",
            "iter: 768 , loss: 0.0023745887\n",
            "iter: 769 , loss: 0.0023732958\n",
            "iter: 770 , loss: 0.0023720055\n",
            "iter: 771 , loss: 0.0023707182\n",
            "iter: 772 , loss: 0.0023694343\n",
            "iter: 773 , loss: 0.0023681547\n",
            "iter: 774 , loss: 0.0023668774\n",
            "iter: 775 , loss: 0.0023656036\n",
            "iter: 776 , loss: 0.0023643307\n",
            "iter: 777 , loss: 0.0023630643\n",
            "iter: 778 , loss: 0.0023618008\n",
            "iter: 779 , loss: 0.00236054\n",
            "iter: 780 , loss: 0.0023592804\n",
            "iter: 781 , loss: 0.002358026\n",
            "iter: 782 , loss: 0.0023567753\n",
            "iter: 783 , loss: 0.0023555271\n",
            "iter: 784 , loss: 0.0023542827\n",
            "iter: 785 , loss: 0.0023530405\n",
            "iter: 786 , loss: 0.002351802\n",
            "iter: 787 , loss: 0.0023505671\n",
            "iter: 788 , loss: 0.002349333\n",
            "iter: 789 , loss: 0.002348103\n",
            "iter: 790 , loss: 0.0023468763\n",
            "iter: 791 , loss: 0.002345653\n",
            "iter: 792 , loss: 0.0023444318\n",
            "iter: 793 , loss: 0.0023432143\n",
            "iter: 794 , loss: 0.0023419994\n",
            "iter: 795 , loss: 0.0023407866\n",
            "iter: 796 , loss: 0.002339578\n",
            "iter: 797 , loss: 0.0023383715\n",
            "iter: 798 , loss: 0.0023371673\n",
            "iter: 799 , loss: 0.0023359659\n",
            "iter: 800 , loss: 0.0023347677\n",
            "iter: 801 , loss: 0.0023335728\n",
            "iter: 802 , loss: 0.0023323805\n",
            "iter: 803 , loss: 0.0023311907\n",
            "iter: 804 , loss: 0.002330004\n",
            "iter: 805 , loss: 0.0023288191\n",
            "iter: 806 , loss: 0.0023276377\n",
            "iter: 807 , loss: 0.0023264554\n",
            "iter: 808 , loss: 0.0023252792\n",
            "iter: 809 , loss: 0.0023241055\n",
            "iter: 810 , loss: 0.0023229339\n",
            "iter: 811 , loss: 0.0023217655\n",
            "iter: 812 , loss: 0.0023205983\n",
            "iter: 813 , loss: 0.0023194344\n",
            "iter: 814 , loss: 0.0023182735\n",
            "iter: 815 , loss: 0.0023171138\n",
            "iter: 816 , loss: 0.0023159576\n",
            "iter: 817 , loss: 0.0023148041\n",
            "iter: 818 , loss: 0.0023136544\n",
            "iter: 819 , loss: 0.002312506\n",
            "iter: 820 , loss: 0.0023113606\n",
            "iter: 821 , loss: 0.0023102185\n",
            "iter: 822 , loss: 0.0023090795\n",
            "iter: 823 , loss: 0.0023079407\n",
            "iter: 824 , loss: 0.002306805\n",
            "iter: 825 , loss: 0.0023056737\n",
            "iter: 826 , loss: 0.0023045444\n",
            "iter: 827 , loss: 0.0023034173\n",
            "iter: 828 , loss: 0.0023022932\n",
            "iter: 829 , loss: 0.0023011717\n",
            "iter: 830 , loss: 0.0023000517\n",
            "iter: 831 , loss: 0.0022989358\n",
            "iter: 832 , loss: 0.0022978226\n",
            "iter: 833 , loss: 0.0022967106\n",
            "iter: 834 , loss: 0.002295602\n",
            "iter: 835 , loss: 0.0022944966\n",
            "iter: 836 , loss: 0.0022933912\n",
            "iter: 837 , loss: 0.0022922882\n",
            "iter: 838 , loss: 0.002291189\n",
            "iter: 839 , loss: 0.0022900915\n",
            "iter: 840 , loss: 0.002288997\n",
            "iter: 841 , loss: 0.0022879052\n",
            "iter: 842 , loss: 0.0022868153\n",
            "iter: 843 , loss: 0.0022857273\n",
            "iter: 844 , loss: 0.0022846418\n",
            "iter: 845 , loss: 0.0022835594\n",
            "iter: 846 , loss: 0.0022824786\n",
            "iter: 847 , loss: 0.002281401\n",
            "iter: 848 , loss: 0.0022803254\n",
            "iter: 849 , loss: 0.002279252\n",
            "iter: 850 , loss: 0.0022781817\n",
            "iter: 851 , loss: 0.0022771142\n",
            "iter: 852 , loss: 0.002276048\n",
            "iter: 853 , loss: 0.0022749857\n",
            "iter: 854 , loss: 0.0022739256\n",
            "iter: 855 , loss: 0.0022728674\n",
            "iter: 856 , loss: 0.0022718124\n",
            "iter: 857 , loss: 0.0022707602\n",
            "iter: 858 , loss: 0.0022697102\n",
            "iter: 859 , loss: 0.0022686615\n",
            "iter: 860 , loss: 0.0022676166\n",
            "iter: 861 , loss: 0.0022665747\n",
            "iter: 862 , loss: 0.0022655334\n",
            "iter: 863 , loss: 0.002264495\n",
            "iter: 864 , loss: 0.0022634596\n",
            "iter: 865 , loss: 0.002262426\n",
            "iter: 866 , loss: 0.0022613944\n",
            "iter: 867 , loss: 0.0022603662\n",
            "iter: 868 , loss: 0.0022593385\n",
            "iter: 869 , loss: 0.0022583138\n",
            "iter: 870 , loss: 0.0022572926\n",
            "iter: 871 , loss: 0.0022562721\n",
            "iter: 872 , loss: 0.0022552556\n",
            "iter: 873 , loss: 0.002254241\n",
            "iter: 874 , loss: 0.002253227\n",
            "iter: 875 , loss: 0.0022522153\n",
            "iter: 876 , loss: 0.002251208\n",
            "iter: 877 , loss: 0.0022502039\n",
            "iter: 878 , loss: 0.0022491994\n",
            "iter: 879 , loss: 0.0022481969\n",
            "iter: 880 , loss: 0.0022471976\n",
            "iter: 881 , loss: 0.0022461999\n",
            "iter: 882 , loss: 0.0022452034\n",
            "iter: 883 , loss: 0.002244209\n",
            "iter: 884 , loss: 0.0022432178\n",
            "iter: 885 , loss: 0.0022422285\n",
            "iter: 886 , loss: 0.0022412415\n",
            "iter: 887 , loss: 0.0022402576\n",
            "iter: 888 , loss: 0.0022392748\n",
            "iter: 889 , loss: 0.0022382934\n",
            "iter: 890 , loss: 0.0022373132\n",
            "iter: 891 , loss: 0.0022363362\n",
            "iter: 892 , loss: 0.0022353614\n",
            "iter: 893 , loss: 0.00223439\n",
            "iter: 894 , loss: 0.00223342\n",
            "iter: 895 , loss: 0.002232454\n",
            "iter: 896 , loss: 0.002231489\n",
            "iter: 897 , loss: 0.0022305262\n",
            "iter: 898 , loss: 0.0022295644\n",
            "iter: 899 , loss: 0.0022286056\n",
            "iter: 900 , loss: 0.0022276503\n",
            "iter: 901 , loss: 0.0022266963\n",
            "iter: 902 , loss: 0.002225746\n",
            "iter: 903 , loss: 0.0022247983\n",
            "iter: 904 , loss: 0.0022238516\n",
            "iter: 905 , loss: 0.002222909\n",
            "iter: 906 , loss: 0.0022219678\n",
            "iter: 907 , loss: 0.0022210279\n",
            "iter: 908 , loss: 0.0022200895\n",
            "iter: 909 , loss: 0.0022191517\n",
            "iter: 910 , loss: 0.002218216\n",
            "iter: 911 , loss: 0.0022172837\n",
            "iter: 912 , loss: 0.0022163487\n",
            "iter: 913 , loss: 0.0022154197\n",
            "iter: 914 , loss: 0.002214493\n",
            "iter: 915 , loss: 0.0022135673\n",
            "iter: 916 , loss: 0.0022126425\n",
            "iter: 917 , loss: 0.00221172\n",
            "iter: 918 , loss: 0.0022107984\n",
            "iter: 919 , loss: 0.0022098774\n",
            "iter: 920 , loss: 0.0022089602\n",
            "iter: 921 , loss: 0.0022080443\n",
            "iter: 922 , loss: 0.0022071302\n",
            "iter: 923 , loss: 0.0022062168\n",
            "iter: 924 , loss: 0.002205305\n",
            "iter: 925 , loss: 0.0022043956\n",
            "iter: 926 , loss: 0.0022034887\n",
            "iter: 927 , loss: 0.0022025825\n",
            "iter: 928 , loss: 0.0022016785\n",
            "iter: 929 , loss: 0.0022007762\n",
            "iter: 930 , loss: 0.0021998743\n",
            "iter: 931 , loss: 0.0021989737\n",
            "iter: 932 , loss: 0.002198074\n",
            "iter: 933 , loss: 0.0021971795\n",
            "iter: 934 , loss: 0.0021962847\n",
            "iter: 935 , loss: 0.002195392\n",
            "iter: 936 , loss: 0.0021944998\n",
            "iter: 937 , loss: 0.0021936093\n",
            "iter: 938 , loss: 0.0021927222\n",
            "iter: 939 , loss: 0.0021918346\n",
            "iter: 940 , loss: 0.0021909473\n",
            "iter: 941 , loss: 0.0021900646\n",
            "iter: 942 , loss: 0.0021891822\n",
            "iter: 943 , loss: 0.002188301\n",
            "iter: 944 , loss: 0.002187423\n",
            "iter: 945 , loss: 0.0021865454\n",
            "iter: 946 , loss: 0.0021856716\n",
            "iter: 947 , loss: 0.0021847975\n",
            "iter: 948 , loss: 0.002183924\n",
            "iter: 949 , loss: 0.002183052\n",
            "iter: 950 , loss: 0.002182187\n",
            "iter: 951 , loss: 0.0021813188\n",
            "iter: 952 , loss: 0.0021804501\n",
            "iter: 953 , loss: 0.0021795845\n",
            "iter: 954 , loss: 0.0021787193\n",
            "iter: 955 , loss: 0.0021778555\n",
            "iter: 956 , loss: 0.0021769933\n",
            "iter: 957 , loss: 0.0021761341\n",
            "iter: 958 , loss: 0.002175275\n",
            "iter: 959 , loss: 0.0021744166\n",
            "iter: 960 , loss: 0.002173558\n",
            "iter: 961 , loss: 0.0021727046\n",
            "iter: 962 , loss: 0.0021718545\n",
            "iter: 963 , loss: 0.002171004\n",
            "iter: 964 , loss: 0.002170154\n",
            "iter: 965 , loss: 0.002169305\n",
            "iter: 966 , loss: 0.0021684568\n",
            "iter: 967 , loss: 0.0021676093\n",
            "iter: 968 , loss: 0.002166766\n",
            "iter: 969 , loss: 0.0021659262\n",
            "iter: 970 , loss: 0.0021650859\n",
            "iter: 971 , loss: 0.0021642502\n",
            "iter: 972 , loss: 0.0021634127\n",
            "iter: 973 , loss: 0.002162577\n",
            "iter: 974 , loss: 0.0021617436\n",
            "iter: 975 , loss: 0.002160914\n",
            "iter: 976 , loss: 0.0021600826\n",
            "iter: 977 , loss: 0.0021592537\n",
            "iter: 978 , loss: 0.0021584264\n",
            "iter: 979 , loss: 0.0021576032\n",
            "iter: 980 , loss: 0.0021567799\n",
            "iter: 981 , loss: 0.002155958\n",
            "iter: 982 , loss: 0.0021551398\n",
            "iter: 983 , loss: 0.002154322\n",
            "iter: 984 , loss: 0.0021535049\n",
            "iter: 985 , loss: 0.0021526916\n",
            "iter: 986 , loss: 0.0021518802\n",
            "iter: 987 , loss: 0.0021510667\n",
            "iter: 988 , loss: 0.00215026\n",
            "iter: 989 , loss: 0.0021494527\n",
            "iter: 990 , loss: 0.0021486478\n",
            "iter: 991 , loss: 0.0021478452\n",
            "iter: 992 , loss: 0.0021470448\n",
            "iter: 993 , loss: 0.0021462447\n",
            "iter: 994 , loss: 0.002145448\n",
            "iter: 995 , loss: 0.002144655\n",
            "iter: 996 , loss: 0.002143863\n",
            "iter: 997 , loss: 0.002143071\n",
            "iter: 998 , loss: 0.0021422815\n",
            "iter: 999 , loss: 0.0021414934\n",
            "iter: 1000 , loss: 0.002140708\n",
            "iter: 1001 , loss: 0.002139924\n",
            "iter: 1002 , loss: 0.0021391422\n",
            "iter: 1003 , loss: 0.0021383627\n",
            "iter: 1004 , loss: 0.002137584\n",
            "iter: 1005 , loss: 0.002136808\n",
            "iter: 1006 , loss: 0.0021360333\n",
            "iter: 1007 , loss: 0.002135259\n",
            "iter: 1008 , loss: 0.002134489\n",
            "iter: 1009 , loss: 0.0021337203\n",
            "iter: 1010 , loss: 0.0021329538\n",
            "iter: 1011 , loss: 0.0021321878\n",
            "iter: 1012 , loss: 0.002131423\n",
            "iter: 1013 , loss: 0.0021306581\n",
            "iter: 1014 , loss: 0.002129896\n",
            "iter: 1015 , loss: 0.002129134\n",
            "iter: 1016 , loss: 0.0021283745\n",
            "iter: 1017 , loss: 0.0021276164\n",
            "iter: 1018 , loss: 0.0021268604\n",
            "iter: 1019 , loss: 0.0021261084\n",
            "iter: 1020 , loss: 0.002125358\n",
            "iter: 1021 , loss: 0.0021246045\n",
            "iter: 1022 , loss: 0.0021238571\n",
            "iter: 1023 , loss: 0.0021231116\n",
            "iter: 1024 , loss: 0.0021223638\n",
            "iter: 1025 , loss: 0.0021216194\n",
            "iter: 1026 , loss: 0.0021208741\n",
            "iter: 1027 , loss: 0.0021201319\n",
            "iter: 1028 , loss: 0.0021193933\n",
            "iter: 1029 , loss: 0.0021186513\n",
            "iter: 1030 , loss: 0.002117912\n",
            "iter: 1031 , loss: 0.0021171751\n",
            "iter: 1032 , loss: 0.0021164382\n",
            "iter: 1033 , loss: 0.0021157041\n",
            "iter: 1034 , loss: 0.0021149707\n",
            "iter: 1035 , loss: 0.0021142422\n",
            "iter: 1036 , loss: 0.0021135118\n",
            "iter: 1037 , loss: 0.0021127807\n",
            "iter: 1038 , loss: 0.0021120526\n",
            "iter: 1039 , loss: 0.002111326\n",
            "iter: 1040 , loss: 0.0021106005\n",
            "iter: 1041 , loss: 0.0021098761\n",
            "iter: 1042 , loss: 0.0021091553\n",
            "iter: 1043 , loss: 0.0021084342\n",
            "iter: 1044 , loss: 0.0021077155\n",
            "iter: 1045 , loss: 0.0021069956\n",
            "iter: 1046 , loss: 0.0021062791\n",
            "iter: 1047 , loss: 0.0021055616\n",
            "iter: 1048 , loss: 0.0021048447\n",
            "iter: 1049 , loss: 0.0021041327\n",
            "iter: 1050 , loss: 0.0021034193\n",
            "iter: 1051 , loss: 0.0021027082\n",
            "iter: 1052 , loss: 0.0021019988\n",
            "iter: 1053 , loss: 0.002101291\n",
            "iter: 1054 , loss: 0.0021005836\n",
            "iter: 1055 , loss: 0.002099878\n",
            "iter: 1056 , loss: 0.0020991731\n",
            "iter: 1057 , loss: 0.0020984747\n",
            "iter: 1058 , loss: 0.0020977694\n",
            "iter: 1059 , loss: 0.00209707\n",
            "iter: 1060 , loss: 0.0020963708\n",
            "iter: 1061 , loss: 0.0020956716\n",
            "iter: 1062 , loss: 0.0020949787\n",
            "iter: 1063 , loss: 0.0020942832\n",
            "iter: 1064 , loss: 0.0020935896\n",
            "iter: 1065 , loss: 0.0020928953\n",
            "iter: 1066 , loss: 0.0020921999\n",
            "iter: 1067 , loss: 0.002091506\n",
            "iter: 1068 , loss: 0.0020908164\n",
            "iter: 1069 , loss: 0.002090126\n",
            "iter: 1070 , loss: 0.0020894401\n",
            "iter: 1071 , loss: 0.0020887537\n",
            "iter: 1072 , loss: 0.0020880662\n",
            "iter: 1073 , loss: 0.002087379\n",
            "iter: 1074 , loss: 0.002086692\n",
            "iter: 1075 , loss: 0.0020860091\n",
            "iter: 1076 , loss: 0.002085326\n",
            "iter: 1077 , loss: 0.0020846466\n",
            "iter: 1078 , loss: 0.0020839667\n",
            "iter: 1079 , loss: 0.0020832857\n",
            "iter: 1080 , loss: 0.0020826068\n",
            "iter: 1081 , loss: 0.0020819306\n",
            "iter: 1082 , loss: 0.002081253\n",
            "iter: 1083 , loss: 0.0020805756\n",
            "iter: 1084 , loss: 0.002079901\n",
            "iter: 1085 , loss: 0.0020792282\n",
            "iter: 1086 , loss: 0.0020785572\n",
            "iter: 1087 , loss: 0.0020778854\n",
            "iter: 1088 , loss: 0.0020772165\n",
            "iter: 1089 , loss: 0.0020765462\n",
            "iter: 1090 , loss: 0.002075874\n",
            "iter: 1091 , loss: 0.0020752053\n",
            "iter: 1092 , loss: 0.0020745408\n",
            "iter: 1093 , loss: 0.0020738782\n",
            "iter: 1094 , loss: 0.0020732149\n",
            "iter: 1095 , loss: 0.0020725518\n",
            "iter: 1096 , loss: 0.0020718905\n",
            "iter: 1097 , loss: 0.0020712276\n",
            "iter: 1098 , loss: 0.002070568\n",
            "iter: 1099 , loss: 0.0020699082\n",
            "iter: 1100 , loss: 0.002069251\n",
            "iter: 1101 , loss: 0.0020685976\n",
            "iter: 1102 , loss: 0.0020679438\n",
            "iter: 1103 , loss: 0.002067291\n",
            "iter: 1104 , loss: 0.0020666365\n",
            "iter: 1105 , loss: 0.002065983\n",
            "iter: 1106 , loss: 0.00206533\n",
            "iter: 1107 , loss: 0.002064682\n",
            "iter: 1108 , loss: 0.0020640353\n",
            "iter: 1109 , loss: 0.0020633838\n",
            "iter: 1110 , loss: 0.0020627368\n",
            "iter: 1111 , loss: 0.0020620858\n",
            "iter: 1112 , loss: 0.0020614383\n",
            "iter: 1113 , loss: 0.0020607924\n",
            "iter: 1114 , loss: 0.0020601503\n",
            "iter: 1115 , loss: 0.0020595076\n",
            "iter: 1116 , loss: 0.002058865\n",
            "iter: 1117 , loss: 0.00205822\n",
            "iter: 1118 , loss: 0.0020575786\n",
            "iter: 1119 , loss: 0.0020569395\n",
            "iter: 1120 , loss: 0.0020563\n",
            "iter: 1121 , loss: 0.0020556618\n",
            "iter: 1122 , loss: 0.0020550222\n",
            "iter: 1123 , loss: 0.0020543858\n",
            "iter: 1124 , loss: 0.0020537493\n",
            "iter: 1125 , loss: 0.002053112\n",
            "iter: 1126 , loss: 0.0020524794\n",
            "iter: 1127 , loss: 0.0020518422\n",
            "iter: 1128 , loss: 0.0020512051\n",
            "iter: 1129 , loss: 0.0020505728\n",
            "iter: 1130 , loss: 0.0020499406\n",
            "iter: 1131 , loss: 0.0020493069\n",
            "iter: 1132 , loss: 0.002048673\n",
            "iter: 1133 , loss: 0.0020480445\n",
            "iter: 1134 , loss: 0.002047417\n",
            "iter: 1135 , loss: 0.0020467895\n",
            "iter: 1136 , loss: 0.0020461627\n",
            "iter: 1137 , loss: 0.0020455369\n",
            "iter: 1138 , loss: 0.0020449108\n",
            "iter: 1139 , loss: 0.0020442887\n",
            "iter: 1140 , loss: 0.0020436656\n",
            "iter: 1141 , loss: 0.0020430428\n",
            "iter: 1142 , loss: 0.002042422\n",
            "iter: 1143 , loss: 0.0020418004\n",
            "iter: 1144 , loss: 0.0020411797\n",
            "iter: 1145 , loss: 0.0020405615\n",
            "iter: 1146 , loss: 0.0020399445\n",
            "iter: 1147 , loss: 0.0020393291\n",
            "iter: 1148 , loss: 0.0020387138\n",
            "iter: 1149 , loss: 0.0020380989\n",
            "iter: 1150 , loss: 0.0020374835\n",
            "iter: 1151 , loss: 0.0020368698\n",
            "iter: 1152 , loss: 0.002036256\n",
            "iter: 1153 , loss: 0.0020356444\n",
            "iter: 1154 , loss: 0.0020350334\n",
            "iter: 1155 , loss: 0.002034422\n",
            "iter: 1156 , loss: 0.0020338106\n",
            "iter: 1157 , loss: 0.0020331994\n",
            "iter: 1158 , loss: 0.0020325887\n",
            "iter: 1159 , loss: 0.0020319794\n",
            "iter: 1160 , loss: 0.0020313738\n",
            "iter: 1161 , loss: 0.0020307696\n",
            "iter: 1162 , loss: 0.0020301635\n",
            "iter: 1163 , loss: 0.0020295586\n",
            "iter: 1164 , loss: 0.0020289558\n",
            "iter: 1165 , loss: 0.002028352\n",
            "iter: 1166 , loss: 0.0020277477\n",
            "iter: 1167 , loss: 0.0020271444\n",
            "iter: 1168 , loss: 0.002026542\n",
            "iter: 1169 , loss: 0.002025938\n",
            "iter: 1170 , loss: 0.002025335\n",
            "iter: 1171 , loss: 0.0020247311\n",
            "iter: 1172 , loss: 0.002024133\n",
            "iter: 1173 , loss: 0.002023537\n",
            "iter: 1174 , loss: 0.0020229393\n",
            "iter: 1175 , loss: 0.0020223418\n",
            "iter: 1176 , loss: 0.0020217416\n",
            "iter: 1177 , loss: 0.0020211479\n",
            "iter: 1178 , loss: 0.0020205546\n",
            "iter: 1179 , loss: 0.0020199611\n",
            "iter: 1180 , loss: 0.002019368\n",
            "iter: 1181 , loss: 0.002018774\n",
            "iter: 1182 , loss: 0.0020181825\n",
            "iter: 1183 , loss: 0.0020175905\n",
            "iter: 1184 , loss: 0.002017\n",
            "iter: 1185 , loss: 0.0020164116\n",
            "iter: 1186 , loss: 0.0020158212\n",
            "iter: 1187 , loss: 0.0020152337\n",
            "iter: 1188 , loss: 0.002014648\n",
            "iter: 1189 , loss: 0.0020140628\n",
            "iter: 1190 , loss: 0.0020134768\n",
            "iter: 1191 , loss: 0.002012894\n",
            "iter: 1192 , loss: 0.002012305\n",
            "iter: 1193 , loss: 0.0020117231\n",
            "iter: 1194 , loss: 0.0020111427\n",
            "iter: 1195 , loss: 0.0020105601\n",
            "iter: 1196 , loss: 0.0020099774\n",
            "iter: 1197 , loss: 0.002009399\n",
            "iter: 1198 , loss: 0.0020088216\n",
            "iter: 1199 , loss: 0.002008243\n",
            "iter: 1200 , loss: 0.0020076658\n",
            "iter: 1201 , loss: 0.0020070896\n",
            "iter: 1202 , loss: 0.0020065121\n",
            "iter: 1203 , loss: 0.0020059373\n",
            "iter: 1204 , loss: 0.0020053627\n",
            "iter: 1205 , loss: 0.002004788\n",
            "iter: 1206 , loss: 0.0020042106\n",
            "iter: 1207 , loss: 0.0020036367\n",
            "iter: 1208 , loss: 0.002003065\n",
            "iter: 1209 , loss: 0.002002494\n",
            "iter: 1210 , loss: 0.0020019212\n",
            "iter: 1211 , loss: 0.0020013526\n",
            "iter: 1212 , loss: 0.002000782\n",
            "iter: 1213 , loss: 0.0020002106\n",
            "iter: 1214 , loss: 0.0019996415\n",
            "iter: 1215 , loss: 0.0019990734\n",
            "iter: 1216 , loss: 0.0019985083\n",
            "iter: 1217 , loss: 0.0019979437\n",
            "iter: 1218 , loss: 0.001997375\n",
            "iter: 1219 , loss: 0.0019968117\n",
            "iter: 1220 , loss: 0.0019962473\n",
            "iter: 1221 , loss: 0.0019956839\n",
            "iter: 1222 , loss: 0.0019951204\n",
            "iter: 1223 , loss: 0.0019945558\n",
            "iter: 1224 , loss: 0.001993996\n",
            "iter: 1225 , loss: 0.0019934368\n",
            "iter: 1226 , loss: 0.0019928778\n",
            "iter: 1227 , loss: 0.0019923158\n",
            "iter: 1228 , loss: 0.001991759\n",
            "iter: 1229 , loss: 0.0019912026\n",
            "iter: 1230 , loss: 0.0019906417\n",
            "iter: 1231 , loss: 0.0019900878\n",
            "iter: 1232 , loss: 0.0019895344\n",
            "iter: 1233 , loss: 0.0019889772\n",
            "iter: 1234 , loss: 0.0019884272\n",
            "iter: 1235 , loss: 0.0019878729\n",
            "iter: 1236 , loss: 0.001987324\n",
            "iter: 1237 , loss: 0.001986776\n",
            "iter: 1238 , loss: 0.0019862256\n",
            "iter: 1239 , loss: 0.0019856805\n",
            "iter: 1240 , loss: 0.0019851367\n",
            "iter: 1241 , loss: 0.00198459\n",
            "iter: 1242 , loss: 0.0019840454\n",
            "iter: 1243 , loss: 0.0019834999\n",
            "iter: 1244 , loss: 0.0019829557\n",
            "iter: 1245 , loss: 0.0019824114\n",
            "iter: 1246 , loss: 0.0019818672\n",
            "iter: 1247 , loss: 0.0019813252\n",
            "iter: 1248 , loss: 0.001980785\n",
            "iter: 1249 , loss: 0.0019802449\n",
            "iter: 1250 , loss: 0.001979709\n",
            "iter: 1251 , loss: 0.001979165\n",
            "iter: 1252 , loss: 0.001978628\n",
            "iter: 1253 , loss: 0.00197809\n",
            "iter: 1254 , loss: 0.0019775578\n",
            "iter: 1255 , loss: 0.0019770192\n",
            "iter: 1256 , loss: 0.0019764865\n",
            "iter: 1257 , loss: 0.0019759543\n",
            "iter: 1258 , loss: 0.001975422\n",
            "iter: 1259 , loss: 0.0019748937\n",
            "iter: 1260 , loss: 0.0019743657\n",
            "iter: 1261 , loss: 0.001973834\n",
            "iter: 1262 , loss: 0.001973309\n",
            "iter: 1263 , loss: 0.0019727808\n",
            "iter: 1264 , loss: 0.0019722555\n",
            "iter: 1265 , loss: 0.0019717312\n",
            "iter: 1266 , loss: 0.0019712083\n",
            "iter: 1267 , loss: 0.001970676\n",
            "iter: 1268 , loss: 0.0019701512\n",
            "iter: 1269 , loss: 0.0019696252\n",
            "iter: 1270 , loss: 0.0019690997\n",
            "iter: 1271 , loss: 0.0019685782\n",
            "iter: 1272 , loss: 0.0019680532\n",
            "iter: 1273 , loss: 0.0019675288\n",
            "iter: 1274 , loss: 0.0019670061\n",
            "iter: 1275 , loss: 0.0019664795\n",
            "iter: 1276 , loss: 0.001965958\n",
            "iter: 1277 , loss: 0.0019654327\n",
            "iter: 1278 , loss: 0.0019649107\n",
            "iter: 1279 , loss: 0.0019643975\n",
            "iter: 1280 , loss: 0.0019638748\n",
            "iter: 1281 , loss: 0.0019633598\n",
            "iter: 1282 , loss: 0.0019628399\n",
            "iter: 1283 , loss: 0.0019623223\n",
            "iter: 1284 , loss: 0.001961805\n",
            "iter: 1285 , loss: 0.0019612869\n",
            "iter: 1286 , loss: 0.001960776\n",
            "iter: 1287 , loss: 0.0019602668\n",
            "iter: 1288 , loss: 0.0019597532\n",
            "iter: 1289 , loss: 0.001959237\n",
            "iter: 1290 , loss: 0.0019587304\n",
            "iter: 1291 , loss: 0.0019582117\n",
            "iter: 1292 , loss: 0.0019577055\n",
            "iter: 1293 , loss: 0.0019571984\n",
            "iter: 1294 , loss: 0.0019566892\n",
            "iter: 1295 , loss: 0.0019561774\n",
            "iter: 1296 , loss: 0.0019556752\n",
            "iter: 1297 , loss: 0.0019551679\n",
            "iter: 1298 , loss: 0.0019546612\n",
            "iter: 1299 , loss: 0.0019541522\n",
            "iter: 1300 , loss: 0.0019536524\n",
            "iter: 1301 , loss: 0.0019531485\n",
            "iter: 1302 , loss: 0.0019526414\n",
            "iter: 1303 , loss: 0.0019521391\n",
            "iter: 1304 , loss: 0.001951634\n",
            "iter: 1305 , loss: 0.0019511301\n",
            "iter: 1306 , loss: 0.0019506231\n",
            "iter: 1307 , loss: 0.0019501144\n",
            "iter: 1308 , loss: 0.0019496079\n",
            "iter: 1309 , loss: 0.001949104\n",
            "iter: 1310 , loss: 0.0019486062\n",
            "iter: 1311 , loss: 0.0019481091\n",
            "iter: 1312 , loss: 0.0019476131\n",
            "iter: 1313 , loss: 0.0019471205\n",
            "iter: 1314 , loss: 0.0019466215\n",
            "iter: 1315 , loss: 0.0019461277\n",
            "iter: 1316 , loss: 0.0019456315\n",
            "iter: 1317 , loss: 0.0019451332\n",
            "iter: 1318 , loss: 0.0019446409\n",
            "iter: 1319 , loss: 0.0019441508\n",
            "iter: 1320 , loss: 0.001943664\n",
            "iter: 1321 , loss: 0.0019431695\n",
            "iter: 1322 , loss: 0.0019426814\n",
            "iter: 1323 , loss: 0.0019421957\n",
            "iter: 1324 , loss: 0.001941709\n",
            "iter: 1325 , loss: 0.001941217\n",
            "iter: 1326 , loss: 0.0019407319\n",
            "iter: 1327 , loss: 0.001940242\n",
            "iter: 1328 , loss: 0.0019397532\n",
            "iter: 1329 , loss: 0.001939267\n",
            "iter: 1330 , loss: 0.0019387801\n",
            "iter: 1331 , loss: 0.001938299\n",
            "iter: 1332 , loss: 0.0019378077\n",
            "iter: 1333 , loss: 0.0019373284\n",
            "iter: 1334 , loss: 0.0019368402\n",
            "iter: 1335 , loss: 0.0019363657\n",
            "iter: 1336 , loss: 0.0019358809\n",
            "iter: 1337 , loss: 0.0019353959\n",
            "iter: 1338 , loss: 0.0019349158\n",
            "iter: 1339 , loss: 0.0019344323\n",
            "iter: 1340 , loss: 0.0019339568\n",
            "iter: 1341 , loss: 0.0019334774\n",
            "iter: 1342 , loss: 0.0019329957\n",
            "iter: 1343 , loss: 0.0019325175\n",
            "iter: 1344 , loss: 0.0019320447\n",
            "iter: 1345 , loss: 0.0019315648\n",
            "iter: 1346 , loss: 0.0019310861\n",
            "iter: 1347 , loss: 0.0019306095\n",
            "iter: 1348 , loss: 0.0019301309\n",
            "iter: 1349 , loss: 0.0019296546\n",
            "iter: 1350 , loss: 0.0019291801\n",
            "iter: 1351 , loss: 0.0019287071\n",
            "iter: 1352 , loss: 0.0019282312\n",
            "iter: 1353 , loss: 0.0019277615\n",
            "iter: 1354 , loss: 0.0019272898\n",
            "iter: 1355 , loss: 0.001926819\n",
            "iter: 1356 , loss: 0.0019263478\n",
            "iter: 1357 , loss: 0.0019258782\n",
            "iter: 1358 , loss: 0.0019254088\n",
            "iter: 1359 , loss: 0.001924938\n",
            "iter: 1360 , loss: 0.0019244713\n",
            "iter: 1361 , loss: 0.0019240048\n",
            "iter: 1362 , loss: 0.0019235376\n",
            "iter: 1363 , loss: 0.00192307\n",
            "iter: 1364 , loss: 0.0019226026\n",
            "iter: 1365 , loss: 0.0019221373\n",
            "iter: 1366 , loss: 0.0019216756\n",
            "iter: 1367 , loss: 0.0019212163\n",
            "iter: 1368 , loss: 0.001920747\n",
            "iter: 1369 , loss: 0.0019202867\n",
            "iter: 1370 , loss: 0.0019198225\n",
            "iter: 1371 , loss: 0.001919361\n",
            "iter: 1372 , loss: 0.0019189002\n",
            "iter: 1373 , loss: 0.0019184398\n",
            "iter: 1374 , loss: 0.0019179828\n",
            "iter: 1375 , loss: 0.0019175189\n",
            "iter: 1376 , loss: 0.0019170619\n",
            "iter: 1377 , loss: 0.0019166047\n",
            "iter: 1378 , loss: 0.0019161399\n",
            "iter: 1379 , loss: 0.0019156808\n",
            "iter: 1380 , loss: 0.0019152239\n",
            "iter: 1381 , loss: 0.0019147677\n",
            "iter: 1382 , loss: 0.0019143085\n",
            "iter: 1383 , loss: 0.001913853\n",
            "iter: 1384 , loss: 0.0019133933\n",
            "iter: 1385 , loss: 0.0019129362\n",
            "iter: 1386 , loss: 0.0019124759\n",
            "iter: 1387 , loss: 0.0019120182\n",
            "iter: 1388 , loss: 0.0019115692\n",
            "iter: 1389 , loss: 0.0019111126\n",
            "iter: 1390 , loss: 0.0019106587\n",
            "iter: 1391 , loss: 0.0019102072\n",
            "iter: 1392 , loss: 0.0019097552\n",
            "iter: 1393 , loss: 0.0019093022\n",
            "iter: 1394 , loss: 0.0019088506\n",
            "iter: 1395 , loss: 0.0019084017\n",
            "iter: 1396 , loss: 0.0019079555\n",
            "iter: 1397 , loss: 0.0019075039\n",
            "iter: 1398 , loss: 0.0019070568\n",
            "iter: 1399 , loss: 0.0019066059\n",
            "iter: 1400 , loss: 0.0019061656\n",
            "iter: 1401 , loss: 0.0019057175\n",
            "iter: 1402 , loss: 0.001905273\n",
            "iter: 1403 , loss: 0.0019048296\n",
            "iter: 1404 , loss: 0.0019043802\n",
            "iter: 1405 , loss: 0.0019039379\n",
            "iter: 1406 , loss: 0.0019034962\n",
            "iter: 1407 , loss: 0.0019030524\n",
            "iter: 1408 , loss: 0.0019026119\n",
            "iter: 1409 , loss: 0.0019021668\n",
            "iter: 1410 , loss: 0.0019017278\n",
            "iter: 1411 , loss: 0.0019012885\n",
            "iter: 1412 , loss: 0.0019008539\n",
            "iter: 1413 , loss: 0.0019004147\n",
            "iter: 1414 , loss: 0.0018999774\n",
            "iter: 1415 , loss: 0.0018995387\n",
            "iter: 1416 , loss: 0.0018990992\n",
            "iter: 1417 , loss: 0.001898659\n",
            "iter: 1418 , loss: 0.0018982238\n",
            "iter: 1419 , loss: 0.0018977864\n",
            "iter: 1420 , loss: 0.0018973501\n",
            "iter: 1421 , loss: 0.0018969193\n",
            "iter: 1422 , loss: 0.0018964808\n",
            "iter: 1423 , loss: 0.0018960445\n",
            "iter: 1424 , loss: 0.0018956105\n",
            "iter: 1425 , loss: 0.0018951731\n",
            "iter: 1426 , loss: 0.0018947369\n",
            "iter: 1427 , loss: 0.0018943029\n",
            "iter: 1428 , loss: 0.0018938704\n",
            "iter: 1429 , loss: 0.0018934375\n",
            "iter: 1430 , loss: 0.0018930061\n",
            "iter: 1431 , loss: 0.0018925751\n",
            "iter: 1432 , loss: 0.0018921399\n",
            "iter: 1433 , loss: 0.0018917094\n",
            "iter: 1434 , loss: 0.0018912824\n",
            "iter: 1435 , loss: 0.0018908539\n",
            "iter: 1436 , loss: 0.0018904246\n",
            "iter: 1437 , loss: 0.0018899966\n",
            "iter: 1438 , loss: 0.001889567\n",
            "iter: 1439 , loss: 0.0018891405\n",
            "iter: 1440 , loss: 0.001888718\n",
            "iter: 1441 , loss: 0.0018882938\n",
            "iter: 1442 , loss: 0.001887864\n",
            "iter: 1443 , loss: 0.0018874365\n",
            "iter: 1444 , loss: 0.0018870111\n",
            "iter: 1445 , loss: 0.001886585\n",
            "iter: 1446 , loss: 0.0018861601\n",
            "iter: 1447 , loss: 0.0018857362\n",
            "iter: 1448 , loss: 0.0018853156\n",
            "iter: 1449 , loss: 0.001884892\n",
            "iter: 1450 , loss: 0.0018844672\n",
            "iter: 1451 , loss: 0.0018840431\n",
            "iter: 1452 , loss: 0.0018836242\n",
            "iter: 1453 , loss: 0.0018831975\n",
            "iter: 1454 , loss: 0.0018827779\n",
            "iter: 1455 , loss: 0.0018823547\n",
            "iter: 1456 , loss: 0.0018819345\n",
            "iter: 1457 , loss: 0.0018815162\n",
            "iter: 1458 , loss: 0.0018810972\n",
            "iter: 1459 , loss: 0.001880674\n",
            "iter: 1460 , loss: 0.0018802534\n",
            "iter: 1461 , loss: 0.0018798359\n",
            "iter: 1462 , loss: 0.0018794135\n",
            "iter: 1463 , loss: 0.0018789953\n",
            "iter: 1464 , loss: 0.0018785833\n",
            "iter: 1465 , loss: 0.001878167\n",
            "iter: 1466 , loss: 0.0018777475\n",
            "iter: 1467 , loss: 0.0018773305\n",
            "iter: 1468 , loss: 0.0018769177\n",
            "iter: 1469 , loss: 0.0018765064\n",
            "iter: 1470 , loss: 0.0018760908\n",
            "iter: 1471 , loss: 0.0018756803\n",
            "iter: 1472 , loss: 0.0018752691\n",
            "iter: 1473 , loss: 0.0018748553\n",
            "iter: 1474 , loss: 0.001874443\n",
            "iter: 1475 , loss: 0.0018740263\n",
            "iter: 1476 , loss: 0.0018736173\n",
            "iter: 1477 , loss: 0.001873205\n",
            "iter: 1478 , loss: 0.0018727899\n",
            "iter: 1479 , loss: 0.0018723807\n",
            "iter: 1480 , loss: 0.0018719704\n",
            "iter: 1481 , loss: 0.0018715614\n",
            "iter: 1482 , loss: 0.0018711548\n",
            "iter: 1483 , loss: 0.0018707458\n",
            "iter: 1484 , loss: 0.0018703385\n",
            "iter: 1485 , loss: 0.0018699312\n",
            "iter: 1486 , loss: 0.0018695225\n",
            "iter: 1487 , loss: 0.0018691146\n",
            "iter: 1488 , loss: 0.0018687093\n",
            "iter: 1489 , loss: 0.0018683018\n",
            "iter: 1490 , loss: 0.0018678921\n",
            "iter: 1491 , loss: 0.001867483\n",
            "iter: 1492 , loss: 0.0018670785\n",
            "iter: 1493 , loss: 0.0018666723\n",
            "iter: 1494 , loss: 0.0018662699\n",
            "iter: 1495 , loss: 0.0018658655\n",
            "iter: 1496 , loss: 0.0018654631\n",
            "iter: 1497 , loss: 0.001865055\n",
            "iter: 1498 , loss: 0.0018646513\n",
            "iter: 1499 , loss: 0.0018642502\n",
            "iter: 1500 , loss: 0.0018638495\n",
            "iter: 1501 , loss: 0.0018634561\n",
            "iter: 1502 , loss: 0.0018630584\n",
            "iter: 1503 , loss: 0.0018626581\n",
            "iter: 1504 , loss: 0.0018622626\n",
            "iter: 1505 , loss: 0.0018618681\n",
            "iter: 1506 , loss: 0.0018614756\n",
            "iter: 1507 , loss: 0.0018610785\n",
            "iter: 1508 , loss: 0.001860683\n",
            "iter: 1509 , loss: 0.001860284\n",
            "iter: 1510 , loss: 0.0018598902\n",
            "iter: 1511 , loss: 0.0018594954\n",
            "iter: 1512 , loss: 0.0018591018\n",
            "iter: 1513 , loss: 0.0018587088\n",
            "iter: 1514 , loss: 0.0018583139\n",
            "iter: 1515 , loss: 0.0018579193\n",
            "iter: 1516 , loss: 0.0018575307\n",
            "iter: 1517 , loss: 0.0018571389\n",
            "iter: 1518 , loss: 0.0018567458\n",
            "iter: 1519 , loss: 0.0018563515\n",
            "iter: 1520 , loss: 0.0018559603\n",
            "iter: 1521 , loss: 0.0018555663\n",
            "iter: 1522 , loss: 0.0018551737\n",
            "iter: 1523 , loss: 0.0018547816\n",
            "iter: 1524 , loss: 0.0018543882\n",
            "iter: 1525 , loss: 0.0018539983\n",
            "iter: 1526 , loss: 0.0018536131\n",
            "iter: 1527 , loss: 0.0018532197\n",
            "iter: 1528 , loss: 0.0018528289\n",
            "iter: 1529 , loss: 0.00185244\n",
            "iter: 1530 , loss: 0.0018520508\n",
            "iter: 1531 , loss: 0.001851661\n",
            "iter: 1532 , loss: 0.0018512777\n",
            "iter: 1533 , loss: 0.0018508855\n",
            "iter: 1534 , loss: 0.0018504966\n",
            "iter: 1535 , loss: 0.0018501073\n",
            "iter: 1536 , loss: 0.0018497208\n",
            "iter: 1537 , loss: 0.0018493339\n",
            "iter: 1538 , loss: 0.0018489489\n",
            "iter: 1539 , loss: 0.0018485616\n",
            "iter: 1540 , loss: 0.0018481779\n",
            "iter: 1541 , loss: 0.0018477933\n",
            "iter: 1542 , loss: 0.001847405\n",
            "iter: 1543 , loss: 0.001847018\n",
            "iter: 1544 , loss: 0.001846631\n",
            "iter: 1545 , loss: 0.0018462534\n",
            "iter: 1546 , loss: 0.001845865\n",
            "iter: 1547 , loss: 0.0018454869\n",
            "iter: 1548 , loss: 0.0018450996\n",
            "iter: 1549 , loss: 0.0018447194\n",
            "iter: 1550 , loss: 0.0018443338\n",
            "iter: 1551 , loss: 0.0018439512\n",
            "iter: 1552 , loss: 0.0018435727\n",
            "iter: 1553 , loss: 0.0018431885\n",
            "iter: 1554 , loss: 0.0018428114\n",
            "iter: 1555 , loss: 0.0018424316\n",
            "iter: 1556 , loss: 0.0018420531\n",
            "iter: 1557 , loss: 0.0018416699\n",
            "iter: 1558 , loss: 0.0018412881\n",
            "iter: 1559 , loss: 0.001840907\n",
            "iter: 1560 , loss: 0.0018405287\n",
            "iter: 1561 , loss: 0.0018401471\n",
            "iter: 1562 , loss: 0.0018397744\n",
            "iter: 1563 , loss: 0.0018393903\n",
            "iter: 1564 , loss: 0.0018390139\n",
            "iter: 1565 , loss: 0.0018386334\n",
            "iter: 1566 , loss: 0.0018382632\n",
            "iter: 1567 , loss: 0.001837883\n",
            "iter: 1568 , loss: 0.001837508\n",
            "iter: 1569 , loss: 0.001837133\n",
            "iter: 1570 , loss: 0.0018367572\n",
            "iter: 1571 , loss: 0.00183638\n",
            "iter: 1572 , loss: 0.0018359991\n",
            "iter: 1573 , loss: 0.0018356267\n",
            "iter: 1574 , loss: 0.001835254\n",
            "iter: 1575 , loss: 0.0018348785\n",
            "iter: 1576 , loss: 0.0018345065\n",
            "iter: 1577 , loss: 0.0018341305\n",
            "iter: 1578 , loss: 0.0018337588\n",
            "iter: 1579 , loss: 0.0018333829\n",
            "iter: 1580 , loss: 0.0018330098\n",
            "iter: 1581 , loss: 0.0018326403\n",
            "iter: 1582 , loss: 0.0018322677\n",
            "iter: 1583 , loss: 0.0018318901\n",
            "iter: 1584 , loss: 0.0018315248\n",
            "iter: 1585 , loss: 0.0018311504\n",
            "iter: 1586 , loss: 0.0018307786\n",
            "iter: 1587 , loss: 0.0018304118\n",
            "iter: 1588 , loss: 0.0018300428\n",
            "iter: 1589 , loss: 0.0018296695\n",
            "iter: 1590 , loss: 0.0018293\n",
            "iter: 1591 , loss: 0.0018289284\n",
            "iter: 1592 , loss: 0.0018285622\n",
            "iter: 1593 , loss: 0.0018281931\n",
            "iter: 1594 , loss: 0.0018278295\n",
            "iter: 1595 , loss: 0.0018274591\n",
            "iter: 1596 , loss: 0.0018270863\n",
            "iter: 1597 , loss: 0.0018267205\n",
            "iter: 1598 , loss: 0.0018263529\n",
            "iter: 1599 , loss: 0.0018259863\n",
            "iter: 1600 , loss: 0.0018256183\n",
            "iter: 1601 , loss: 0.0018252571\n",
            "iter: 1602 , loss: 0.0018248892\n",
            "iter: 1603 , loss: 0.0018245227\n",
            "iter: 1604 , loss: 0.0018241615\n",
            "iter: 1605 , loss: 0.0018237927\n",
            "iter: 1606 , loss: 0.0018234272\n",
            "iter: 1607 , loss: 0.001823068\n",
            "iter: 1608 , loss: 0.001822701\n",
            "iter: 1609 , loss: 0.0018223397\n",
            "iter: 1610 , loss: 0.0018219777\n",
            "iter: 1611 , loss: 0.001821615\n",
            "iter: 1612 , loss: 0.0018212534\n",
            "iter: 1613 , loss: 0.0018208913\n",
            "iter: 1614 , loss: 0.0018205303\n",
            "iter: 1615 , loss: 0.0018201665\n",
            "iter: 1616 , loss: 0.0018198037\n",
            "iter: 1617 , loss: 0.0018194466\n",
            "iter: 1618 , loss: 0.0018190822\n",
            "iter: 1619 , loss: 0.0018187207\n",
            "iter: 1620 , loss: 0.0018183592\n",
            "iter: 1621 , loss: 0.0018180016\n",
            "iter: 1622 , loss: 0.0018176384\n",
            "iter: 1623 , loss: 0.0018172786\n",
            "iter: 1624 , loss: 0.0018169219\n",
            "iter: 1625 , loss: 0.0018165645\n",
            "iter: 1626 , loss: 0.0018162036\n",
            "iter: 1627 , loss: 0.0018158488\n",
            "iter: 1628 , loss: 0.0018154914\n",
            "iter: 1629 , loss: 0.0018151335\n",
            "iter: 1630 , loss: 0.0018147784\n",
            "iter: 1631 , loss: 0.0018144258\n",
            "iter: 1632 , loss: 0.0018140632\n",
            "iter: 1633 , loss: 0.0018137085\n",
            "iter: 1634 , loss: 0.0018133577\n",
            "iter: 1635 , loss: 0.0018130052\n",
            "iter: 1636 , loss: 0.0018126523\n",
            "iter: 1637 , loss: 0.0018122972\n",
            "iter: 1638 , loss: 0.0018119437\n",
            "iter: 1639 , loss: 0.0018115909\n",
            "iter: 1640 , loss: 0.001811231\n",
            "iter: 1641 , loss: 0.0018108794\n",
            "iter: 1642 , loss: 0.0018105253\n",
            "iter: 1643 , loss: 0.0018101747\n",
            "iter: 1644 , loss: 0.0018098225\n",
            "iter: 1645 , loss: 0.0018094721\n",
            "iter: 1646 , loss: 0.0018091222\n",
            "iter: 1647 , loss: 0.0018087716\n",
            "iter: 1648 , loss: 0.001808422\n",
            "iter: 1649 , loss: 0.0018080669\n",
            "iter: 1650 , loss: 0.0018077159\n",
            "iter: 1651 , loss: 0.0018073665\n",
            "iter: 1652 , loss: 0.0018070167\n",
            "iter: 1653 , loss: 0.0018066684\n",
            "iter: 1654 , loss: 0.0018063155\n",
            "iter: 1655 , loss: 0.0018059678\n",
            "iter: 1656 , loss: 0.0018056163\n",
            "iter: 1657 , loss: 0.0018052689\n",
            "iter: 1658 , loss: 0.0018049157\n",
            "iter: 1659 , loss: 0.001804566\n",
            "iter: 1660 , loss: 0.0018042221\n",
            "iter: 1661 , loss: 0.0018038753\n",
            "iter: 1662 , loss: 0.0018035272\n",
            "iter: 1663 , loss: 0.0018031838\n",
            "iter: 1664 , loss: 0.001802837\n",
            "iter: 1665 , loss: 0.001802495\n",
            "iter: 1666 , loss: 0.0018021534\n",
            "iter: 1667 , loss: 0.0018018057\n",
            "iter: 1668 , loss: 0.0018014681\n",
            "iter: 1669 , loss: 0.0018011242\n",
            "iter: 1670 , loss: 0.0018007848\n",
            "iter: 1671 , loss: 0.0018004407\n",
            "iter: 1672 , loss: 0.0018000954\n",
            "iter: 1673 , loss: 0.0017997507\n",
            "iter: 1674 , loss: 0.0017994046\n",
            "iter: 1675 , loss: 0.0017990634\n",
            "iter: 1676 , loss: 0.0017987181\n",
            "iter: 1677 , loss: 0.0017983742\n",
            "iter: 1678 , loss: 0.0017980345\n",
            "iter: 1679 , loss: 0.0017976884\n",
            "iter: 1680 , loss: 0.0017973509\n",
            "iter: 1681 , loss: 0.0017970057\n",
            "iter: 1682 , loss: 0.0017966703\n",
            "iter: 1683 , loss: 0.0017963299\n",
            "iter: 1684 , loss: 0.0017959892\n",
            "iter: 1685 , loss: 0.0017956494\n",
            "iter: 1686 , loss: 0.0017953102\n",
            "iter: 1687 , loss: 0.0017949685\n",
            "iter: 1688 , loss: 0.0017946278\n",
            "iter: 1689 , loss: 0.001794288\n",
            "iter: 1690 , loss: 0.001793949\n",
            "iter: 1691 , loss: 0.0017936097\n",
            "iter: 1692 , loss: 0.0017932712\n",
            "iter: 1693 , loss: 0.0017929329\n",
            "iter: 1694 , loss: 0.0017925927\n",
            "iter: 1695 , loss: 0.0017922584\n",
            "iter: 1696 , loss: 0.0017919206\n",
            "iter: 1697 , loss: 0.0017915819\n",
            "iter: 1698 , loss: 0.0017912444\n",
            "iter: 1699 , loss: 0.0017909106\n",
            "iter: 1700 , loss: 0.0017905724\n",
            "iter: 1701 , loss: 0.0017902325\n",
            "iter: 1702 , loss: 0.0017898995\n",
            "iter: 1703 , loss: 0.0017895623\n",
            "iter: 1704 , loss: 0.0017892264\n",
            "iter: 1705 , loss: 0.0017888957\n",
            "iter: 1706 , loss: 0.0017885641\n",
            "iter: 1707 , loss: 0.0017882249\n",
            "iter: 1708 , loss: 0.0017878923\n",
            "iter: 1709 , loss: 0.001787564\n",
            "iter: 1710 , loss: 0.0017872255\n",
            "iter: 1711 , loss: 0.0017868896\n",
            "iter: 1712 , loss: 0.0017865519\n",
            "iter: 1713 , loss: 0.0017862206\n",
            "iter: 1714 , loss: 0.0017858848\n",
            "iter: 1715 , loss: 0.0017855464\n",
            "iter: 1716 , loss: 0.0017852206\n",
            "iter: 1717 , loss: 0.0017848824\n",
            "iter: 1718 , loss: 0.001784549\n",
            "iter: 1719 , loss: 0.0017842167\n",
            "iter: 1720 , loss: 0.0017838825\n",
            "iter: 1721 , loss: 0.0017835493\n",
            "iter: 1722 , loss: 0.0017832163\n",
            "iter: 1723 , loss: 0.0017828796\n",
            "iter: 1724 , loss: 0.0017825556\n",
            "iter: 1725 , loss: 0.0017822246\n",
            "iter: 1726 , loss: 0.0017818969\n",
            "iter: 1727 , loss: 0.001781571\n",
            "iter: 1728 , loss: 0.0017812419\n",
            "iter: 1729 , loss: 0.0017809152\n",
            "iter: 1730 , loss: 0.0017805862\n",
            "iter: 1731 , loss: 0.0017802579\n",
            "iter: 1732 , loss: 0.0017799329\n",
            "iter: 1733 , loss: 0.0017796013\n",
            "iter: 1734 , loss: 0.0017792794\n",
            "iter: 1735 , loss: 0.0017789513\n",
            "iter: 1736 , loss: 0.0017786222\n",
            "iter: 1737 , loss: 0.0017782903\n",
            "iter: 1738 , loss: 0.001777961\n",
            "iter: 1739 , loss: 0.0017776367\n",
            "iter: 1740 , loss: 0.0017773056\n",
            "iter: 1741 , loss: 0.0017769804\n",
            "iter: 1742 , loss: 0.0017766517\n",
            "iter: 1743 , loss: 0.0017763252\n",
            "iter: 1744 , loss: 0.001775999\n",
            "iter: 1745 , loss: 0.0017756678\n",
            "iter: 1746 , loss: 0.0017753441\n",
            "iter: 1747 , loss: 0.0017750211\n",
            "iter: 1748 , loss: 0.0017746937\n",
            "iter: 1749 , loss: 0.0017743654\n",
            "iter: 1750 , loss: 0.0017740405\n",
            "iter: 1751 , loss: 0.0017737183\n",
            "iter: 1752 , loss: 0.0017733944\n",
            "iter: 1753 , loss: 0.0017730698\n",
            "iter: 1754 , loss: 0.0017727491\n",
            "iter: 1755 , loss: 0.0017724207\n",
            "iter: 1756 , loss: 0.001772099\n",
            "iter: 1757 , loss: 0.0017717812\n",
            "iter: 1758 , loss: 0.0017714576\n",
            "iter: 1759 , loss: 0.0017711351\n",
            "iter: 1760 , loss: 0.0017708172\n",
            "iter: 1761 , loss: 0.001770496\n",
            "iter: 1762 , loss: 0.0017701705\n",
            "iter: 1763 , loss: 0.0017698482\n",
            "iter: 1764 , loss: 0.0017695323\n",
            "iter: 1765 , loss: 0.0017692058\n",
            "iter: 1766 , loss: 0.0017688805\n",
            "iter: 1767 , loss: 0.001768563\n",
            "iter: 1768 , loss: 0.0017682473\n",
            "iter: 1769 , loss: 0.0017679234\n",
            "iter: 1770 , loss: 0.0017676024\n",
            "iter: 1771 , loss: 0.0017672899\n",
            "iter: 1772 , loss: 0.0017669684\n",
            "iter: 1773 , loss: 0.0017666447\n",
            "iter: 1774 , loss: 0.0017663308\n",
            "iter: 1775 , loss: 0.0017660109\n",
            "iter: 1776 , loss: 0.0017656902\n",
            "iter: 1777 , loss: 0.0017653756\n",
            "iter: 1778 , loss: 0.0017650595\n",
            "iter: 1779 , loss: 0.0017647418\n",
            "iter: 1780 , loss: 0.001764421\n",
            "iter: 1781 , loss: 0.0017641046\n",
            "iter: 1782 , loss: 0.0017637904\n",
            "iter: 1783 , loss: 0.0017634686\n",
            "iter: 1784 , loss: 0.0017631551\n",
            "iter: 1785 , loss: 0.0017628483\n",
            "iter: 1786 , loss: 0.0017625325\n",
            "iter: 1787 , loss: 0.0017622175\n",
            "iter: 1788 , loss: 0.0017619076\n",
            "iter: 1789 , loss: 0.0017615892\n",
            "iter: 1790 , loss: 0.0017612766\n",
            "iter: 1791 , loss: 0.0017609647\n",
            "iter: 1792 , loss: 0.0017606551\n",
            "iter: 1793 , loss: 0.0017603408\n",
            "iter: 1794 , loss: 0.0017600262\n",
            "iter: 1795 , loss: 0.0017597092\n",
            "iter: 1796 , loss: 0.0017594004\n",
            "iter: 1797 , loss: 0.0017590898\n",
            "iter: 1798 , loss: 0.0017587836\n",
            "iter: 1799 , loss: 0.00175847\n",
            "iter: 1800 , loss: 0.0017581546\n",
            "iter: 1801 , loss: 0.0017578482\n",
            "iter: 1802 , loss: 0.0017575343\n",
            "iter: 1803 , loss: 0.0017572255\n",
            "iter: 1804 , loss: 0.0017569227\n",
            "iter: 1805 , loss: 0.001756619\n",
            "iter: 1806 , loss: 0.0017563058\n",
            "iter: 1807 , loss: 0.0017559935\n",
            "iter: 1808 , loss: 0.0017556831\n",
            "iter: 1809 , loss: 0.0017553754\n",
            "iter: 1810 , loss: 0.0017550691\n",
            "iter: 1811 , loss: 0.0017547649\n",
            "iter: 1812 , loss: 0.0017544588\n",
            "iter: 1813 , loss: 0.0017541543\n",
            "iter: 1814 , loss: 0.0017538507\n",
            "iter: 1815 , loss: 0.0017535487\n",
            "iter: 1816 , loss: 0.0017532473\n",
            "iter: 1817 , loss: 0.0017529401\n",
            "iter: 1818 , loss: 0.0017526385\n",
            "iter: 1819 , loss: 0.0017523356\n",
            "iter: 1820 , loss: 0.0017520331\n",
            "iter: 1821 , loss: 0.0017517279\n",
            "iter: 1822 , loss: 0.0017514202\n",
            "iter: 1823 , loss: 0.0017511154\n",
            "iter: 1824 , loss: 0.0017508104\n",
            "iter: 1825 , loss: 0.0017505102\n",
            "iter: 1826 , loss: 0.0017502089\n",
            "iter: 1827 , loss: 0.0017499062\n",
            "iter: 1828 , loss: 0.0017496026\n",
            "iter: 1829 , loss: 0.001749298\n",
            "iter: 1830 , loss: 0.0017489982\n",
            "iter: 1831 , loss: 0.0017486926\n",
            "iter: 1832 , loss: 0.001748391\n",
            "iter: 1833 , loss: 0.0017480969\n",
            "iter: 1834 , loss: 0.0017477999\n",
            "iter: 1835 , loss: 0.001747498\n",
            "iter: 1836 , loss: 0.0017472024\n",
            "iter: 1837 , loss: 0.0017469057\n",
            "iter: 1838 , loss: 0.0017466083\n",
            "iter: 1839 , loss: 0.0017463159\n",
            "iter: 1840 , loss: 0.0017460177\n",
            "iter: 1841 , loss: 0.0017457246\n",
            "iter: 1842 , loss: 0.0017454288\n",
            "iter: 1843 , loss: 0.0017451314\n",
            "iter: 1844 , loss: 0.0017448388\n",
            "iter: 1845 , loss: 0.0017445451\n",
            "iter: 1846 , loss: 0.0017442539\n",
            "iter: 1847 , loss: 0.0017439603\n",
            "iter: 1848 , loss: 0.0017436645\n",
            "iter: 1849 , loss: 0.0017433767\n",
            "iter: 1850 , loss: 0.0017430802\n",
            "iter: 1851 , loss: 0.0017427859\n",
            "iter: 1852 , loss: 0.0017424936\n",
            "iter: 1853 , loss: 0.001742206\n",
            "iter: 1854 , loss: 0.001741914\n",
            "iter: 1855 , loss: 0.0017416241\n",
            "iter: 1856 , loss: 0.0017413321\n",
            "iter: 1857 , loss: 0.0017410432\n",
            "iter: 1858 , loss: 0.0017407555\n",
            "iter: 1859 , loss: 0.0017404642\n",
            "iter: 1860 , loss: 0.0017401747\n",
            "iter: 1861 , loss: 0.0017398932\n",
            "iter: 1862 , loss: 0.0017395971\n",
            "iter: 1863 , loss: 0.0017393107\n",
            "iter: 1864 , loss: 0.0017390279\n",
            "iter: 1865 , loss: 0.0017387384\n",
            "iter: 1866 , loss: 0.0017384508\n",
            "iter: 1867 , loss: 0.0017381656\n",
            "iter: 1868 , loss: 0.0017378852\n",
            "iter: 1869 , loss: 0.0017375932\n",
            "iter: 1870 , loss: 0.0017373094\n",
            "iter: 1871 , loss: 0.0017370256\n",
            "iter: 1872 , loss: 0.001736743\n",
            "iter: 1873 , loss: 0.0017364601\n",
            "iter: 1874 , loss: 0.0017361786\n",
            "iter: 1875 , loss: 0.0017358982\n",
            "iter: 1876 , loss: 0.0017356209\n",
            "iter: 1877 , loss: 0.0017353416\n",
            "iter: 1878 , loss: 0.0017350648\n",
            "iter: 1879 , loss: 0.0017347842\n",
            "iter: 1880 , loss: 0.0017345097\n",
            "iter: 1881 , loss: 0.0017342359\n",
            "iter: 1882 , loss: 0.0017339572\n",
            "iter: 1883 , loss: 0.0017336819\n",
            "iter: 1884 , loss: 0.0017334063\n",
            "iter: 1885 , loss: 0.0017331289\n",
            "iter: 1886 , loss: 0.0017328514\n",
            "iter: 1887 , loss: 0.0017325785\n",
            "iter: 1888 , loss: 0.0017323068\n",
            "iter: 1889 , loss: 0.0017320318\n",
            "iter: 1890 , loss: 0.0017317614\n",
            "iter: 1891 , loss: 0.0017314961\n",
            "iter: 1892 , loss: 0.0017312239\n",
            "iter: 1893 , loss: 0.001730952\n",
            "iter: 1894 , loss: 0.0017306829\n",
            "iter: 1895 , loss: 0.0017304131\n",
            "iter: 1896 , loss: 0.0017301342\n",
            "iter: 1897 , loss: 0.001729858\n",
            "iter: 1898 , loss: 0.0017295843\n",
            "iter: 1899 , loss: 0.0017293078\n",
            "iter: 1900 , loss: 0.0017290334\n",
            "iter: 1901 , loss: 0.0017287623\n",
            "iter: 1902 , loss: 0.0017284947\n",
            "iter: 1903 , loss: 0.0017282264\n",
            "iter: 1904 , loss: 0.0017279587\n",
            "iter: 1905 , loss: 0.0017276909\n",
            "iter: 1906 , loss: 0.0017274215\n",
            "iter: 1907 , loss: 0.0017271511\n",
            "iter: 1908 , loss: 0.0017268836\n",
            "iter: 1909 , loss: 0.001726614\n",
            "iter: 1910 , loss: 0.0017263464\n",
            "iter: 1911 , loss: 0.0017260734\n",
            "iter: 1912 , loss: 0.0017258031\n",
            "iter: 1913 , loss: 0.0017255346\n",
            "iter: 1914 , loss: 0.0017252626\n",
            "iter: 1915 , loss: 0.001724994\n",
            "iter: 1916 , loss: 0.0017247252\n",
            "iter: 1917 , loss: 0.001724456\n",
            "iter: 1918 , loss: 0.0017241917\n",
            "iter: 1919 , loss: 0.0017239221\n",
            "iter: 1920 , loss: 0.0017236584\n",
            "iter: 1921 , loss: 0.0017233868\n",
            "iter: 1922 , loss: 0.0017231245\n",
            "iter: 1923 , loss: 0.0017228607\n",
            "iter: 1924 , loss: 0.0017225973\n",
            "iter: 1925 , loss: 0.0017223277\n",
            "iter: 1926 , loss: 0.0017220672\n",
            "iter: 1927 , loss: 0.0017218082\n",
            "iter: 1928 , loss: 0.0017215471\n",
            "iter: 1929 , loss: 0.0017212867\n",
            "iter: 1930 , loss: 0.0017210211\n",
            "iter: 1931 , loss: 0.0017207687\n",
            "iter: 1932 , loss: 0.0017205022\n",
            "iter: 1933 , loss: 0.001720246\n",
            "iter: 1934 , loss: 0.0017199795\n",
            "iter: 1935 , loss: 0.0017197155\n",
            "iter: 1936 , loss: 0.0017194507\n",
            "iter: 1937 , loss: 0.0017191904\n",
            "iter: 1938 , loss: 0.0017189224\n",
            "iter: 1939 , loss: 0.0017186573\n",
            "iter: 1940 , loss: 0.0017183952\n",
            "iter: 1941 , loss: 0.0017181319\n",
            "iter: 1942 , loss: 0.0017178667\n",
            "iter: 1943 , loss: 0.0017176035\n",
            "iter: 1944 , loss: 0.0017173362\n",
            "iter: 1945 , loss: 0.0017170806\n",
            "iter: 1946 , loss: 0.0017168128\n",
            "iter: 1947 , loss: 0.0017165554\n",
            "iter: 1948 , loss: 0.0017162804\n",
            "iter: 1949 , loss: 0.0017160211\n",
            "iter: 1950 , loss: 0.0017157574\n",
            "iter: 1951 , loss: 0.0017154892\n",
            "iter: 1952 , loss: 0.0017152257\n",
            "iter: 1953 , loss: 0.0017149578\n",
            "iter: 1954 , loss: 0.0017146923\n",
            "iter: 1955 , loss: 0.0017144259\n",
            "iter: 1956 , loss: 0.0017141567\n",
            "iter: 1957 , loss: 0.0017138924\n",
            "iter: 1958 , loss: 0.0017136261\n",
            "iter: 1959 , loss: 0.0017133649\n",
            "iter: 1960 , loss: 0.0017130942\n",
            "iter: 1961 , loss: 0.0017128302\n",
            "iter: 1962 , loss: 0.0017125629\n",
            "iter: 1963 , loss: 0.0017122986\n",
            "iter: 1964 , loss: 0.001712038\n",
            "iter: 1965 , loss: 0.0017117665\n",
            "iter: 1966 , loss: 0.0017115043\n",
            "iter: 1967 , loss: 0.001711241\n",
            "iter: 1968 , loss: 0.001710978\n",
            "iter: 1969 , loss: 0.0017107144\n",
            "iter: 1970 , loss: 0.0017104552\n",
            "iter: 1971 , loss: 0.001710194\n",
            "iter: 1972 , loss: 0.0017099251\n",
            "iter: 1973 , loss: 0.0017096702\n",
            "iter: 1974 , loss: 0.0017094027\n",
            "iter: 1975 , loss: 0.001709142\n",
            "iter: 1976 , loss: 0.0017088825\n",
            "iter: 1977 , loss: 0.0017086163\n",
            "iter: 1978 , loss: 0.0017083497\n",
            "iter: 1979 , loss: 0.0017080846\n",
            "iter: 1980 , loss: 0.0017078284\n",
            "iter: 1981 , loss: 0.0017075671\n",
            "iter: 1982 , loss: 0.0017073068\n",
            "iter: 1983 , loss: 0.0017070423\n",
            "iter: 1984 , loss: 0.0017067846\n",
            "iter: 1985 , loss: 0.0017065256\n",
            "iter: 1986 , loss: 0.0017062667\n",
            "iter: 1987 , loss: 0.0017060023\n",
            "iter: 1988 , loss: 0.001705736\n",
            "iter: 1989 , loss: 0.0017054757\n",
            "iter: 1990 , loss: 0.0017052132\n",
            "iter: 1991 , loss: 0.0017049563\n",
            "iter: 1992 , loss: 0.0017047009\n",
            "iter: 1993 , loss: 0.0017044317\n",
            "iter: 1994 , loss: 0.0017041707\n",
            "iter: 1995 , loss: 0.0017039138\n",
            "iter: 1996 , loss: 0.0017036586\n",
            "iter: 1997 , loss: 0.0017033927\n",
            "iter: 1998 , loss: 0.0017031359\n",
            "iter: 1999 , loss: 0.0017028849\n",
            "iter: 2000 , loss: 0.0017026194\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRsC5OC-SgLF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "e65fb7b2-9eb4-4ec4-dae6-d2e44b3c65bd"
      },
      "source": [
        "iter = np.arange(1,len(test_losses)+1)\n",
        "plt.title('NN Mean Squerd Error minimization')\n",
        "plt.plot(iter, test_losses, label='Testing loss')\n",
        "plt.plot(iter, train_losses, label='Training loss')\n",
        "plt.legend()\n",
        "plt.xlabel('Number of iterations')\n",
        "plt.ylabel('Mean Squerd Error')\n",
        "plt.show()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0uQG8nzLzN_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c8be673-e0ab-4456-a306-a90a5f663763"
      },
      "source": [
        "sess.run(loss, feed_dict = {x: data_x_test, y_: data_y_test})"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0017026194"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTENJ5xYo0wP"
      },
      "source": [
        ""
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HFYlKQKL5OB"
      },
      "source": [
        ""
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pz8Xc5nD_Uye"
      },
      "source": [
        ""
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6DGe1xp_U1V"
      },
      "source": [
        ""
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoXEQ4bs_U4M"
      },
      "source": [
        ""
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWo9rxNu_U8K"
      },
      "source": [
        ""
      ],
      "execution_count": 29,
      "outputs": []
    }
  ]
}