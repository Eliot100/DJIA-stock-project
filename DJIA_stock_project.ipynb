{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DJIA stock project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDUQkKf_jq-p"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8X7Wvabnrfg"
      },
      "source": [
        "df_RedditNews = pd.read_csv('https://raw.githubusercontent.com/Eliot100/DJIA-stock-project/main/RedditNews.csv')\n",
        "df_DJIA = pd.read_csv('https://raw.githubusercontent.com/Eliot100/DJIA-stock-project/main/upload_DJIA_table.csv')\n",
        "df_Combined_News_DJIA = pd.read_csv('https://raw.githubusercontent.com/Eliot100/DJIA-stock-project/main/Combined_News_DJIA.csv')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "08KB3HKENk3T",
        "outputId": "2af6d954-b431-4fa7-f41e-b3e5926ab737"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "\n",
        "df_DJIA2 = df_DJIA.copy()\n",
        "df_DJIA2[['Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']] = \\\n",
        "  scaler.fit_transform(df_DJIA2[['Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']])\n",
        "  \n",
        "df_DJIA2.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Adj Close</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2016-07-01</td>\n",
              "      <td>0.933579</td>\n",
              "      <td>0.940047</td>\n",
              "      <td>0.939734</td>\n",
              "      <td>0.938290</td>\n",
              "      <td>-0.778698</td>\n",
              "      <td>0.938290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2016-06-30</td>\n",
              "      <td>0.897638</td>\n",
              "      <td>0.927717</td>\n",
              "      <td>0.904977</td>\n",
              "      <td>0.934995</td>\n",
              "      <td>-0.626052</td>\n",
              "      <td>0.934995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2016-06-29</td>\n",
              "      <td>0.854005</td>\n",
              "      <td>0.888874</td>\n",
              "      <td>0.861634</td>\n",
              "      <td>0.894995</td>\n",
              "      <td>-0.706021</td>\n",
              "      <td>0.894995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2016-06-28</td>\n",
              "      <td>0.808881</td>\n",
              "      <td>0.838231</td>\n",
              "      <td>0.816642</td>\n",
              "      <td>0.846554</td>\n",
              "      <td>-0.688587</td>\n",
              "      <td>0.846554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2016-06-27</td>\n",
              "      <td>0.836872</td>\n",
              "      <td>0.828866</td>\n",
              "      <td>0.795049</td>\n",
              "      <td>0.800745</td>\n",
              "      <td>-0.608918</td>\n",
              "      <td>0.800745</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Date      Open      High       Low     Close    Volume  Adj Close\n",
              "0  2016-07-01  0.933579  0.940047  0.939734  0.938290 -0.778698   0.938290\n",
              "1  2016-06-30  0.897638  0.927717  0.904977  0.934995 -0.626052   0.934995\n",
              "2  2016-06-29  0.854005  0.888874  0.861634  0.894995 -0.706021   0.894995\n",
              "3  2016-06-28  0.808881  0.838231  0.816642  0.846554 -0.688587   0.846554\n",
              "4  2016-06-27  0.836872  0.828866  0.795049  0.800745 -0.608918   0.800745"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXldyEiwMHFQ",
        "outputId": "7a460c3c-6433-4084-bbf5-44e84526afc7"
      },
      "source": [
        "df_Combined = df_DJIA2.copy()\n",
        "for i in range(0,59):\n",
        "  df_Combined[str(i+1)+\" day before Open\"] = \"\"\n",
        "  df_Combined[str(i+1)+\" day before High\"] = \"\"\n",
        "  df_Combined[str(i+1)+\" day before Low\"] = \"\"\n",
        "  df_Combined[str(i+1)+\" day before Close\"] = \"\"\n",
        "  df_Combined[str(i+1)+\" day before Volume\"] = \"\"\n",
        "  df_Combined[str(i+1)+\" day before Adj Close\"] = \"\"\n",
        "\n",
        "for i in range(0,25):\n",
        "  df_Combined[\"Top\"+str(i+1)] = \"\"\n",
        "\n",
        "for j in range(0, df_DJIA2.shape[0]-59):\n",
        "  for i in range(0, 59):\n",
        "    df_Combined[str(i+1)+\" day before Open\"][j] = df_Combined[\"Open\"][j+i+1]\n",
        "    df_Combined[str(i+1)+\" day before High\"][j] = df_Combined[\"High\"][j+i+1]\n",
        "    df_Combined[str(i+1)+\" day before Low\"][j] = df_Combined[\"Low\"][j+i+1]\n",
        "    df_Combined[str(i+1)+\" day before Close\"][j] = df_Combined[\"Close\"][j+i+1]\n",
        "    df_Combined[str(i+1)+\" day before Volume\"][j] = df_Combined[\"Volume\"][j+i+1]\n",
        "    df_Combined[str(i+1)+\" day before Adj Close\"][j] = df_Combined[\"Adj Close\"][j+i+1]\n",
        "\n",
        "for i in range(0,25):\n",
        "  df_Combined[\"Top\"+str(i+1)] = \"\"\n",
        "\n",
        "for i in range(0, df_DJIA2.shape[0]):\n",
        "  News_Date_array = df_RedditNews[df_RedditNews[\"Date\"] == df_Combined[\"Date\"][i]][\"News\"].to_numpy()\n",
        "  for j in range(0, News_Date_array.shape[0]):\n",
        "    df_Combined[\"Top\"+str(j+1)][i] = News_Date_array[j]\n",
        "\n",
        "df_Combined = df_Combined[:-59]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "Te1WmkTyv64J",
        "outputId": "e5157822-46ce-45d5-94af-544001076b0c"
      },
      "source": [
        "df_Combined.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>1 day before Open</th>\n",
              "      <th>1 day before High</th>\n",
              "      <th>1 day before Low</th>\n",
              "      <th>1 day before Close</th>\n",
              "      <th>1 day before Volume</th>\n",
              "      <th>1 day before Adj Close</th>\n",
              "      <th>2 day before Open</th>\n",
              "      <th>2 day before High</th>\n",
              "      <th>2 day before Low</th>\n",
              "      <th>2 day before Close</th>\n",
              "      <th>2 day before Volume</th>\n",
              "      <th>2 day before Adj Close</th>\n",
              "      <th>3 day before Open</th>\n",
              "      <th>3 day before High</th>\n",
              "      <th>3 day before Low</th>\n",
              "      <th>3 day before Close</th>\n",
              "      <th>3 day before Volume</th>\n",
              "      <th>3 day before Adj Close</th>\n",
              "      <th>4 day before Open</th>\n",
              "      <th>4 day before High</th>\n",
              "      <th>4 day before Low</th>\n",
              "      <th>4 day before Close</th>\n",
              "      <th>4 day before Volume</th>\n",
              "      <th>4 day before Adj Close</th>\n",
              "      <th>5 day before Open</th>\n",
              "      <th>5 day before High</th>\n",
              "      <th>5 day before Low</th>\n",
              "      <th>5 day before Close</th>\n",
              "      <th>5 day before Volume</th>\n",
              "      <th>5 day before Adj Close</th>\n",
              "      <th>6 day before Open</th>\n",
              "      <th>6 day before High</th>\n",
              "      <th>6 day before Low</th>\n",
              "      <th>...</th>\n",
              "      <th>57 day before Close</th>\n",
              "      <th>57 day before Volume</th>\n",
              "      <th>57 day before Adj Close</th>\n",
              "      <th>58 day before Open</th>\n",
              "      <th>58 day before High</th>\n",
              "      <th>58 day before Low</th>\n",
              "      <th>58 day before Close</th>\n",
              "      <th>58 day before Volume</th>\n",
              "      <th>58 day before Adj Close</th>\n",
              "      <th>59 day before Open</th>\n",
              "      <th>59 day before High</th>\n",
              "      <th>59 day before Low</th>\n",
              "      <th>59 day before Close</th>\n",
              "      <th>59 day before Volume</th>\n",
              "      <th>59 day before Adj Close</th>\n",
              "      <th>Top1</th>\n",
              "      <th>Top2</th>\n",
              "      <th>Top3</th>\n",
              "      <th>Top4</th>\n",
              "      <th>Top5</th>\n",
              "      <th>Top6</th>\n",
              "      <th>Top7</th>\n",
              "      <th>Top8</th>\n",
              "      <th>Top9</th>\n",
              "      <th>Top10</th>\n",
              "      <th>Top11</th>\n",
              "      <th>Top12</th>\n",
              "      <th>Top13</th>\n",
              "      <th>Top14</th>\n",
              "      <th>Top15</th>\n",
              "      <th>Top16</th>\n",
              "      <th>Top17</th>\n",
              "      <th>Top18</th>\n",
              "      <th>Top19</th>\n",
              "      <th>Top20</th>\n",
              "      <th>Top21</th>\n",
              "      <th>Top22</th>\n",
              "      <th>Top23</th>\n",
              "      <th>Top24</th>\n",
              "      <th>Top25</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2016-07-01</td>\n",
              "      <td>0.933579</td>\n",
              "      <td>0.940047</td>\n",
              "      <td>0.939734</td>\n",
              "      <td>0.938290</td>\n",
              "      <td>-0.778698</td>\n",
              "      <td>0.938290</td>\n",
              "      <td>0.897638</td>\n",
              "      <td>0.927717</td>\n",
              "      <td>0.904977</td>\n",
              "      <td>0.934995</td>\n",
              "      <td>-0.626052</td>\n",
              "      <td>0.934995</td>\n",
              "      <td>0.854005</td>\n",
              "      <td>0.888874</td>\n",
              "      <td>0.861634</td>\n",
              "      <td>0.894995</td>\n",
              "      <td>-0.706021</td>\n",
              "      <td>0.894995</td>\n",
              "      <td>0.808881</td>\n",
              "      <td>0.838231</td>\n",
              "      <td>0.816642</td>\n",
              "      <td>0.846554</td>\n",
              "      <td>-0.688587</td>\n",
              "      <td>0.846554</td>\n",
              "      <td>0.836872</td>\n",
              "      <td>0.828866</td>\n",
              "      <td>0.795049</td>\n",
              "      <td>0.800745</td>\n",
              "      <td>-0.608918</td>\n",
              "      <td>0.800745</td>\n",
              "      <td>0.937385</td>\n",
              "      <td>0.930469</td>\n",
              "      <td>0.844743</td>\n",
              "      <td>0.845029</td>\n",
              "      <td>-0.308067</td>\n",
              "      <td>0.845029</td>\n",
              "      <td>0.919961</td>\n",
              "      <td>0.94154</td>\n",
              "      <td>0.927397</td>\n",
              "      <td>...</td>\n",
              "      <td>0.899512</td>\n",
              "      <td>-0.782119</td>\n",
              "      <td>0.899512</td>\n",
              "      <td>0.876177</td>\n",
              "      <td>0.893533</td>\n",
              "      <td>0.878559</td>\n",
              "      <td>0.87149</td>\n",
              "      <td>-0.70386</td>\n",
              "      <td>0.87149</td>\n",
              "      <td>0.870893</td>\n",
              "      <td>0.887156</td>\n",
              "      <td>0.873858</td>\n",
              "      <td>0.874984</td>\n",
              "      <td>-0.78521</td>\n",
              "      <td>0.874984</td>\n",
              "      <td>A 117-year-old woman in Mexico City finally re...</td>\n",
              "      <td>IMF chief backs Athens as permanent Olympic host</td>\n",
              "      <td>The president of France says if Brexit won, so...</td>\n",
              "      <td>British Man Who Must Give Police 24 Hours' Not...</td>\n",
              "      <td>100+ Nobel laureates urge Greenpeace to stop o...</td>\n",
              "      <td>Brazil: Huge spike in number of police killing...</td>\n",
              "      <td>Austria's highest court annuls presidential el...</td>\n",
              "      <td>Facebook wins privacy case, can track any Belg...</td>\n",
              "      <td>Switzerland denies Muslim girls citizenship af...</td>\n",
              "      <td>China kills millions of innocent meditators fo...</td>\n",
              "      <td>France Cracks Down on Factory Farms - A viral ...</td>\n",
              "      <td>Abbas PLO Faction Calls Killer of 13-Year-Old ...</td>\n",
              "      <td>Taiwanese warship accidentally fires missile t...</td>\n",
              "      <td>Iran celebrates American Human Rights Week, mo...</td>\n",
              "      <td>U.N. panel moves to curb bias against L.G.B.T....</td>\n",
              "      <td>The United States has placed Myanmar, Uzbekist...</td>\n",
              "      <td>S&amp;amp;P revises European Union credit rating t...</td>\n",
              "      <td>India gets $1 billion loan from World Bank for...</td>\n",
              "      <td>U.S. sailors detained by Iran spoke too much u...</td>\n",
              "      <td>Mass fish kill in Vietnam solved as Taiwan ste...</td>\n",
              "      <td>Philippines president Rodrigo Duterte urges pe...</td>\n",
              "      <td>Spain arrests three Pakistanis accused of prom...</td>\n",
              "      <td>Venezuela, where anger over food shortages is ...</td>\n",
              "      <td>A Hindu temple worker has been killed by three...</td>\n",
              "      <td>Ozone layer hole seems to be healing - US &amp;amp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2016-06-30</td>\n",
              "      <td>0.897638</td>\n",
              "      <td>0.927717</td>\n",
              "      <td>0.904977</td>\n",
              "      <td>0.934995</td>\n",
              "      <td>-0.626052</td>\n",
              "      <td>0.934995</td>\n",
              "      <td>0.854005</td>\n",
              "      <td>0.888874</td>\n",
              "      <td>0.861634</td>\n",
              "      <td>0.894995</td>\n",
              "      <td>-0.706021</td>\n",
              "      <td>0.894995</td>\n",
              "      <td>0.808881</td>\n",
              "      <td>0.838231</td>\n",
              "      <td>0.816642</td>\n",
              "      <td>0.846554</td>\n",
              "      <td>-0.688587</td>\n",
              "      <td>0.846554</td>\n",
              "      <td>0.836872</td>\n",
              "      <td>0.828866</td>\n",
              "      <td>0.795049</td>\n",
              "      <td>0.800745</td>\n",
              "      <td>-0.608918</td>\n",
              "      <td>0.800745</td>\n",
              "      <td>0.937385</td>\n",
              "      <td>0.930469</td>\n",
              "      <td>0.844743</td>\n",
              "      <td>0.845029</td>\n",
              "      <td>-0.308067</td>\n",
              "      <td>0.845029</td>\n",
              "      <td>0.919961</td>\n",
              "      <td>0.94154</td>\n",
              "      <td>0.927397</td>\n",
              "      <td>0.948778</td>\n",
              "      <td>-0.730957</td>\n",
              "      <td>0.948778</td>\n",
              "      <td>0.918017</td>\n",
              "      <td>0.925922</td>\n",
              "      <td>0.9149</td>\n",
              "      <td>...</td>\n",
              "      <td>0.87149</td>\n",
              "      <td>-0.70386</td>\n",
              "      <td>0.87149</td>\n",
              "      <td>0.870893</td>\n",
              "      <td>0.887156</td>\n",
              "      <td>0.873858</td>\n",
              "      <td>0.874984</td>\n",
              "      <td>-0.78521</td>\n",
              "      <td>0.874984</td>\n",
              "      <td>0.893308</td>\n",
              "      <td>0.885914</td>\n",
              "      <td>0.866414</td>\n",
              "      <td>0.869034</td>\n",
              "      <td>-0.754812</td>\n",
              "      <td>0.869034</td>\n",
              "      <td>Jamaica proposes marijuana dispensers for tour...</td>\n",
              "      <td>Stephen Hawking says pollution and 'stupidity'...</td>\n",
              "      <td>Boris Johnson says he will not run for Tory pa...</td>\n",
              "      <td>Six gay men in Ivory Coast were abused and for...</td>\n",
              "      <td>Switzerland denies citizenship to Muslim immig...</td>\n",
              "      <td>Palestinian terrorist stabs israeli teen girl ...</td>\n",
              "      <td>Puerto Rico will default on $1 billion of debt...</td>\n",
              "      <td>Republic of Ireland fans to be awarded medal f...</td>\n",
              "      <td>Afghan suicide bomber 'kills up to 40' - BBC News</td>\n",
              "      <td>US airstrikes kill at least 250 ISIS fighters ...</td>\n",
              "      <td>Turkish Cop Who Took Down Istanbul Gunman Hail...</td>\n",
              "      <td>Cannabis compounds could treat Alzheimer's by ...</td>\n",
              "      <td>Japan's top court has approved blanket surveil...</td>\n",
              "      <td>CIA Gave Romania Millions to Host Secret Prisons</td>\n",
              "      <td>Groups urge U.N. to suspend Saudi Arabia from ...</td>\n",
              "      <td>Googles free wifi at Indian railway stations i...</td>\n",
              "      <td>Mounting evidence suggests 'hobbits' were wipe...</td>\n",
              "      <td>The men who carried out Tuesday's terror attac...</td>\n",
              "      <td>Calls to suspend Saudi Arabia from UN Human Ri...</td>\n",
              "      <td>More Than 100 Nobel Laureates Call Out Greenpe...</td>\n",
              "      <td>British pedophile sentenced to 85 years in US ...</td>\n",
              "      <td>US permitted 1,200 offshore fracks in Gulf of ...</td>\n",
              "      <td>We will be swimming in ridicule - French beach...</td>\n",
              "      <td>UEFA says no minutes of silence for Istanbul v...</td>\n",
              "      <td>Law Enforcement Sources: Gun Used in Paris Ter...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 386 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         Date  ...                                              Top25\n",
              "0  2016-07-01  ...  Ozone layer hole seems to be healing - US &amp...\n",
              "1  2016-06-30  ...  Law Enforcement Sources: Gun Used in Paris Ter...\n",
              "\n",
              "[2 rows x 386 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PPyrouheS8-",
        "outputId": "ee406018-38b2-4dfd-a832-4f8a337c6033"
      },
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "sna = SentimentIntensityAnalyzer()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgTom4oVeTCb",
        "outputId": "aa1f32c6-fa08-42fc-dc49-63422fa06415"
      },
      "source": [
        "df_Combined2 = df_Combined.copy()\n",
        "for i in range(0, df_Combined2.shape[0]):\n",
        "  News_Date_array = df_RedditNews[df_RedditNews[\"Date\"] == df_Combined2[\"Date\"][i]][\"News\"].to_numpy()\n",
        "\n",
        "  for j in range(0, 25):\n",
        "    df_Combined2[\"Top\"+str(j+1)][i] = sna.polarity_scores(df_Combined2[\"Top\"+str(j+1)][i])[\"compound\"]\n",
        "    "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bs9Dxqnp3wAt"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_nos3er2mkS"
      },
      "source": [
        "df_final = df_Combined2.drop(['Date', 'High', 'Low', 'Volume', 'Adj Close'], axis=1)\n",
        "X_df = df_final.drop(['Close'], axis=1)\n",
        "Y_df = df_final['Close']"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmyHXqsBHDNM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "a8a9bad3-11a6-48f8-8e02-70f8672883cf"
      },
      "source": [
        "df_final.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>Close</th>\n",
              "      <th>1 day before Open</th>\n",
              "      <th>1 day before High</th>\n",
              "      <th>1 day before Low</th>\n",
              "      <th>1 day before Close</th>\n",
              "      <th>1 day before Volume</th>\n",
              "      <th>1 day before Adj Close</th>\n",
              "      <th>2 day before Open</th>\n",
              "      <th>2 day before High</th>\n",
              "      <th>2 day before Low</th>\n",
              "      <th>2 day before Close</th>\n",
              "      <th>2 day before Volume</th>\n",
              "      <th>2 day before Adj Close</th>\n",
              "      <th>3 day before Open</th>\n",
              "      <th>3 day before High</th>\n",
              "      <th>3 day before Low</th>\n",
              "      <th>3 day before Close</th>\n",
              "      <th>3 day before Volume</th>\n",
              "      <th>3 day before Adj Close</th>\n",
              "      <th>4 day before Open</th>\n",
              "      <th>4 day before High</th>\n",
              "      <th>4 day before Low</th>\n",
              "      <th>4 day before Close</th>\n",
              "      <th>4 day before Volume</th>\n",
              "      <th>4 day before Adj Close</th>\n",
              "      <th>5 day before Open</th>\n",
              "      <th>5 day before High</th>\n",
              "      <th>5 day before Low</th>\n",
              "      <th>5 day before Close</th>\n",
              "      <th>5 day before Volume</th>\n",
              "      <th>5 day before Adj Close</th>\n",
              "      <th>6 day before Open</th>\n",
              "      <th>6 day before High</th>\n",
              "      <th>6 day before Low</th>\n",
              "      <th>6 day before Close</th>\n",
              "      <th>6 day before Volume</th>\n",
              "      <th>6 day before Adj Close</th>\n",
              "      <th>7 day before Open</th>\n",
              "      <th>7 day before High</th>\n",
              "      <th>...</th>\n",
              "      <th>57 day before Close</th>\n",
              "      <th>57 day before Volume</th>\n",
              "      <th>57 day before Adj Close</th>\n",
              "      <th>58 day before Open</th>\n",
              "      <th>58 day before High</th>\n",
              "      <th>58 day before Low</th>\n",
              "      <th>58 day before Close</th>\n",
              "      <th>58 day before Volume</th>\n",
              "      <th>58 day before Adj Close</th>\n",
              "      <th>59 day before Open</th>\n",
              "      <th>59 day before High</th>\n",
              "      <th>59 day before Low</th>\n",
              "      <th>59 day before Close</th>\n",
              "      <th>59 day before Volume</th>\n",
              "      <th>59 day before Adj Close</th>\n",
              "      <th>Top1</th>\n",
              "      <th>Top2</th>\n",
              "      <th>Top3</th>\n",
              "      <th>Top4</th>\n",
              "      <th>Top5</th>\n",
              "      <th>Top6</th>\n",
              "      <th>Top7</th>\n",
              "      <th>Top8</th>\n",
              "      <th>Top9</th>\n",
              "      <th>Top10</th>\n",
              "      <th>Top11</th>\n",
              "      <th>Top12</th>\n",
              "      <th>Top13</th>\n",
              "      <th>Top14</th>\n",
              "      <th>Top15</th>\n",
              "      <th>Top16</th>\n",
              "      <th>Top17</th>\n",
              "      <th>Top18</th>\n",
              "      <th>Top19</th>\n",
              "      <th>Top20</th>\n",
              "      <th>Top21</th>\n",
              "      <th>Top22</th>\n",
              "      <th>Top23</th>\n",
              "      <th>Top24</th>\n",
              "      <th>Top25</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.933579</td>\n",
              "      <td>0.938290</td>\n",
              "      <td>0.897638</td>\n",
              "      <td>0.927717</td>\n",
              "      <td>0.904977</td>\n",
              "      <td>0.934995</td>\n",
              "      <td>-0.626052</td>\n",
              "      <td>0.934995</td>\n",
              "      <td>0.854005</td>\n",
              "      <td>0.888874</td>\n",
              "      <td>0.861634</td>\n",
              "      <td>0.894995</td>\n",
              "      <td>-0.706021</td>\n",
              "      <td>0.894995</td>\n",
              "      <td>0.808881</td>\n",
              "      <td>0.838231</td>\n",
              "      <td>0.816642</td>\n",
              "      <td>0.846554</td>\n",
              "      <td>-0.688587</td>\n",
              "      <td>0.846554</td>\n",
              "      <td>0.836872</td>\n",
              "      <td>0.828866</td>\n",
              "      <td>0.795049</td>\n",
              "      <td>0.800745</td>\n",
              "      <td>-0.608918</td>\n",
              "      <td>0.800745</td>\n",
              "      <td>0.937385</td>\n",
              "      <td>0.930469</td>\n",
              "      <td>0.844743</td>\n",
              "      <td>0.845029</td>\n",
              "      <td>-0.308067</td>\n",
              "      <td>0.845029</td>\n",
              "      <td>0.919961</td>\n",
              "      <td>0.94154</td>\n",
              "      <td>0.927397</td>\n",
              "      <td>0.948778</td>\n",
              "      <td>-0.730957</td>\n",
              "      <td>0.948778</td>\n",
              "      <td>0.918017</td>\n",
              "      <td>0.925922</td>\n",
              "      <td>...</td>\n",
              "      <td>0.899512</td>\n",
              "      <td>-0.782119</td>\n",
              "      <td>0.899512</td>\n",
              "      <td>0.876177</td>\n",
              "      <td>0.893533</td>\n",
              "      <td>0.878559</td>\n",
              "      <td>0.87149</td>\n",
              "      <td>-0.70386</td>\n",
              "      <td>0.87149</td>\n",
              "      <td>0.870893</td>\n",
              "      <td>0.887156</td>\n",
              "      <td>0.873858</td>\n",
              "      <td>0.874984</td>\n",
              "      <td>-0.78521</td>\n",
              "      <td>0.874984</td>\n",
              "      <td>-0.5574</td>\n",
              "      <td>-0.0516</td>\n",
              "      <td>0.5719</td>\n",
              "      <td>-0.8658</td>\n",
              "      <td>-0.296</td>\n",
              "      <td>-0.4404</td>\n",
              "      <td>-0.3182</td>\n",
              "      <td>0.5612</td>\n",
              "      <td>-0.7351</td>\n",
              "      <td>-0.2732</td>\n",
              "      <td>-0.8402</td>\n",
              "      <td>-0.6486</td>\n",
              "      <td>-0.4767</td>\n",
              "      <td>0.1779</td>\n",
              "      <td>-0.1027</td>\n",
              "      <td>-0.5859</td>\n",
              "      <td>0.3818</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.4019</td>\n",
              "      <td>-0.3182</td>\n",
              "      <td>-0.9509</td>\n",
              "      <td>-0.3818</td>\n",
              "      <td>-0.9618</td>\n",
              "      <td>-0.9432</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.897638</td>\n",
              "      <td>0.934995</td>\n",
              "      <td>0.854005</td>\n",
              "      <td>0.888874</td>\n",
              "      <td>0.861634</td>\n",
              "      <td>0.894995</td>\n",
              "      <td>-0.706021</td>\n",
              "      <td>0.894995</td>\n",
              "      <td>0.808881</td>\n",
              "      <td>0.838231</td>\n",
              "      <td>0.816642</td>\n",
              "      <td>0.846554</td>\n",
              "      <td>-0.688587</td>\n",
              "      <td>0.846554</td>\n",
              "      <td>0.836872</td>\n",
              "      <td>0.828866</td>\n",
              "      <td>0.795049</td>\n",
              "      <td>0.800745</td>\n",
              "      <td>-0.608918</td>\n",
              "      <td>0.800745</td>\n",
              "      <td>0.937385</td>\n",
              "      <td>0.930469</td>\n",
              "      <td>0.844743</td>\n",
              "      <td>0.845029</td>\n",
              "      <td>-0.308067</td>\n",
              "      <td>0.845029</td>\n",
              "      <td>0.919961</td>\n",
              "      <td>0.94154</td>\n",
              "      <td>0.927397</td>\n",
              "      <td>0.948778</td>\n",
              "      <td>-0.730957</td>\n",
              "      <td>0.948778</td>\n",
              "      <td>0.918017</td>\n",
              "      <td>0.925922</td>\n",
              "      <td>0.9149</td>\n",
              "      <td>0.90964</td>\n",
              "      <td>-0.756853</td>\n",
              "      <td>0.90964</td>\n",
              "      <td>0.917109</td>\n",
              "      <td>0.918651</td>\n",
              "      <td>...</td>\n",
              "      <td>0.87149</td>\n",
              "      <td>-0.70386</td>\n",
              "      <td>0.87149</td>\n",
              "      <td>0.870893</td>\n",
              "      <td>0.887156</td>\n",
              "      <td>0.873858</td>\n",
              "      <td>0.874984</td>\n",
              "      <td>-0.78521</td>\n",
              "      <td>0.874984</td>\n",
              "      <td>0.893308</td>\n",
              "      <td>0.885914</td>\n",
              "      <td>0.866414</td>\n",
              "      <td>0.869034</td>\n",
              "      <td>-0.754812</td>\n",
              "      <td>0.869034</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.4141</td>\n",
              "      <td>0.4019</td>\n",
              "      <td>-0.8934</td>\n",
              "      <td>-0.6124</td>\n",
              "      <td>-0.91</td>\n",
              "      <td>-0.3612</td>\n",
              "      <td>0.7003</td>\n",
              "      <td>-0.8402</td>\n",
              "      <td>-0.7096</td>\n",
              "      <td>0.6705</td>\n",
              "      <td>0.4019</td>\n",
              "      <td>-0.5423</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.3182</td>\n",
              "      <td>0.7351</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.7579</td>\n",
              "      <td>-0.3182</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.9578</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.872</td>\n",
              "      <td>-0.5423</td>\n",
              "      <td>-0.875</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 381 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Open     Close 1 day before Open  ...   Top23   Top24  Top25\n",
              "0  0.933579  0.938290          0.897638  ... -0.9618 -0.9432      0\n",
              "1  0.897638  0.934995          0.854005  ...  -0.872 -0.5423 -0.875\n",
              "\n",
              "[2 rows x 381 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jjkCDh51cOd"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(X_df, Y_df, test_size=0.3, random_state=1)\n",
        "import tensorflow as tf"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQffrPhz3ZRU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "656e6ebf-b776-4b7e-9208-1b99e1b711ed"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0jZT19bZcfD"
      },
      "source": [
        "y_train = np.array(y_train,ndmin=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxjOxcZ21Ap6"
      },
      "source": [
        "features = x_train.shape[1]\n",
        "x = tf.placeholder(tf.float32, shape=[x_train.shape[0],x_train.shape[1]])\n",
        "y_ = tf.placeholder(tf.float32, shape=[y_train.shape[0],y_train.shape[1]])\n",
        "W = tf.Variable(tf.zeros([features,1]))\n",
        "b = tf.Variable(tf.zeros([1]))\n",
        "predict = tf.matmul(x,W) + b\n",
        "loss = tf.reduce_mean(tf.pow(predict - y_, 2))\n",
        "update = tf.train.GradientDescentOptimizer(0.005).minimize(loss)\n",
        "data_x = x_train.to_numpy()\n",
        "data_y = y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2anyebtU_LP"
      },
      "source": [
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGphoG1d_6q8"
      },
      "source": [
        "for i in range(0,5000):\n",
        "    sess.run(update, feed_dict = {x:data_x, y_:data_y})\n",
        "    # if i+1 % 10 == 0 :\n",
        "    #     print('Iteration:' , i+1 , ' W(1->5):' , sess.run(W[0:5]) , ' b:' , sess.run(b), ' loss:', loss.eval(session=sess, feed_dict = {x:data_x, y_:data_y}))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeuQdHMBhS43"
      },
      "source": [
        "y_test = np.array(y_test, ndmin=2)\n",
        "x_test_p = tf.placeholder(tf.float32, shape=[None,x_test.shape[1]])\n",
        "y_test_p = tf.placeholder(tf.float32, shape=[None,y_test.shape[1]])\n",
        "predict = tf.matmul(x_test_p,W) + b\n",
        "loss = tf.reduce_mean(tf.pow(predict - y_test_p, 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhsXbLHrhS7s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "866bfa49-fa58-476e-9716-346736615f48"
      },
      "source": [
        "sess.run(loss, feed_dict = {x_test_p: x_test, y_test_p: y_test})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.28245184"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASRzbReOZBV3"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(X_df, Y_df, test_size=0.3, random_state=1)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hy3NsGSLOU0i"
      },
      "source": [
        "train_size = x_train.shape[0]\n",
        "test_size = x_test.shape[0]"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcwH3YPEc9RQ"
      },
      "source": [
        "# https://stackoverflow.com/questions/40994583/how-to-implement-tensorflows-next-batch-for-own-data\n",
        "def get_batch(num, data, labels):\n",
        "    data['y'] = labels\n",
        "    data = data.sample(num)\n",
        "    data_shuffle = data.sample(num)\n",
        "    labels_shuffle = np.array(data['y'], ndmin=2)\n",
        "    return np.asarray(data_shuffle), labels_shuffle.reshape((data.shape[0], 1))"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCQ7gkN3KoVJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wc7uHRWYKorM"
      },
      "source": [
        "y_train = np.array(y_train,ndmin=2)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oX45HVuNSgIK"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h78yXovxc9VT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a2817aa-56e6-4dd0-9546-1c0fa2c0dddd"
      },
      "source": [
        "step_size = 0.00005\n",
        "(hidden1_size, hidden2_size, hidden3_size, hidden4_size, hidden5_size) = (612, 256, 128, 64, 8)\n",
        "\n",
        "x = tf.placeholder(tf.float32, shape=[None, x_train.shape[1]])\n",
        "y_ = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "\n",
        "W1 = tf.Variable(tf.truncated_normal([x_train.shape[1], hidden1_size], stddev=0.1))\n",
        "b1 = tf.Variable(tf.constant(0.01, shape=[hidden1_size]))\n",
        "z1 = tf.nn.relu(tf.matmul(x,W1)+b1)\n",
        "\n",
        "# W2 = tf.Variable(tf.truncated_normal([hidden1_size, 1], stddev=0.1))\n",
        "# b2 = tf.Variable(tf.constant(0.1, shape=[1]))\n",
        "W2 = tf.Variable(tf.truncated_normal([hidden1_size, hidden2_size], stddev=0.1))\n",
        "b2 = tf.Variable(tf.constant(0.1, shape=[hidden2_size]))\n",
        "z2 = tf.nn.relu(tf.matmul(z1,W2)+b2)\n",
        "\n",
        "# W3 = tf.Variable(tf.truncated_normal([hidden2_size, 1], stddev=0.1))\n",
        "# b3 = tf.Variable(tf.constant(0.1, shape=[1]))\n",
        "W3 = tf.Variable(tf.truncated_normal([hidden2_size, hidden3_size], stddev=0.1))\n",
        "b3 = tf.Variable(tf.constant(0.01, shape=[hidden3_size]))\n",
        "z3 = tf.nn.relu(tf.matmul(z2,W3)+b3)\n",
        "\n",
        "# W4 = tf.Variable(tf.truncated_normal([hidden3_size, 1], stddev=0.1))\n",
        "# b4 = tf.Variable(tf.constant(0.01, shape=[1]))\n",
        "W4 = tf.Variable(tf.truncated_normal([hidden3_size, hidden4_size], stddev=0.1))\n",
        "b4 = tf.Variable(tf.constant(0.01, shape=[hidden4_size]))\n",
        "z4 = tf.nn.relu(tf.matmul(z3,W4)+b4)\n",
        "\n",
        "W5 = tf.Variable(tf.truncated_normal([hidden4_size, hidden5_size], stddev=0.1))\n",
        "b5 = tf.Variable(tf.constant(0.01, shape=[hidden5_size]))\n",
        "z5 = tf.nn.relu(tf.matmul(z4,W5)+b5)\n",
        "\n",
        "W6 = tf.Variable(tf.truncated_normal([hidden5_size, 1], stddev=0.1))\n",
        "b6 = tf.Variable(tf.constant(0.01, shape=[1]))\n",
        "\n",
        "predict = tf.matmul(z5,W6) + b6\n",
        "# predict = tf.matmul(z4,W5) + b5\n",
        "# predict = tf.matmul(z3,W4) + b4\n",
        "# predict = tf.matmul(z2,W3) + b3\n",
        "# predict = tf.matmul(z1,W2) + b2\n",
        "\n",
        "loss = tf.reduce_mean(tf.pow(predict - y_, 2))\n",
        "train_step = tf.train.GradientDescentOptimizer(step_size).minimize(loss)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "sess = tf.Session()\n",
        "train_losses = []\n",
        "sess.run(init)\n",
        "test_losses = [] \n",
        "batch_xn, batch_yn = get_batch(train_size, x_train, y_train)\n",
        "batch_xt, batch_yt = get_batch(test_size, x_test, y_test)\n",
        "iter_plot = 200\n",
        "for i in range (iter_plot):\n",
        "    for _ in range(50):\n",
        "      sess.run(train_step, feed_dict={x: batch_xn, y_: batch_yn})\n",
        "    temp_loss = sess.run(loss, feed_dict={x: batch_xn, y_: batch_yn})\n",
        "    train_losses.append(temp_loss)\n",
        "    temp_loss = sess.run(loss, feed_dict={x: batch_xt, y_: batch_yt})\n",
        "    test_losses.append(temp_loss)\n",
        "    print('iter:', i+1,', loss:', temp_loss)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iter: 1 , loss: 0.2956264\n",
            "iter: 2 , loss: 0.29524758\n",
            "iter: 3 , loss: 0.29487863\n",
            "iter: 4 , loss: 0.29451916\n",
            "iter: 5 , loss: 0.29416916\n",
            "iter: 6 , loss: 0.29382882\n",
            "iter: 7 , loss: 0.2934976\n",
            "iter: 8 , loss: 0.2931756\n",
            "iter: 9 , loss: 0.29286256\n",
            "iter: 10 , loss: 0.29255855\n",
            "iter: 11 , loss: 0.2922632\n",
            "iter: 12 , loss: 0.29197624\n",
            "iter: 13 , loss: 0.29169735\n",
            "iter: 14 , loss: 0.29142642\n",
            "iter: 15 , loss: 0.29116333\n",
            "iter: 16 , loss: 0.29090804\n",
            "iter: 17 , loss: 0.2906603\n",
            "iter: 18 , loss: 0.29041973\n",
            "iter: 19 , loss: 0.29018635\n",
            "iter: 20 , loss: 0.2899599\n",
            "iter: 21 , loss: 0.28974035\n",
            "iter: 22 , loss: 0.28952724\n",
            "iter: 23 , loss: 0.2893207\n",
            "iter: 24 , loss: 0.2891208\n",
            "iter: 25 , loss: 0.28892767\n",
            "iter: 26 , loss: 0.2887408\n",
            "iter: 27 , loss: 0.2885599\n",
            "iter: 28 , loss: 0.28838462\n",
            "iter: 29 , loss: 0.28821507\n",
            "iter: 30 , loss: 0.288051\n",
            "iter: 31 , loss: 0.2878927\n",
            "iter: 32 , loss: 0.28773963\n",
            "iter: 33 , loss: 0.2875917\n",
            "iter: 34 , loss: 0.28744882\n",
            "iter: 35 , loss: 0.2873109\n",
            "iter: 36 , loss: 0.28717765\n",
            "iter: 37 , loss: 0.28704903\n",
            "iter: 38 , loss: 0.28692484\n",
            "iter: 39 , loss: 0.286805\n",
            "iter: 40 , loss: 0.2866894\n",
            "iter: 41 , loss: 0.28657776\n",
            "iter: 42 , loss: 0.2864702\n",
            "iter: 43 , loss: 0.2863665\n",
            "iter: 44 , loss: 0.28626668\n",
            "iter: 45 , loss: 0.28617054\n",
            "iter: 46 , loss: 0.28607816\n",
            "iter: 47 , loss: 0.28598922\n",
            "iter: 48 , loss: 0.2859036\n",
            "iter: 49 , loss: 0.2858212\n",
            "iter: 50 , loss: 0.28574196\n",
            "iter: 51 , loss: 0.2856657\n",
            "iter: 52 , loss: 0.2855925\n",
            "iter: 53 , loss: 0.28552207\n",
            "iter: 54 , loss: 0.28545442\n",
            "iter: 55 , loss: 0.28538933\n",
            "iter: 56 , loss: 0.28532684\n",
            "iter: 57 , loss: 0.28526682\n",
            "iter: 58 , loss: 0.28520915\n",
            "iter: 59 , loss: 0.2851538\n",
            "iter: 60 , loss: 0.2851006\n",
            "iter: 61 , loss: 0.28504956\n",
            "iter: 62 , loss: 0.2850006\n",
            "iter: 63 , loss: 0.28495345\n",
            "iter: 64 , loss: 0.28490835\n",
            "iter: 65 , loss: 0.28486526\n",
            "iter: 66 , loss: 0.28482392\n",
            "iter: 67 , loss: 0.28478438\n",
            "iter: 68 , loss: 0.2847465\n",
            "iter: 69 , loss: 0.28471032\n",
            "iter: 70 , loss: 0.28467572\n",
            "iter: 71 , loss: 0.28464267\n",
            "iter: 72 , loss: 0.28461108\n",
            "iter: 73 , loss: 0.28458086\n",
            "iter: 74 , loss: 0.28455207\n",
            "iter: 75 , loss: 0.28452456\n",
            "iter: 76 , loss: 0.28449818\n",
            "iter: 77 , loss: 0.28447312\n",
            "iter: 78 , loss: 0.2844491\n",
            "iter: 79 , loss: 0.2844262\n",
            "iter: 80 , loss: 0.2844044\n",
            "iter: 81 , loss: 0.28438357\n",
            "iter: 82 , loss: 0.28436372\n",
            "iter: 83 , loss: 0.28434488\n",
            "iter: 84 , loss: 0.2843269\n",
            "iter: 85 , loss: 0.28430977\n",
            "iter: 86 , loss: 0.28429344\n",
            "iter: 87 , loss: 0.2842779\n",
            "iter: 88 , loss: 0.28426304\n",
            "iter: 89 , loss: 0.28424898\n",
            "iter: 90 , loss: 0.28423557\n",
            "iter: 91 , loss: 0.28422284\n",
            "iter: 92 , loss: 0.28421077\n",
            "iter: 93 , loss: 0.28419927\n",
            "iter: 94 , loss: 0.28418836\n",
            "iter: 95 , loss: 0.284178\n",
            "iter: 96 , loss: 0.28416818\n",
            "iter: 97 , loss: 0.28415895\n",
            "iter: 98 , loss: 0.28415018\n",
            "iter: 99 , loss: 0.2841418\n",
            "iter: 100 , loss: 0.28413394\n",
            "iter: 101 , loss: 0.28412643\n",
            "iter: 102 , loss: 0.2841193\n",
            "iter: 103 , loss: 0.2841126\n",
            "iter: 104 , loss: 0.28410622\n",
            "iter: 105 , loss: 0.28410023\n",
            "iter: 106 , loss: 0.2840945\n",
            "iter: 107 , loss: 0.2840891\n",
            "iter: 108 , loss: 0.284084\n",
            "iter: 109 , loss: 0.28407913\n",
            "iter: 110 , loss: 0.28407452\n",
            "iter: 111 , loss: 0.28407016\n",
            "iter: 112 , loss: 0.284066\n",
            "iter: 113 , loss: 0.28406206\n",
            "iter: 114 , loss: 0.28405827\n",
            "iter: 115 , loss: 0.28405476\n",
            "iter: 116 , loss: 0.28405136\n",
            "iter: 117 , loss: 0.2840481\n",
            "iter: 118 , loss: 0.28404504\n",
            "iter: 119 , loss: 0.28404215\n",
            "iter: 120 , loss: 0.28403938\n",
            "iter: 121 , loss: 0.2840368\n",
            "iter: 122 , loss: 0.2840343\n",
            "iter: 123 , loss: 0.28403196\n",
            "iter: 124 , loss: 0.28402972\n",
            "iter: 125 , loss: 0.28402755\n",
            "iter: 126 , loss: 0.28402546\n",
            "iter: 127 , loss: 0.2840235\n",
            "iter: 128 , loss: 0.28402162\n",
            "iter: 129 , loss: 0.28401983\n",
            "iter: 130 , loss: 0.2840181\n",
            "iter: 131 , loss: 0.28401652\n",
            "iter: 132 , loss: 0.28401494\n",
            "iter: 133 , loss: 0.28401342\n",
            "iter: 134 , loss: 0.284012\n",
            "iter: 135 , loss: 0.2840106\n",
            "iter: 136 , loss: 0.28400922\n",
            "iter: 137 , loss: 0.2840079\n",
            "iter: 138 , loss: 0.28400666\n",
            "iter: 139 , loss: 0.2840054\n",
            "iter: 140 , loss: 0.28400415\n",
            "iter: 141 , loss: 0.284003\n",
            "iter: 142 , loss: 0.28400186\n",
            "iter: 143 , loss: 0.28400075\n",
            "iter: 144 , loss: 0.28399965\n",
            "iter: 145 , loss: 0.28399852\n",
            "iter: 146 , loss: 0.28399742\n",
            "iter: 147 , loss: 0.28399628\n",
            "iter: 148 , loss: 0.28399518\n",
            "iter: 149 , loss: 0.28399408\n",
            "iter: 150 , loss: 0.28399298\n",
            "iter: 151 , loss: 0.2839919\n",
            "iter: 152 , loss: 0.2839908\n",
            "iter: 153 , loss: 0.28398967\n",
            "iter: 154 , loss: 0.2839885\n",
            "iter: 155 , loss: 0.28398737\n",
            "iter: 156 , loss: 0.2839862\n",
            "iter: 157 , loss: 0.28398508\n",
            "iter: 158 , loss: 0.28398386\n",
            "iter: 159 , loss: 0.28398263\n",
            "iter: 160 , loss: 0.28398144\n",
            "iter: 161 , loss: 0.2839803\n",
            "iter: 162 , loss: 0.28397912\n",
            "iter: 163 , loss: 0.28397793\n",
            "iter: 164 , loss: 0.2839768\n",
            "iter: 165 , loss: 0.2839756\n",
            "iter: 166 , loss: 0.2839744\n",
            "iter: 167 , loss: 0.28397325\n",
            "iter: 168 , loss: 0.2839721\n",
            "iter: 169 , loss: 0.28397095\n",
            "iter: 170 , loss: 0.28396976\n",
            "iter: 171 , loss: 0.28396854\n",
            "iter: 172 , loss: 0.28396735\n",
            "iter: 173 , loss: 0.28396612\n",
            "iter: 174 , loss: 0.28396487\n",
            "iter: 175 , loss: 0.28396365\n",
            "iter: 176 , loss: 0.28396234\n",
            "iter: 177 , loss: 0.2839611\n",
            "iter: 178 , loss: 0.28395978\n",
            "iter: 179 , loss: 0.2839585\n",
            "iter: 180 , loss: 0.2839572\n",
            "iter: 181 , loss: 0.28395587\n",
            "iter: 182 , loss: 0.2839545\n",
            "iter: 183 , loss: 0.28395307\n",
            "iter: 184 , loss: 0.2839517\n",
            "iter: 185 , loss: 0.28395033\n",
            "iter: 186 , loss: 0.28394893\n",
            "iter: 187 , loss: 0.28394753\n",
            "iter: 188 , loss: 0.28394613\n",
            "iter: 189 , loss: 0.28394464\n",
            "iter: 190 , loss: 0.28394318\n",
            "iter: 191 , loss: 0.28394172\n",
            "iter: 192 , loss: 0.28394026\n",
            "iter: 193 , loss: 0.28393877\n",
            "iter: 194 , loss: 0.28393725\n",
            "iter: 195 , loss: 0.28393576\n",
            "iter: 196 , loss: 0.2839342\n",
            "iter: 197 , loss: 0.28393263\n",
            "iter: 198 , loss: 0.28393108\n",
            "iter: 199 , loss: 0.28392944\n",
            "iter: 200 , loss: 0.28392783\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRsC5OC-SgLF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "75ae0c82-2a25-4ee6-b66c-e7095e4e26ee"
      },
      "source": [
        "iter = np.arange(1,iter_plot+1)\n",
        "plt.title('Testing Mean Squerd Error minimization')\n",
        "plt.plot(iter, test_losses, label='Testing loss')\n",
        "plt.plot(iter, train_losses, label='Training loss')\n",
        "plt.legend()\n",
        "plt.xlabel('Number of iterations')\n",
        "plt.ylabel('Mean Squerd Error')\n",
        "plt.show()"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Khlsn3AvSgOR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxXANjQ61vex"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0uQG8nzLzN_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HFYlKQKL5OB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8RIQ4WCL5KG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}