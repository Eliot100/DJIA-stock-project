{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DJIA stock project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDUQkKf_jq-p"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8X7Wvabnrfg"
      },
      "source": [
        "df_RedditNews = pd.read_csv('https://raw.githubusercontent.com/Eliot100/DJIA-stock-project/main/RedditNews.csv')\n",
        "df_DJIA = pd.read_csv('https://raw.githubusercontent.com/Eliot100/DJIA-stock-project/main/upload_DJIA_table.csv')\n",
        "df_Combined_News_DJIA = pd.read_csv('https://raw.githubusercontent.com/Eliot100/DJIA-stock-project/main/Combined_News_DJIA.csv')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "08KB3HKENk3T",
        "outputId": "622e8e3c-9653-476d-f8df-2317e534d2ad"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "\n",
        "df_DJIA2 = df_DJIA.copy()\n",
        "df_DJIA2[['Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']] = \\\n",
        "  scaler.fit_transform(df_DJIA2[['Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']])\n",
        "  \n",
        "df_DJIA2.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Adj Close</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2016-07-01</td>\n",
              "      <td>0.933579</td>\n",
              "      <td>0.940047</td>\n",
              "      <td>0.939734</td>\n",
              "      <td>0.938290</td>\n",
              "      <td>-0.778698</td>\n",
              "      <td>0.938290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2016-06-30</td>\n",
              "      <td>0.897638</td>\n",
              "      <td>0.927717</td>\n",
              "      <td>0.904977</td>\n",
              "      <td>0.934995</td>\n",
              "      <td>-0.626052</td>\n",
              "      <td>0.934995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2016-06-29</td>\n",
              "      <td>0.854005</td>\n",
              "      <td>0.888874</td>\n",
              "      <td>0.861634</td>\n",
              "      <td>0.894995</td>\n",
              "      <td>-0.706021</td>\n",
              "      <td>0.894995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2016-06-28</td>\n",
              "      <td>0.808881</td>\n",
              "      <td>0.838231</td>\n",
              "      <td>0.816642</td>\n",
              "      <td>0.846554</td>\n",
              "      <td>-0.688587</td>\n",
              "      <td>0.846554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2016-06-27</td>\n",
              "      <td>0.836872</td>\n",
              "      <td>0.828866</td>\n",
              "      <td>0.795049</td>\n",
              "      <td>0.800745</td>\n",
              "      <td>-0.608918</td>\n",
              "      <td>0.800745</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Date      Open      High       Low     Close    Volume  Adj Close\n",
              "0  2016-07-01  0.933579  0.940047  0.939734  0.938290 -0.778698   0.938290\n",
              "1  2016-06-30  0.897638  0.927717  0.904977  0.934995 -0.626052   0.934995\n",
              "2  2016-06-29  0.854005  0.888874  0.861634  0.894995 -0.706021   0.894995\n",
              "3  2016-06-28  0.808881  0.838231  0.816642  0.846554 -0.688587   0.846554\n",
              "4  2016-06-27  0.836872  0.828866  0.795049  0.800745 -0.608918   0.800745"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXldyEiwMHFQ",
        "outputId": "ee6614d2-e93c-434a-a248-63f86c95e7a6"
      },
      "source": [
        "df_Combined = df_DJIA2.copy()\n",
        "for i in range(0,59):\n",
        "  df_Combined[str(i+1)+\" day before Open\"] = \"\"\n",
        "  df_Combined[str(i+1)+\" day before High\"] = \"\"\n",
        "  df_Combined[str(i+1)+\" day before Low\"] = \"\"\n",
        "  df_Combined[str(i+1)+\" day before Close\"] = \"\"\n",
        "  df_Combined[str(i+1)+\" day before Volume\"] = \"\"\n",
        "  df_Combined[str(i+1)+\" day before Adj Close\"] = \"\"\n",
        "\n",
        "for i in range(0,25):\n",
        "  df_Combined[\"Top\"+str(i+1)] = \"\"\n",
        "\n",
        "for j in range(0, df_DJIA2.shape[0]-59):\n",
        "  for i in range(0, 59):\n",
        "    df_Combined[str(i+1)+\" day before Open\"][j] = df_Combined[\"Open\"][j+i+1]\n",
        "    df_Combined[str(i+1)+\" day before High\"][j] = df_Combined[\"High\"][j+i+1]\n",
        "    df_Combined[str(i+1)+\" day before Low\"][j] = df_Combined[\"Low\"][j+i+1]\n",
        "    df_Combined[str(i+1)+\" day before Close\"][j] = df_Combined[\"Close\"][j+i+1]\n",
        "    df_Combined[str(i+1)+\" day before Volume\"][j] = df_Combined[\"Volume\"][j+i+1]\n",
        "    df_Combined[str(i+1)+\" day before Adj Close\"][j] = df_Combined[\"Adj Close\"][j+i+1]\n",
        "\n",
        "for i in range(0,25):\n",
        "  df_Combined[\"Top\"+str(i+1)] = \"\"\n",
        "\n",
        "for i in range(0, df_DJIA2.shape[0]):\n",
        "  News_Date_array = df_RedditNews[df_RedditNews[\"Date\"] == df_Combined[\"Date\"][i]][\"News\"].to_numpy()\n",
        "  for j in range(0, News_Date_array.shape[0]):\n",
        "    df_Combined[\"Top\"+str(j+1)][i] = News_Date_array[j]\n",
        "\n",
        "df_Combined = df_Combined[:-59]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "Te1WmkTyv64J",
        "outputId": "ff826369-6158-4a9c-f72d-bca6091f28b1"
      },
      "source": [
        "df_Combined.head(2)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>1 day before Open</th>\n",
              "      <th>1 day before High</th>\n",
              "      <th>1 day before Low</th>\n",
              "      <th>1 day before Close</th>\n",
              "      <th>1 day before Volume</th>\n",
              "      <th>1 day before Adj Close</th>\n",
              "      <th>2 day before Open</th>\n",
              "      <th>2 day before High</th>\n",
              "      <th>2 day before Low</th>\n",
              "      <th>2 day before Close</th>\n",
              "      <th>2 day before Volume</th>\n",
              "      <th>2 day before Adj Close</th>\n",
              "      <th>3 day before Open</th>\n",
              "      <th>3 day before High</th>\n",
              "      <th>3 day before Low</th>\n",
              "      <th>3 day before Close</th>\n",
              "      <th>3 day before Volume</th>\n",
              "      <th>3 day before Adj Close</th>\n",
              "      <th>4 day before Open</th>\n",
              "      <th>4 day before High</th>\n",
              "      <th>4 day before Low</th>\n",
              "      <th>4 day before Close</th>\n",
              "      <th>4 day before Volume</th>\n",
              "      <th>4 day before Adj Close</th>\n",
              "      <th>5 day before Open</th>\n",
              "      <th>5 day before High</th>\n",
              "      <th>5 day before Low</th>\n",
              "      <th>5 day before Close</th>\n",
              "      <th>5 day before Volume</th>\n",
              "      <th>5 day before Adj Close</th>\n",
              "      <th>6 day before Open</th>\n",
              "      <th>6 day before High</th>\n",
              "      <th>6 day before Low</th>\n",
              "      <th>...</th>\n",
              "      <th>57 day before Close</th>\n",
              "      <th>57 day before Volume</th>\n",
              "      <th>57 day before Adj Close</th>\n",
              "      <th>58 day before Open</th>\n",
              "      <th>58 day before High</th>\n",
              "      <th>58 day before Low</th>\n",
              "      <th>58 day before Close</th>\n",
              "      <th>58 day before Volume</th>\n",
              "      <th>58 day before Adj Close</th>\n",
              "      <th>59 day before Open</th>\n",
              "      <th>59 day before High</th>\n",
              "      <th>59 day before Low</th>\n",
              "      <th>59 day before Close</th>\n",
              "      <th>59 day before Volume</th>\n",
              "      <th>59 day before Adj Close</th>\n",
              "      <th>Top1</th>\n",
              "      <th>Top2</th>\n",
              "      <th>Top3</th>\n",
              "      <th>Top4</th>\n",
              "      <th>Top5</th>\n",
              "      <th>Top6</th>\n",
              "      <th>Top7</th>\n",
              "      <th>Top8</th>\n",
              "      <th>Top9</th>\n",
              "      <th>Top10</th>\n",
              "      <th>Top11</th>\n",
              "      <th>Top12</th>\n",
              "      <th>Top13</th>\n",
              "      <th>Top14</th>\n",
              "      <th>Top15</th>\n",
              "      <th>Top16</th>\n",
              "      <th>Top17</th>\n",
              "      <th>Top18</th>\n",
              "      <th>Top19</th>\n",
              "      <th>Top20</th>\n",
              "      <th>Top21</th>\n",
              "      <th>Top22</th>\n",
              "      <th>Top23</th>\n",
              "      <th>Top24</th>\n",
              "      <th>Top25</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2016-07-01</td>\n",
              "      <td>0.933579</td>\n",
              "      <td>0.940047</td>\n",
              "      <td>0.939734</td>\n",
              "      <td>0.938290</td>\n",
              "      <td>-0.778698</td>\n",
              "      <td>0.938290</td>\n",
              "      <td>0.897638</td>\n",
              "      <td>0.927717</td>\n",
              "      <td>0.904977</td>\n",
              "      <td>0.934995</td>\n",
              "      <td>-0.626052</td>\n",
              "      <td>0.934995</td>\n",
              "      <td>0.854005</td>\n",
              "      <td>0.888874</td>\n",
              "      <td>0.861634</td>\n",
              "      <td>0.894995</td>\n",
              "      <td>-0.706021</td>\n",
              "      <td>0.894995</td>\n",
              "      <td>0.808881</td>\n",
              "      <td>0.838231</td>\n",
              "      <td>0.816642</td>\n",
              "      <td>0.846554</td>\n",
              "      <td>-0.688587</td>\n",
              "      <td>0.846554</td>\n",
              "      <td>0.836872</td>\n",
              "      <td>0.828866</td>\n",
              "      <td>0.795049</td>\n",
              "      <td>0.800745</td>\n",
              "      <td>-0.608918</td>\n",
              "      <td>0.800745</td>\n",
              "      <td>0.937385</td>\n",
              "      <td>0.930469</td>\n",
              "      <td>0.844743</td>\n",
              "      <td>0.845029</td>\n",
              "      <td>-0.308067</td>\n",
              "      <td>0.845029</td>\n",
              "      <td>0.919961</td>\n",
              "      <td>0.94154</td>\n",
              "      <td>0.927397</td>\n",
              "      <td>...</td>\n",
              "      <td>0.899512</td>\n",
              "      <td>-0.782119</td>\n",
              "      <td>0.899512</td>\n",
              "      <td>0.876177</td>\n",
              "      <td>0.893533</td>\n",
              "      <td>0.878559</td>\n",
              "      <td>0.87149</td>\n",
              "      <td>-0.70386</td>\n",
              "      <td>0.87149</td>\n",
              "      <td>0.870893</td>\n",
              "      <td>0.887156</td>\n",
              "      <td>0.873858</td>\n",
              "      <td>0.874984</td>\n",
              "      <td>-0.78521</td>\n",
              "      <td>0.874984</td>\n",
              "      <td>A 117-year-old woman in Mexico City finally re...</td>\n",
              "      <td>IMF chief backs Athens as permanent Olympic host</td>\n",
              "      <td>The president of France says if Brexit won, so...</td>\n",
              "      <td>British Man Who Must Give Police 24 Hours' Not...</td>\n",
              "      <td>100+ Nobel laureates urge Greenpeace to stop o...</td>\n",
              "      <td>Brazil: Huge spike in number of police killing...</td>\n",
              "      <td>Austria's highest court annuls presidential el...</td>\n",
              "      <td>Facebook wins privacy case, can track any Belg...</td>\n",
              "      <td>Switzerland denies Muslim girls citizenship af...</td>\n",
              "      <td>China kills millions of innocent meditators fo...</td>\n",
              "      <td>France Cracks Down on Factory Farms - A viral ...</td>\n",
              "      <td>Abbas PLO Faction Calls Killer of 13-Year-Old ...</td>\n",
              "      <td>Taiwanese warship accidentally fires missile t...</td>\n",
              "      <td>Iran celebrates American Human Rights Week, mo...</td>\n",
              "      <td>U.N. panel moves to curb bias against L.G.B.T....</td>\n",
              "      <td>The United States has placed Myanmar, Uzbekist...</td>\n",
              "      <td>S&amp;amp;P revises European Union credit rating t...</td>\n",
              "      <td>India gets $1 billion loan from World Bank for...</td>\n",
              "      <td>U.S. sailors detained by Iran spoke too much u...</td>\n",
              "      <td>Mass fish kill in Vietnam solved as Taiwan ste...</td>\n",
              "      <td>Philippines president Rodrigo Duterte urges pe...</td>\n",
              "      <td>Spain arrests three Pakistanis accused of prom...</td>\n",
              "      <td>Venezuela, where anger over food shortages is ...</td>\n",
              "      <td>A Hindu temple worker has been killed by three...</td>\n",
              "      <td>Ozone layer hole seems to be healing - US &amp;amp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2016-06-30</td>\n",
              "      <td>0.897638</td>\n",
              "      <td>0.927717</td>\n",
              "      <td>0.904977</td>\n",
              "      <td>0.934995</td>\n",
              "      <td>-0.626052</td>\n",
              "      <td>0.934995</td>\n",
              "      <td>0.854005</td>\n",
              "      <td>0.888874</td>\n",
              "      <td>0.861634</td>\n",
              "      <td>0.894995</td>\n",
              "      <td>-0.706021</td>\n",
              "      <td>0.894995</td>\n",
              "      <td>0.808881</td>\n",
              "      <td>0.838231</td>\n",
              "      <td>0.816642</td>\n",
              "      <td>0.846554</td>\n",
              "      <td>-0.688587</td>\n",
              "      <td>0.846554</td>\n",
              "      <td>0.836872</td>\n",
              "      <td>0.828866</td>\n",
              "      <td>0.795049</td>\n",
              "      <td>0.800745</td>\n",
              "      <td>-0.608918</td>\n",
              "      <td>0.800745</td>\n",
              "      <td>0.937385</td>\n",
              "      <td>0.930469</td>\n",
              "      <td>0.844743</td>\n",
              "      <td>0.845029</td>\n",
              "      <td>-0.308067</td>\n",
              "      <td>0.845029</td>\n",
              "      <td>0.919961</td>\n",
              "      <td>0.94154</td>\n",
              "      <td>0.927397</td>\n",
              "      <td>0.948778</td>\n",
              "      <td>-0.730957</td>\n",
              "      <td>0.948778</td>\n",
              "      <td>0.918017</td>\n",
              "      <td>0.925922</td>\n",
              "      <td>0.9149</td>\n",
              "      <td>...</td>\n",
              "      <td>0.87149</td>\n",
              "      <td>-0.70386</td>\n",
              "      <td>0.87149</td>\n",
              "      <td>0.870893</td>\n",
              "      <td>0.887156</td>\n",
              "      <td>0.873858</td>\n",
              "      <td>0.874984</td>\n",
              "      <td>-0.78521</td>\n",
              "      <td>0.874984</td>\n",
              "      <td>0.893308</td>\n",
              "      <td>0.885914</td>\n",
              "      <td>0.866414</td>\n",
              "      <td>0.869034</td>\n",
              "      <td>-0.754812</td>\n",
              "      <td>0.869034</td>\n",
              "      <td>Jamaica proposes marijuana dispensers for tour...</td>\n",
              "      <td>Stephen Hawking says pollution and 'stupidity'...</td>\n",
              "      <td>Boris Johnson says he will not run for Tory pa...</td>\n",
              "      <td>Six gay men in Ivory Coast were abused and for...</td>\n",
              "      <td>Switzerland denies citizenship to Muslim immig...</td>\n",
              "      <td>Palestinian terrorist stabs israeli teen girl ...</td>\n",
              "      <td>Puerto Rico will default on $1 billion of debt...</td>\n",
              "      <td>Republic of Ireland fans to be awarded medal f...</td>\n",
              "      <td>Afghan suicide bomber 'kills up to 40' - BBC News</td>\n",
              "      <td>US airstrikes kill at least 250 ISIS fighters ...</td>\n",
              "      <td>Turkish Cop Who Took Down Istanbul Gunman Hail...</td>\n",
              "      <td>Cannabis compounds could treat Alzheimer's by ...</td>\n",
              "      <td>Japan's top court has approved blanket surveil...</td>\n",
              "      <td>CIA Gave Romania Millions to Host Secret Prisons</td>\n",
              "      <td>Groups urge U.N. to suspend Saudi Arabia from ...</td>\n",
              "      <td>Googles free wifi at Indian railway stations i...</td>\n",
              "      <td>Mounting evidence suggests 'hobbits' were wipe...</td>\n",
              "      <td>The men who carried out Tuesday's terror attac...</td>\n",
              "      <td>Calls to suspend Saudi Arabia from UN Human Ri...</td>\n",
              "      <td>More Than 100 Nobel Laureates Call Out Greenpe...</td>\n",
              "      <td>British pedophile sentenced to 85 years in US ...</td>\n",
              "      <td>US permitted 1,200 offshore fracks in Gulf of ...</td>\n",
              "      <td>We will be swimming in ridicule - French beach...</td>\n",
              "      <td>UEFA says no minutes of silence for Istanbul v...</td>\n",
              "      <td>Law Enforcement Sources: Gun Used in Paris Ter...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 386 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         Date  ...                                              Top25\n",
              "0  2016-07-01  ...  Ozone layer hole seems to be healing - US &amp...\n",
              "1  2016-06-30  ...  Law Enforcement Sources: Gun Used in Paris Ter...\n",
              "\n",
              "[2 rows x 386 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PPyrouheS8-",
        "outputId": "586999b1-3ec5-4d35-8046-809009357720"
      },
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "sna = SentimentIntensityAnalyzer()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgTom4oVeTCb",
        "outputId": "9d2643d1-6217-49bc-9202-d357c1407b03"
      },
      "source": [
        "df_Combined2 = df_Combined.copy()\n",
        "for i in range(0, df_Combined2.shape[0]):\n",
        "  News_Date_array = df_RedditNews[df_RedditNews[\"Date\"] == df_Combined2[\"Date\"][i]][\"News\"].to_numpy()\n",
        "\n",
        "  for j in range(0, 25):\n",
        "    df_Combined2[\"Top\"+str(j+1)][i] = sna.polarity_scores(df_Combined2[\"Top\"+str(j+1)][i])[\"compound\"]\n",
        "    "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bs9Dxqnp3wAt"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_nos3er2mkS"
      },
      "source": [
        "df_final = df_Combined2.drop(['Date', 'High', 'Low', 'Volume', 'Adj Close'], axis=1)\n",
        "X_df = df_final.drop(['Close'], axis=1)\n",
        "Y_df = df_final['Close']"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmyHXqsBHDNM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "b763ae40-56a0-49d7-d768-b2f96910021e"
      },
      "source": [
        "df_final.head(2)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>Close</th>\n",
              "      <th>1 day before Open</th>\n",
              "      <th>1 day before High</th>\n",
              "      <th>1 day before Low</th>\n",
              "      <th>1 day before Close</th>\n",
              "      <th>1 day before Volume</th>\n",
              "      <th>1 day before Adj Close</th>\n",
              "      <th>2 day before Open</th>\n",
              "      <th>2 day before High</th>\n",
              "      <th>2 day before Low</th>\n",
              "      <th>2 day before Close</th>\n",
              "      <th>2 day before Volume</th>\n",
              "      <th>2 day before Adj Close</th>\n",
              "      <th>3 day before Open</th>\n",
              "      <th>3 day before High</th>\n",
              "      <th>3 day before Low</th>\n",
              "      <th>3 day before Close</th>\n",
              "      <th>3 day before Volume</th>\n",
              "      <th>3 day before Adj Close</th>\n",
              "      <th>4 day before Open</th>\n",
              "      <th>4 day before High</th>\n",
              "      <th>4 day before Low</th>\n",
              "      <th>4 day before Close</th>\n",
              "      <th>4 day before Volume</th>\n",
              "      <th>4 day before Adj Close</th>\n",
              "      <th>5 day before Open</th>\n",
              "      <th>5 day before High</th>\n",
              "      <th>5 day before Low</th>\n",
              "      <th>5 day before Close</th>\n",
              "      <th>5 day before Volume</th>\n",
              "      <th>5 day before Adj Close</th>\n",
              "      <th>6 day before Open</th>\n",
              "      <th>6 day before High</th>\n",
              "      <th>6 day before Low</th>\n",
              "      <th>6 day before Close</th>\n",
              "      <th>6 day before Volume</th>\n",
              "      <th>6 day before Adj Close</th>\n",
              "      <th>7 day before Open</th>\n",
              "      <th>7 day before High</th>\n",
              "      <th>...</th>\n",
              "      <th>57 day before Close</th>\n",
              "      <th>57 day before Volume</th>\n",
              "      <th>57 day before Adj Close</th>\n",
              "      <th>58 day before Open</th>\n",
              "      <th>58 day before High</th>\n",
              "      <th>58 day before Low</th>\n",
              "      <th>58 day before Close</th>\n",
              "      <th>58 day before Volume</th>\n",
              "      <th>58 day before Adj Close</th>\n",
              "      <th>59 day before Open</th>\n",
              "      <th>59 day before High</th>\n",
              "      <th>59 day before Low</th>\n",
              "      <th>59 day before Close</th>\n",
              "      <th>59 day before Volume</th>\n",
              "      <th>59 day before Adj Close</th>\n",
              "      <th>Top1</th>\n",
              "      <th>Top2</th>\n",
              "      <th>Top3</th>\n",
              "      <th>Top4</th>\n",
              "      <th>Top5</th>\n",
              "      <th>Top6</th>\n",
              "      <th>Top7</th>\n",
              "      <th>Top8</th>\n",
              "      <th>Top9</th>\n",
              "      <th>Top10</th>\n",
              "      <th>Top11</th>\n",
              "      <th>Top12</th>\n",
              "      <th>Top13</th>\n",
              "      <th>Top14</th>\n",
              "      <th>Top15</th>\n",
              "      <th>Top16</th>\n",
              "      <th>Top17</th>\n",
              "      <th>Top18</th>\n",
              "      <th>Top19</th>\n",
              "      <th>Top20</th>\n",
              "      <th>Top21</th>\n",
              "      <th>Top22</th>\n",
              "      <th>Top23</th>\n",
              "      <th>Top24</th>\n",
              "      <th>Top25</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.933579</td>\n",
              "      <td>0.938290</td>\n",
              "      <td>0.897638</td>\n",
              "      <td>0.927717</td>\n",
              "      <td>0.904977</td>\n",
              "      <td>0.934995</td>\n",
              "      <td>-0.626052</td>\n",
              "      <td>0.934995</td>\n",
              "      <td>0.854005</td>\n",
              "      <td>0.888874</td>\n",
              "      <td>0.861634</td>\n",
              "      <td>0.894995</td>\n",
              "      <td>-0.706021</td>\n",
              "      <td>0.894995</td>\n",
              "      <td>0.808881</td>\n",
              "      <td>0.838231</td>\n",
              "      <td>0.816642</td>\n",
              "      <td>0.846554</td>\n",
              "      <td>-0.688587</td>\n",
              "      <td>0.846554</td>\n",
              "      <td>0.836872</td>\n",
              "      <td>0.828866</td>\n",
              "      <td>0.795049</td>\n",
              "      <td>0.800745</td>\n",
              "      <td>-0.608918</td>\n",
              "      <td>0.800745</td>\n",
              "      <td>0.937385</td>\n",
              "      <td>0.930469</td>\n",
              "      <td>0.844743</td>\n",
              "      <td>0.845029</td>\n",
              "      <td>-0.308067</td>\n",
              "      <td>0.845029</td>\n",
              "      <td>0.919961</td>\n",
              "      <td>0.94154</td>\n",
              "      <td>0.927397</td>\n",
              "      <td>0.948778</td>\n",
              "      <td>-0.730957</td>\n",
              "      <td>0.948778</td>\n",
              "      <td>0.918017</td>\n",
              "      <td>0.925922</td>\n",
              "      <td>...</td>\n",
              "      <td>0.899512</td>\n",
              "      <td>-0.782119</td>\n",
              "      <td>0.899512</td>\n",
              "      <td>0.876177</td>\n",
              "      <td>0.893533</td>\n",
              "      <td>0.878559</td>\n",
              "      <td>0.87149</td>\n",
              "      <td>-0.70386</td>\n",
              "      <td>0.87149</td>\n",
              "      <td>0.870893</td>\n",
              "      <td>0.887156</td>\n",
              "      <td>0.873858</td>\n",
              "      <td>0.874984</td>\n",
              "      <td>-0.78521</td>\n",
              "      <td>0.874984</td>\n",
              "      <td>-0.5574</td>\n",
              "      <td>-0.0516</td>\n",
              "      <td>0.5719</td>\n",
              "      <td>-0.8658</td>\n",
              "      <td>-0.296</td>\n",
              "      <td>-0.4404</td>\n",
              "      <td>-0.3182</td>\n",
              "      <td>0.5612</td>\n",
              "      <td>-0.7351</td>\n",
              "      <td>-0.2732</td>\n",
              "      <td>-0.8402</td>\n",
              "      <td>-0.6486</td>\n",
              "      <td>-0.4767</td>\n",
              "      <td>0.1779</td>\n",
              "      <td>-0.1027</td>\n",
              "      <td>-0.5859</td>\n",
              "      <td>0.3818</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.4019</td>\n",
              "      <td>-0.3182</td>\n",
              "      <td>-0.9509</td>\n",
              "      <td>-0.3818</td>\n",
              "      <td>-0.9618</td>\n",
              "      <td>-0.9432</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.897638</td>\n",
              "      <td>0.934995</td>\n",
              "      <td>0.854005</td>\n",
              "      <td>0.888874</td>\n",
              "      <td>0.861634</td>\n",
              "      <td>0.894995</td>\n",
              "      <td>-0.706021</td>\n",
              "      <td>0.894995</td>\n",
              "      <td>0.808881</td>\n",
              "      <td>0.838231</td>\n",
              "      <td>0.816642</td>\n",
              "      <td>0.846554</td>\n",
              "      <td>-0.688587</td>\n",
              "      <td>0.846554</td>\n",
              "      <td>0.836872</td>\n",
              "      <td>0.828866</td>\n",
              "      <td>0.795049</td>\n",
              "      <td>0.800745</td>\n",
              "      <td>-0.608918</td>\n",
              "      <td>0.800745</td>\n",
              "      <td>0.937385</td>\n",
              "      <td>0.930469</td>\n",
              "      <td>0.844743</td>\n",
              "      <td>0.845029</td>\n",
              "      <td>-0.308067</td>\n",
              "      <td>0.845029</td>\n",
              "      <td>0.919961</td>\n",
              "      <td>0.94154</td>\n",
              "      <td>0.927397</td>\n",
              "      <td>0.948778</td>\n",
              "      <td>-0.730957</td>\n",
              "      <td>0.948778</td>\n",
              "      <td>0.918017</td>\n",
              "      <td>0.925922</td>\n",
              "      <td>0.9149</td>\n",
              "      <td>0.90964</td>\n",
              "      <td>-0.756853</td>\n",
              "      <td>0.90964</td>\n",
              "      <td>0.917109</td>\n",
              "      <td>0.918651</td>\n",
              "      <td>...</td>\n",
              "      <td>0.87149</td>\n",
              "      <td>-0.70386</td>\n",
              "      <td>0.87149</td>\n",
              "      <td>0.870893</td>\n",
              "      <td>0.887156</td>\n",
              "      <td>0.873858</td>\n",
              "      <td>0.874984</td>\n",
              "      <td>-0.78521</td>\n",
              "      <td>0.874984</td>\n",
              "      <td>0.893308</td>\n",
              "      <td>0.885914</td>\n",
              "      <td>0.866414</td>\n",
              "      <td>0.869034</td>\n",
              "      <td>-0.754812</td>\n",
              "      <td>0.869034</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.4141</td>\n",
              "      <td>0.4019</td>\n",
              "      <td>-0.8934</td>\n",
              "      <td>-0.6124</td>\n",
              "      <td>-0.91</td>\n",
              "      <td>-0.3612</td>\n",
              "      <td>0.7003</td>\n",
              "      <td>-0.8402</td>\n",
              "      <td>-0.7096</td>\n",
              "      <td>0.6705</td>\n",
              "      <td>0.4019</td>\n",
              "      <td>-0.5423</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.3182</td>\n",
              "      <td>0.7351</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.7579</td>\n",
              "      <td>-0.3182</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.9578</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.872</td>\n",
              "      <td>-0.5423</td>\n",
              "      <td>-0.875</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 381 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Open     Close 1 day before Open  ...   Top23   Top24  Top25\n",
              "0  0.933579  0.938290          0.897638  ... -0.9618 -0.9432      0\n",
              "1  0.897638  0.934995          0.854005  ...  -0.872 -0.5423 -0.875\n",
              "\n",
              "[2 rows x 381 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jjkCDh51cOd"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(X_df, Y_df, test_size=0.3, random_state=1)\n",
        "# import tensorflow as tf"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQffrPhz3ZRU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33c7ff72-560d-44b5-8fc4-785a2e073ed0"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0jZT19bZcfD"
      },
      "source": [
        "y_train = np.array(y_train,ndmin=2).T\n",
        "y_test = np.array(y_test,ndmin=2).T"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxjOxcZ21Ap6"
      },
      "source": [
        "features = x_train.shape[1]\n",
        "x = tf.placeholder(tf.float32, shape=[None,x_train.shape[1]])\n",
        "y_ = tf.placeholder(tf.float32, shape=[None,y_train.shape[1]])\n",
        "W = tf.Variable(tf.zeros([features,1]))\n",
        "b = tf.Variable(tf.zeros([1]))\n",
        "predict = tf.matmul(x,W) + b\n",
        "loss = tf.reduce_mean(tf.pow(predict - y_, 2))\n",
        "update = tf.train.GradientDescentOptimizer(0.005).minimize(loss)\n",
        "data_x = x_train.to_numpy()\n",
        "data_y = y_train\n",
        "data_x_test = x_test.to_numpy()\n",
        "data_y_test = y_test"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2anyebtU_LP"
      },
      "source": [
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGphoG1d_6q8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8223e099-378f-42f3-cdce-3dec000876ca"
      },
      "source": [
        "train_losses = []\n",
        "test_losses = []\n",
        "for i in range(1500):\n",
        "    sess.run(update, feed_dict = {x:data_x, y_:data_y})\n",
        "  \n",
        "    temp_loss = sess.run(loss, feed_dict = {x: data_x, y_: data_y})\n",
        "    train_losses.append(temp_loss)\n",
        "    temp_loss = sess.run(loss, feed_dict = {x: data_x_test, y_: data_y_test})\n",
        "    test_losses.append(temp_loss)\n",
        "    print('iter:', i+1,', loss:', temp_loss)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter: 1 , loss: 0.0085042\n",
            "iter: 2 , loss: 0.008090665\n",
            "iter: 3 , loss: 0.0077856923\n",
            "iter: 4 , loss: 0.0075409426\n",
            "iter: 5 , loss: 0.007341893\n",
            "iter: 6 , loss: 0.0071775704\n",
            "iter: 7 , loss: 0.0070397174\n",
            "iter: 8 , loss: 0.0069221035\n",
            "iter: 9 , loss: 0.006820031\n",
            "iter: 10 , loss: 0.006729948\n",
            "iter: 11 , loss: 0.0066491696\n",
            "iter: 12 , loss: 0.006575661\n",
            "iter: 13 , loss: 0.0065078717\n",
            "iter: 14 , loss: 0.0064446297\n",
            "iter: 15 , loss: 0.006385035\n",
            "iter: 16 , loss: 0.0063284026\n",
            "iter: 17 , loss: 0.0062742024\n",
            "iter: 18 , loss: 0.00622203\n",
            "iter: 19 , loss: 0.006171572\n",
            "iter: 20 , loss: 0.006122584\n",
            "iter: 21 , loss: 0.0060748723\n",
            "iter: 22 , loss: 0.006028291\n",
            "iter: 23 , loss: 0.0059827184\n",
            "iter: 24 , loss: 0.0059380597\n",
            "iter: 25 , loss: 0.005894241\n",
            "iter: 26 , loss: 0.005851199\n",
            "iter: 27 , loss: 0.0058088824\n",
            "iter: 28 , loss: 0.005767252\n",
            "iter: 29 , loss: 0.005726271\n",
            "iter: 30 , loss: 0.00568591\n",
            "iter: 31 , loss: 0.005646145\n",
            "iter: 32 , loss: 0.005606953\n",
            "iter: 33 , loss: 0.0055683167\n",
            "iter: 34 , loss: 0.005530217\n",
            "iter: 35 , loss: 0.0054926425\n",
            "iter: 36 , loss: 0.005455577\n",
            "iter: 37 , loss: 0.005419008\n",
            "iter: 38 , loss: 0.005382927\n",
            "iter: 39 , loss: 0.0053473213\n",
            "iter: 40 , loss: 0.005312183\n",
            "iter: 41 , loss: 0.005277502\n",
            "iter: 42 , loss: 0.005243271\n",
            "iter: 43 , loss: 0.005209481\n",
            "iter: 44 , loss: 0.005176125\n",
            "iter: 45 , loss: 0.0051431945\n",
            "iter: 46 , loss: 0.005110685\n",
            "iter: 47 , loss: 0.0050785863\n",
            "iter: 48 , loss: 0.005046895\n",
            "iter: 49 , loss: 0.005015604\n",
            "iter: 50 , loss: 0.004984706\n",
            "iter: 51 , loss: 0.004954197\n",
            "iter: 52 , loss: 0.00492407\n",
            "iter: 53 , loss: 0.004894319\n",
            "iter: 54 , loss: 0.004864939\n",
            "iter: 55 , loss: 0.004835925\n",
            "iter: 56 , loss: 0.004807273\n",
            "iter: 57 , loss: 0.004778974\n",
            "iter: 58 , loss: 0.004751027\n",
            "iter: 59 , loss: 0.0047234255\n",
            "iter: 60 , loss: 0.004696163\n",
            "iter: 61 , loss: 0.0046692374\n",
            "iter: 62 , loss: 0.0046426426\n",
            "iter: 63 , loss: 0.0046163737\n",
            "iter: 64 , loss: 0.0045904275\n",
            "iter: 65 , loss: 0.004564799\n",
            "iter: 66 , loss: 0.0045394823\n",
            "iter: 67 , loss: 0.004514475\n",
            "iter: 68 , loss: 0.004489772\n",
            "iter: 69 , loss: 0.0044653695\n",
            "iter: 70 , loss: 0.004441262\n",
            "iter: 71 , loss: 0.004417449\n",
            "iter: 72 , loss: 0.004393921\n",
            "iter: 73 , loss: 0.004370679\n",
            "iter: 74 , loss: 0.0043477174\n",
            "iter: 75 , loss: 0.004325032\n",
            "iter: 76 , loss: 0.0043026186\n",
            "iter: 77 , loss: 0.0042804754\n",
            "iter: 78 , loss: 0.0042585963\n",
            "iter: 79 , loss: 0.00423698\n",
            "iter: 80 , loss: 0.0042156205\n",
            "iter: 81 , loss: 0.0041945158\n",
            "iter: 82 , loss: 0.0041736625\n",
            "iter: 83 , loss: 0.004153057\n",
            "iter: 84 , loss: 0.004132696\n",
            "iter: 85 , loss: 0.0041125766\n",
            "iter: 86 , loss: 0.0040926943\n",
            "iter: 87 , loss: 0.0040730466\n",
            "iter: 88 , loss: 0.00405363\n",
            "iter: 89 , loss: 0.004034442\n",
            "iter: 90 , loss: 0.0040154792\n",
            "iter: 91 , loss: 0.003996738\n",
            "iter: 92 , loss: 0.0039782166\n",
            "iter: 93 , loss: 0.003959911\n",
            "iter: 94 , loss: 0.003941819\n",
            "iter: 95 , loss: 0.0039239377\n",
            "iter: 96 , loss: 0.003906263\n",
            "iter: 97 , loss: 0.003888794\n",
            "iter: 98 , loss: 0.0038715266\n",
            "iter: 99 , loss: 0.0038544585\n",
            "iter: 100 , loss: 0.0038375875\n",
            "iter: 101 , loss: 0.0038209101\n",
            "iter: 102 , loss: 0.0038044234\n",
            "iter: 103 , loss: 0.0037881266\n",
            "iter: 104 , loss: 0.003772016\n",
            "iter: 105 , loss: 0.0037560882\n",
            "iter: 106 , loss: 0.0037403428\n",
            "iter: 107 , loss: 0.003724776\n",
            "iter: 108 , loss: 0.0037093854\n",
            "iter: 109 , loss: 0.0036941688\n",
            "iter: 110 , loss: 0.0036791244\n",
            "iter: 111 , loss: 0.0036642498\n",
            "iter: 112 , loss: 0.0036495419\n",
            "iter: 113 , loss: 0.0036350004\n",
            "iter: 114 , loss: 0.0036206204\n",
            "iter: 115 , loss: 0.0036064014\n",
            "iter: 116 , loss: 0.0035923412\n",
            "iter: 117 , loss: 0.0035784368\n",
            "iter: 118 , loss: 0.0035646872\n",
            "iter: 119 , loss: 0.00355109\n",
            "iter: 120 , loss: 0.003537643\n",
            "iter: 121 , loss: 0.0035243446\n",
            "iter: 122 , loss: 0.0035111925\n",
            "iter: 123 , loss: 0.0034981852\n",
            "iter: 124 , loss: 0.0034853194\n",
            "iter: 125 , loss: 0.0034725934\n",
            "iter: 126 , loss: 0.0034600091\n",
            "iter: 127 , loss: 0.0034475597\n",
            "iter: 128 , loss: 0.0034352462\n",
            "iter: 129 , loss: 0.0034230654\n",
            "iter: 130 , loss: 0.0034110164\n",
            "iter: 131 , loss: 0.0033990976\n",
            "iter: 132 , loss: 0.0033873068\n",
            "iter: 133 , loss: 0.0033756427\n",
            "iter: 134 , loss: 0.0033641031\n",
            "iter: 135 , loss: 0.003352687\n",
            "iter: 136 , loss: 0.0033413917\n",
            "iter: 137 , loss: 0.003330217\n",
            "iter: 138 , loss: 0.0033191608\n",
            "iter: 139 , loss: 0.0033082208\n",
            "iter: 140 , loss: 0.003297397\n",
            "iter: 141 , loss: 0.0032866865\n",
            "iter: 142 , loss: 0.003276089\n",
            "iter: 143 , loss: 0.0032656016\n",
            "iter: 144 , loss: 0.003255225\n",
            "iter: 145 , loss: 0.0032449544\n",
            "iter: 146 , loss: 0.003234792\n",
            "iter: 147 , loss: 0.0032247335\n",
            "iter: 148 , loss: 0.00321478\n",
            "iter: 149 , loss: 0.003204929\n",
            "iter: 150 , loss: 0.0031951787\n",
            "iter: 151 , loss: 0.0031855286\n",
            "iter: 152 , loss: 0.0031759774\n",
            "iter: 153 , loss: 0.0031665228\n",
            "iter: 154 , loss: 0.0031571637\n",
            "iter: 155 , loss: 0.0031479015\n",
            "iter: 156 , loss: 0.0031387308\n",
            "iter: 157 , loss: 0.0031296539\n",
            "iter: 158 , loss: 0.0031206675\n",
            "iter: 159 , loss: 0.003111772\n",
            "iter: 160 , loss: 0.0031029652\n",
            "iter: 161 , loss: 0.003094246\n",
            "iter: 162 , loss: 0.003085613\n",
            "iter: 163 , loss: 0.003077066\n",
            "iter: 164 , loss: 0.0030686029\n",
            "iter: 165 , loss: 0.0030602242\n",
            "iter: 166 , loss: 0.003051927\n",
            "iter: 167 , loss: 0.0030437114\n",
            "iter: 168 , loss: 0.0030355763\n",
            "iter: 169 , loss: 0.0030275197\n",
            "iter: 170 , loss: 0.0030195413\n",
            "iter: 171 , loss: 0.0030116402\n",
            "iter: 172 , loss: 0.003003816\n",
            "iter: 173 , loss: 0.0029960666\n",
            "iter: 174 , loss: 0.0029883909\n",
            "iter: 175 , loss: 0.0029807899\n",
            "iter: 176 , loss: 0.0029732604\n",
            "iter: 177 , loss: 0.0029658028\n",
            "iter: 178 , loss: 0.0029584158\n",
            "iter: 179 , loss: 0.0029510986\n",
            "iter: 180 , loss: 0.0029438501\n",
            "iter: 181 , loss: 0.0029366696\n",
            "iter: 182 , loss: 0.0029295569\n",
            "iter: 183 , loss: 0.00292251\n",
            "iter: 184 , loss: 0.0029155288\n",
            "iter: 185 , loss: 0.002908612\n",
            "iter: 186 , loss: 0.0029017592\n",
            "iter: 187 , loss: 0.0028949694\n",
            "iter: 188 , loss: 0.002888242\n",
            "iter: 189 , loss: 0.0028815758\n",
            "iter: 190 , loss: 0.0028749711\n",
            "iter: 191 , loss: 0.0028684258\n",
            "iter: 192 , loss: 0.0028619398\n",
            "iter: 193 , loss: 0.0028555125\n",
            "iter: 194 , loss: 0.0028491428\n",
            "iter: 195 , loss: 0.0028428305\n",
            "iter: 196 , loss: 0.002836574\n",
            "iter: 197 , loss: 0.0028303748\n",
            "iter: 198 , loss: 0.0028242294\n",
            "iter: 199 , loss: 0.002818138\n",
            "iter: 200 , loss: 0.0028121008\n",
            "iter: 201 , loss: 0.0028061166\n",
            "iter: 202 , loss: 0.002800185\n",
            "iter: 203 , loss: 0.002794305\n",
            "iter: 204 , loss: 0.0027884757\n",
            "iter: 205 , loss: 0.0027826978\n",
            "iter: 206 , loss: 0.0027769688\n",
            "iter: 207 , loss: 0.0027712896\n",
            "iter: 208 , loss: 0.0027656585\n",
            "iter: 209 , loss: 0.0027600764\n",
            "iter: 210 , loss: 0.0027545413\n",
            "iter: 211 , loss: 0.0027490526\n",
            "iter: 212 , loss: 0.0027436113\n",
            "iter: 213 , loss: 0.0027382155\n",
            "iter: 214 , loss: 0.0027328648\n",
            "iter: 215 , loss: 0.0027275588\n",
            "iter: 216 , loss: 0.0027222976\n",
            "iter: 217 , loss: 0.002717079\n",
            "iter: 218 , loss: 0.0027119038\n",
            "iter: 219 , loss: 0.0027067713\n",
            "iter: 220 , loss: 0.0027016816\n",
            "iter: 221 , loss: 0.0026966326\n",
            "iter: 222 , loss: 0.0026916256\n",
            "iter: 223 , loss: 0.0026866584\n",
            "iter: 224 , loss: 0.0026817317\n",
            "iter: 225 , loss: 0.002676845\n",
            "iter: 226 , loss: 0.002671997\n",
            "iter: 227 , loss: 0.0026671872\n",
            "iter: 228 , loss: 0.0026624175\n",
            "iter: 229 , loss: 0.0026576833\n",
            "iter: 230 , loss: 0.002652989\n",
            "iter: 231 , loss: 0.0026483296\n",
            "iter: 232 , loss: 0.0026437074\n",
            "iter: 233 , loss: 0.002639122\n",
            "iter: 234 , loss: 0.0026345714\n",
            "iter: 235 , loss: 0.002630057\n",
            "iter: 236 , loss: 0.0026255767\n",
            "iter: 237 , loss: 0.0026211313\n",
            "iter: 238 , loss: 0.0026167196\n",
            "iter: 239 , loss: 0.0026123412\n",
            "iter: 240 , loss: 0.002607997\n",
            "iter: 241 , loss: 0.0026036855\n",
            "iter: 242 , loss: 0.0025994068\n",
            "iter: 243 , loss: 0.0025951592\n",
            "iter: 244 , loss: 0.0025909443\n",
            "iter: 245 , loss: 0.0025867603\n",
            "iter: 246 , loss: 0.0025826069\n",
            "iter: 247 , loss: 0.0025784848\n",
            "iter: 248 , loss: 0.0025743933\n",
            "iter: 249 , loss: 0.002570332\n",
            "iter: 250 , loss: 0.0025663006\n",
            "iter: 251 , loss: 0.0025622977\n",
            "iter: 252 , loss: 0.0025583236\n",
            "iter: 253 , loss: 0.0025543792\n",
            "iter: 254 , loss: 0.0025504627\n",
            "iter: 255 , loss: 0.002546574\n",
            "iter: 256 , loss: 0.0025427134\n",
            "iter: 257 , loss: 0.0025388794\n",
            "iter: 258 , loss: 0.0025350733\n",
            "iter: 259 , loss: 0.0025312945\n",
            "iter: 260 , loss: 0.002527541\n",
            "iter: 261 , loss: 0.0025238146\n",
            "iter: 262 , loss: 0.0025201137\n",
            "iter: 263 , loss: 0.002516439\n",
            "iter: 264 , loss: 0.0025127886\n",
            "iter: 265 , loss: 0.0025091649\n",
            "iter: 266 , loss: 0.0025055648\n",
            "iter: 267 , loss: 0.0025019892\n",
            "iter: 268 , loss: 0.0024984379\n",
            "iter: 269 , loss: 0.0024949114\n",
            "iter: 270 , loss: 0.0024914083\n",
            "iter: 271 , loss: 0.0024879286\n",
            "iter: 272 , loss: 0.0024844722\n",
            "iter: 273 , loss: 0.0024810382\n",
            "iter: 274 , loss: 0.0024776275\n",
            "iter: 275 , loss: 0.0024742396\n",
            "iter: 276 , loss: 0.0024708733\n",
            "iter: 277 , loss: 0.0024675291\n",
            "iter: 278 , loss: 0.0024642078\n",
            "iter: 279 , loss: 0.0024609067\n",
            "iter: 280 , loss: 0.0024576273\n",
            "iter: 281 , loss: 0.0024543686\n",
            "iter: 282 , loss: 0.0024511316\n",
            "iter: 283 , loss: 0.0024479155\n",
            "iter: 284 , loss: 0.002444718\n",
            "iter: 285 , loss: 0.0024415427\n",
            "iter: 286 , loss: 0.002438386\n",
            "iter: 287 , loss: 0.0024352502\n",
            "iter: 288 , loss: 0.0024321338\n",
            "iter: 289 , loss: 0.0024290362\n",
            "iter: 290 , loss: 0.002425958\n",
            "iter: 291 , loss: 0.0024228995\n",
            "iter: 292 , loss: 0.0024198585\n",
            "iter: 293 , loss: 0.0024168366\n",
            "iter: 294 , loss: 0.0024138337\n",
            "iter: 295 , loss: 0.0024108477\n",
            "iter: 296 , loss: 0.0024078805\n",
            "iter: 297 , loss: 0.002404932\n",
            "iter: 298 , loss: 0.0024020004\n",
            "iter: 299 , loss: 0.0023990863\n",
            "iter: 300 , loss: 0.0023961891\n",
            "iter: 301 , loss: 0.0023933095\n",
            "iter: 302 , loss: 0.0023904461\n",
            "iter: 303 , loss: 0.0023876007\n",
            "iter: 304 , loss: 0.0023847711\n",
            "iter: 305 , loss: 0.0023819583\n",
            "iter: 306 , loss: 0.0023791615\n",
            "iter: 307 , loss: 0.002376381\n",
            "iter: 308 , loss: 0.0023736162\n",
            "iter: 309 , loss: 0.0023708676\n",
            "iter: 310 , loss: 0.0023681344\n",
            "iter: 311 , loss: 0.0023654166\n",
            "iter: 312 , loss: 0.0023627144\n",
            "iter: 313 , loss: 0.0023600275\n",
            "iter: 314 , loss: 0.0023573553\n",
            "iter: 315 , loss: 0.0023546978\n",
            "iter: 316 , loss: 0.0023520554\n",
            "iter: 317 , loss: 0.0023494272\n",
            "iter: 318 , loss: 0.0023468141\n",
            "iter: 319 , loss: 0.0023442148\n",
            "iter: 320 , loss: 0.00234163\n",
            "iter: 321 , loss: 0.002339059\n",
            "iter: 322 , loss: 0.0023365016\n",
            "iter: 323 , loss: 0.002333959\n",
            "iter: 324 , loss: 0.0023314294\n",
            "iter: 325 , loss: 0.002328913\n",
            "iter: 326 , loss: 0.00232641\n",
            "iter: 327 , loss: 0.002323921\n",
            "iter: 328 , loss: 0.0023214445\n",
            "iter: 329 , loss: 0.0023189816\n",
            "iter: 330 , loss: 0.002316531\n",
            "iter: 331 , loss: 0.0023140933\n",
            "iter: 332 , loss: 0.0023116684\n",
            "iter: 333 , loss: 0.0023092562\n",
            "iter: 334 , loss: 0.0023068555\n",
            "iter: 335 , loss: 0.002304468\n",
            "iter: 336 , loss: 0.002302093\n",
            "iter: 337 , loss: 0.0022997288\n",
            "iter: 338 , loss: 0.0022973772\n",
            "iter: 339 , loss: 0.0022950377\n",
            "iter: 340 , loss: 0.00229271\n",
            "iter: 341 , loss: 0.0022903935\n",
            "iter: 342 , loss: 0.0022880887\n",
            "iter: 343 , loss: 0.002285795\n",
            "iter: 344 , loss: 0.0022835126\n",
            "iter: 345 , loss: 0.002281242\n",
            "iter: 346 , loss: 0.0022789822\n",
            "iter: 347 , loss: 0.0022767335\n",
            "iter: 348 , loss: 0.002274496\n",
            "iter: 349 , loss: 0.0022722688\n",
            "iter: 350 , loss: 0.0022700524\n",
            "iter: 351 , loss: 0.0022678466\n",
            "iter: 352 , loss: 0.0022656517\n",
            "iter: 353 , loss: 0.0022634668\n",
            "iter: 354 , loss: 0.0022612924\n",
            "iter: 355 , loss: 0.0022591287\n",
            "iter: 356 , loss: 0.0022569748\n",
            "iter: 357 , loss: 0.0022548304\n",
            "iter: 358 , loss: 0.002252697\n",
            "iter: 359 , loss: 0.002250573\n",
            "iter: 360 , loss: 0.002248459\n",
            "iter: 361 , loss: 0.0022463545\n",
            "iter: 362 , loss: 0.0022442597\n",
            "iter: 363 , loss: 0.002242175\n",
            "iter: 364 , loss: 0.0022400988\n",
            "iter: 365 , loss: 0.0022380329\n",
            "iter: 366 , loss: 0.0022359753\n",
            "iter: 367 , loss: 0.0022339278\n",
            "iter: 368 , loss: 0.0022318894\n",
            "iter: 369 , loss: 0.0022298603\n",
            "iter: 370 , loss: 0.0022278398\n",
            "iter: 371 , loss: 0.0022258281\n",
            "iter: 372 , loss: 0.0022238258\n",
            "iter: 373 , loss: 0.0022218314\n",
            "iter: 374 , loss: 0.002219847\n",
            "iter: 375 , loss: 0.00221787\n",
            "iter: 376 , loss: 0.0022159023\n",
            "iter: 377 , loss: 0.002213943\n",
            "iter: 378 , loss: 0.0022119922\n",
            "iter: 379 , loss: 0.0022100501\n",
            "iter: 380 , loss: 0.0022081155\n",
            "iter: 381 , loss: 0.0022061893\n",
            "iter: 382 , loss: 0.0022042715\n",
            "iter: 383 , loss: 0.0022023618\n",
            "iter: 384 , loss: 0.0022004603\n",
            "iter: 385 , loss: 0.0021985664\n",
            "iter: 386 , loss: 0.0021966808\n",
            "iter: 387 , loss: 0.0021948027\n",
            "iter: 388 , loss: 0.0021929324\n",
            "iter: 389 , loss: 0.00219107\n",
            "iter: 390 , loss: 0.002189215\n",
            "iter: 391 , loss: 0.0021873675\n",
            "iter: 392 , loss: 0.002185528\n",
            "iter: 393 , loss: 0.002183696\n",
            "iter: 394 , loss: 0.0021818709\n",
            "iter: 395 , loss: 0.0021800536\n",
            "iter: 396 , loss: 0.0021782431\n",
            "iter: 397 , loss: 0.0021764406\n",
            "iter: 398 , loss: 0.0021746447\n",
            "iter: 399 , loss: 0.0021728561\n",
            "iter: 400 , loss: 0.0021710754\n",
            "iter: 401 , loss: 0.0021693003\n",
            "iter: 402 , loss: 0.0021675332\n",
            "iter: 403 , loss: 0.0021657725\n",
            "iter: 404 , loss: 0.0021640193\n",
            "iter: 405 , loss: 0.002162272\n",
            "iter: 406 , loss: 0.002160532\n",
            "iter: 407 , loss: 0.0021587987\n",
            "iter: 408 , loss: 0.0021570725\n",
            "iter: 409 , loss: 0.0021553517\n",
            "iter: 410 , loss: 0.0021536385\n",
            "iter: 411 , loss: 0.0021519316\n",
            "iter: 412 , loss: 0.0021502313\n",
            "iter: 413 , loss: 0.002148537\n",
            "iter: 414 , loss: 0.0021468499\n",
            "iter: 415 , loss: 0.0021451684\n",
            "iter: 416 , loss: 0.0021434934\n",
            "iter: 417 , loss: 0.002141825\n",
            "iter: 418 , loss: 0.0021401618\n",
            "iter: 419 , loss: 0.0021385055\n",
            "iter: 420 , loss: 0.0021368554\n",
            "iter: 421 , loss: 0.0021352107\n",
            "iter: 422 , loss: 0.0021335725\n",
            "iter: 423 , loss: 0.00213194\n",
            "iter: 424 , loss: 0.0021303135\n",
            "iter: 425 , loss: 0.0021286933\n",
            "iter: 426 , loss: 0.0021270793\n",
            "iter: 427 , loss: 0.002125471\n",
            "iter: 428 , loss: 0.0021238674\n",
            "iter: 429 , loss: 0.0021222702\n",
            "iter: 430 , loss: 0.0021206783\n",
            "iter: 431 , loss: 0.0021190923\n",
            "iter: 432 , loss: 0.0021175123\n",
            "iter: 433 , loss: 0.0021159376\n",
            "iter: 434 , loss: 0.0021143686\n",
            "iter: 435 , loss: 0.0021128042\n",
            "iter: 436 , loss: 0.0021112463\n",
            "iter: 437 , loss: 0.002109694\n",
            "iter: 438 , loss: 0.002108146\n",
            "iter: 439 , loss: 0.0021066037\n",
            "iter: 440 , loss: 0.0021050675\n",
            "iter: 441 , loss: 0.002103536\n",
            "iter: 442 , loss: 0.0021020097\n",
            "iter: 443 , loss: 0.0021004882\n",
            "iter: 444 , loss: 0.002098973\n",
            "iter: 445 , loss: 0.002097462\n",
            "iter: 446 , loss: 0.0020959564\n",
            "iter: 447 , loss: 0.0020944555\n",
            "iter: 448 , loss: 0.0020929603\n",
            "iter: 449 , loss: 0.00209147\n",
            "iter: 450 , loss: 0.0020899845\n",
            "iter: 451 , loss: 0.0020885041\n",
            "iter: 452 , loss: 0.0020870278\n",
            "iter: 453 , loss: 0.0020855574\n",
            "iter: 454 , loss: 0.0020840915\n",
            "iter: 455 , loss: 0.0020826305\n",
            "iter: 456 , loss: 0.0020811742\n",
            "iter: 457 , loss: 0.0020797227\n",
            "iter: 458 , loss: 0.0020782757\n",
            "iter: 459 , loss: 0.0020768335\n",
            "iter: 460 , loss: 0.0020753958\n",
            "iter: 461 , loss: 0.002073963\n",
            "iter: 462 , loss: 0.0020725348\n",
            "iter: 463 , loss: 0.002071111\n",
            "iter: 464 , loss: 0.002069692\n",
            "iter: 465 , loss: 0.0020682774\n",
            "iter: 466 , loss: 0.0020668672\n",
            "iter: 467 , loss: 0.0020654616\n",
            "iter: 468 , loss: 0.00206406\n",
            "iter: 469 , loss: 0.0020626632\n",
            "iter: 470 , loss: 0.002061271\n",
            "iter: 471 , loss: 0.0020598827\n",
            "iter: 472 , loss: 0.0020584995\n",
            "iter: 473 , loss: 0.0020571195\n",
            "iter: 474 , loss: 0.0020557449\n",
            "iter: 475 , loss: 0.0020543735\n",
            "iter: 476 , loss: 0.002053007\n",
            "iter: 477 , loss: 0.002051644\n",
            "iter: 478 , loss: 0.002050286\n",
            "iter: 479 , loss: 0.0020489318\n",
            "iter: 480 , loss: 0.0020475818\n",
            "iter: 481 , loss: 0.0020462354\n",
            "iter: 482 , loss: 0.0020448938\n",
            "iter: 483 , loss: 0.0020435557\n",
            "iter: 484 , loss: 0.002042222\n",
            "iter: 485 , loss: 0.0020408921\n",
            "iter: 486 , loss: 0.0020395666\n",
            "iter: 487 , loss: 0.002038245\n",
            "iter: 488 , loss: 0.0020369273\n",
            "iter: 489 , loss: 0.0020356125\n",
            "iter: 490 , loss: 0.0020343028\n",
            "iter: 491 , loss: 0.0020329969\n",
            "iter: 492 , loss: 0.0020316937\n",
            "iter: 493 , loss: 0.002030396\n",
            "iter: 494 , loss: 0.0020291004\n",
            "iter: 495 , loss: 0.0020278096\n",
            "iter: 496 , loss: 0.0020265228\n",
            "iter: 497 , loss: 0.0020252387\n",
            "iter: 498 , loss: 0.0020239593\n",
            "iter: 499 , loss: 0.0020226834\n",
            "iter: 500 , loss: 0.0020214105\n",
            "iter: 501 , loss: 0.0020201418\n",
            "iter: 502 , loss: 0.0020188764\n",
            "iter: 503 , loss: 0.0020176154\n",
            "iter: 504 , loss: 0.0020163574\n",
            "iter: 505 , loss: 0.0020151027\n",
            "iter: 506 , loss: 0.0020138521\n",
            "iter: 507 , loss: 0.0020126044\n",
            "iter: 508 , loss: 0.002011361\n",
            "iter: 509 , loss: 0.0020101208\n",
            "iter: 510 , loss: 0.002008884\n",
            "iter: 511 , loss: 0.0020076507\n",
            "iter: 512 , loss: 0.0020064209\n",
            "iter: 513 , loss: 0.002005194\n",
            "iter: 514 , loss: 0.0020039713\n",
            "iter: 515 , loss: 0.0020027515\n",
            "iter: 516 , loss: 0.0020015351\n",
            "iter: 517 , loss: 0.002000322\n",
            "iter: 518 , loss: 0.0019991128\n",
            "iter: 519 , loss: 0.001997906\n",
            "iter: 520 , loss: 0.001996703\n",
            "iter: 521 , loss: 0.0019955034\n",
            "iter: 522 , loss: 0.001994307\n",
            "iter: 523 , loss: 0.0019931137\n",
            "iter: 524 , loss: 0.001991924\n",
            "iter: 525 , loss: 0.001990737\n",
            "iter: 526 , loss: 0.0019895525\n",
            "iter: 527 , loss: 0.0019883725\n",
            "iter: 528 , loss: 0.0019871958\n",
            "iter: 529 , loss: 0.0019860212\n",
            "iter: 530 , loss: 0.0019848503\n",
            "iter: 531 , loss: 0.0019836824\n",
            "iter: 532 , loss: 0.0019825178\n",
            "iter: 533 , loss: 0.001981356\n",
            "iter: 534 , loss: 0.0019801974\n",
            "iter: 535 , loss: 0.0019790411\n",
            "iter: 536 , loss: 0.0019778884\n",
            "iter: 537 , loss: 0.0019767394\n",
            "iter: 538 , loss: 0.001975593\n",
            "iter: 539 , loss: 0.0019744493\n",
            "iter: 540 , loss: 0.0019733084\n",
            "iter: 541 , loss: 0.0019721705\n",
            "iter: 542 , loss: 0.0019710357\n",
            "iter: 543 , loss: 0.0019699046\n",
            "iter: 544 , loss: 0.0019687756\n",
            "iter: 545 , loss: 0.0019676494\n",
            "iter: 546 , loss: 0.0019665258\n",
            "iter: 547 , loss: 0.0019654061\n",
            "iter: 548 , loss: 0.0019642883\n",
            "iter: 549 , loss: 0.001963174\n",
            "iter: 550 , loss: 0.001962062\n",
            "iter: 551 , loss: 0.0019609532\n",
            "iter: 552 , loss: 0.0019598473\n",
            "iter: 553 , loss: 0.0019587444\n",
            "iter: 554 , loss: 0.0019576435\n",
            "iter: 555 , loss: 0.0019565455\n",
            "iter: 556 , loss: 0.0019554512\n",
            "iter: 557 , loss: 0.0019543585\n",
            "iter: 558 , loss: 0.0019532687\n",
            "iter: 559 , loss: 0.001952182\n",
            "iter: 560 , loss: 0.0019510979\n",
            "iter: 561 , loss: 0.0019500161\n",
            "iter: 562 , loss: 0.0019489374\n",
            "iter: 563 , loss: 0.0019478613\n",
            "iter: 564 , loss: 0.0019467879\n",
            "iter: 565 , loss: 0.0019457171\n",
            "iter: 566 , loss: 0.0019446486\n",
            "iter: 567 , loss: 0.0019435831\n",
            "iter: 568 , loss: 0.0019425199\n",
            "iter: 569 , loss: 0.0019414596\n",
            "iter: 570 , loss: 0.0019404015\n",
            "iter: 571 , loss: 0.0019393461\n",
            "iter: 572 , loss: 0.0019382935\n",
            "iter: 573 , loss: 0.0019372432\n",
            "iter: 574 , loss: 0.0019361954\n",
            "iter: 575 , loss: 0.0019351501\n",
            "iter: 576 , loss: 0.0019341075\n",
            "iter: 577 , loss: 0.001933067\n",
            "iter: 578 , loss: 0.0019320293\n",
            "iter: 579 , loss: 0.0019309936\n",
            "iter: 580 , loss: 0.0019299611\n",
            "iter: 581 , loss: 0.0019289309\n",
            "iter: 582 , loss: 0.0019279022\n",
            "iter: 583 , loss: 0.001926877\n",
            "iter: 584 , loss: 0.0019258537\n",
            "iter: 585 , loss: 0.001924833\n",
            "iter: 586 , loss: 0.0019238145\n",
            "iter: 587 , loss: 0.0019227987\n",
            "iter: 588 , loss: 0.0019217851\n",
            "iter: 589 , loss: 0.001920774\n",
            "iter: 590 , loss: 0.0019197654\n",
            "iter: 591 , loss: 0.0019187584\n",
            "iter: 592 , loss: 0.0019177542\n",
            "iter: 593 , loss: 0.0019167528\n",
            "iter: 594 , loss: 0.0019157528\n",
            "iter: 595 , loss: 0.0019147554\n",
            "iter: 596 , loss: 0.0019137606\n",
            "iter: 597 , loss: 0.0019127674\n",
            "iter: 598 , loss: 0.0019117771\n",
            "iter: 599 , loss: 0.0019107886\n",
            "iter: 600 , loss: 0.0019098027\n",
            "iter: 601 , loss: 0.0019088188\n",
            "iter: 602 , loss: 0.0019078375\n",
            "iter: 603 , loss: 0.0019068578\n",
            "iter: 604 , loss: 0.0019058812\n",
            "iter: 605 , loss: 0.0019049061\n",
            "iter: 606 , loss: 0.0019039334\n",
            "iter: 607 , loss: 0.0019029628\n",
            "iter: 608 , loss: 0.0019019945\n",
            "iter: 609 , loss: 0.0019010281\n",
            "iter: 610 , loss: 0.0019000643\n",
            "iter: 611 , loss: 0.0018991024\n",
            "iter: 612 , loss: 0.0018981427\n",
            "iter: 613 , loss: 0.0018971848\n",
            "iter: 614 , loss: 0.0018962297\n",
            "iter: 615 , loss: 0.0018952753\n",
            "iter: 616 , loss: 0.0018943246\n",
            "iter: 617 , loss: 0.0018933752\n",
            "iter: 618 , loss: 0.0018924279\n",
            "iter: 619 , loss: 0.0018914829\n",
            "iter: 620 , loss: 0.0018905395\n",
            "iter: 621 , loss: 0.0018895993\n",
            "iter: 622 , loss: 0.0018886599\n",
            "iter: 623 , loss: 0.0018877225\n",
            "iter: 624 , loss: 0.0018867884\n",
            "iter: 625 , loss: 0.0018858551\n",
            "iter: 626 , loss: 0.0018849244\n",
            "iter: 627 , loss: 0.0018839954\n",
            "iter: 628 , loss: 0.0018830686\n",
            "iter: 629 , loss: 0.0018821442\n",
            "iter: 630 , loss: 0.0018812212\n",
            "iter: 631 , loss: 0.0018803002\n",
            "iter: 632 , loss: 0.001879381\n",
            "iter: 633 , loss: 0.0018784644\n",
            "iter: 634 , loss: 0.0018775488\n",
            "iter: 635 , loss: 0.0018766363\n",
            "iter: 636 , loss: 0.0018757246\n",
            "iter: 637 , loss: 0.0018748157\n",
            "iter: 638 , loss: 0.0018739076\n",
            "iter: 639 , loss: 0.0018730027\n",
            "iter: 640 , loss: 0.0018720996\n",
            "iter: 641 , loss: 0.0018711974\n",
            "iter: 642 , loss: 0.0018702976\n",
            "iter: 643 , loss: 0.0018693999\n",
            "iter: 644 , loss: 0.0018685041\n",
            "iter: 645 , loss: 0.0018676096\n",
            "iter: 646 , loss: 0.0018667174\n",
            "iter: 647 , loss: 0.0018658271\n",
            "iter: 648 , loss: 0.0018649383\n",
            "iter: 649 , loss: 0.001864052\n",
            "iter: 650 , loss: 0.0018631673\n",
            "iter: 651 , loss: 0.0018622834\n",
            "iter: 652 , loss: 0.0018614026\n",
            "iter: 653 , loss: 0.0018605229\n",
            "iter: 654 , loss: 0.0018596451\n",
            "iter: 655 , loss: 0.0018587693\n",
            "iter: 656 , loss: 0.001857895\n",
            "iter: 657 , loss: 0.0018570228\n",
            "iter: 658 , loss: 0.0018561527\n",
            "iter: 659 , loss: 0.0018552832\n",
            "iter: 660 , loss: 0.001854416\n",
            "iter: 661 , loss: 0.0018535512\n",
            "iter: 662 , loss: 0.0018526872\n",
            "iter: 663 , loss: 0.0018518257\n",
            "iter: 664 , loss: 0.0018509653\n",
            "iter: 665 , loss: 0.0018501073\n",
            "iter: 666 , loss: 0.0018492509\n",
            "iter: 667 , loss: 0.0018483957\n",
            "iter: 668 , loss: 0.0018475428\n",
            "iter: 669 , loss: 0.0018466909\n",
            "iter: 670 , loss: 0.001845841\n",
            "iter: 671 , loss: 0.0018449927\n",
            "iter: 672 , loss: 0.0018441459\n",
            "iter: 673 , loss: 0.0018433019\n",
            "iter: 674 , loss: 0.0018424584\n",
            "iter: 675 , loss: 0.0018416167\n",
            "iter: 676 , loss: 0.0018407773\n",
            "iter: 677 , loss: 0.0018399389\n",
            "iter: 678 , loss: 0.0018391026\n",
            "iter: 679 , loss: 0.0018382673\n",
            "iter: 680 , loss: 0.0018374347\n",
            "iter: 681 , loss: 0.0018366026\n",
            "iter: 682 , loss: 0.0018357727\n",
            "iter: 683 , loss: 0.0018349444\n",
            "iter: 684 , loss: 0.0018341172\n",
            "iter: 685 , loss: 0.0018332923\n",
            "iter: 686 , loss: 0.0018324683\n",
            "iter: 687 , loss: 0.0018316464\n",
            "iter: 688 , loss: 0.0018308259\n",
            "iter: 689 , loss: 0.001830007\n",
            "iter: 690 , loss: 0.0018291894\n",
            "iter: 691 , loss: 0.001828374\n",
            "iter: 692 , loss: 0.0018275604\n",
            "iter: 693 , loss: 0.0018267473\n",
            "iter: 694 , loss: 0.0018259363\n",
            "iter: 695 , loss: 0.0018251266\n",
            "iter: 696 , loss: 0.0018243187\n",
            "iter: 697 , loss: 0.0018235124\n",
            "iter: 698 , loss: 0.0018227071\n",
            "iter: 699 , loss: 0.0018219036\n",
            "iter: 700 , loss: 0.0018211022\n",
            "iter: 701 , loss: 0.0018203018\n",
            "iter: 702 , loss: 0.0018195026\n",
            "iter: 703 , loss: 0.0018187055\n",
            "iter: 704 , loss: 0.0018179087\n",
            "iter: 705 , loss: 0.0018171147\n",
            "iter: 706 , loss: 0.0018163215\n",
            "iter: 707 , loss: 0.0018155301\n",
            "iter: 708 , loss: 0.0018147401\n",
            "iter: 709 , loss: 0.0018139512\n",
            "iter: 710 , loss: 0.0018131644\n",
            "iter: 711 , loss: 0.0018123788\n",
            "iter: 712 , loss: 0.0018115952\n",
            "iter: 713 , loss: 0.0018108122\n",
            "iter: 714 , loss: 0.0018100307\n",
            "iter: 715 , loss: 0.0018092511\n",
            "iter: 716 , loss: 0.001808473\n",
            "iter: 717 , loss: 0.0018076963\n",
            "iter: 718 , loss: 0.0018069203\n",
            "iter: 719 , loss: 0.0018061461\n",
            "iter: 720 , loss: 0.0018053738\n",
            "iter: 721 , loss: 0.001804602\n",
            "iter: 722 , loss: 0.0018038321\n",
            "iter: 723 , loss: 0.0018030636\n",
            "iter: 724 , loss: 0.0018022964\n",
            "iter: 725 , loss: 0.001801531\n",
            "iter: 726 , loss: 0.0018007659\n",
            "iter: 727 , loss: 0.0018000035\n",
            "iter: 728 , loss: 0.0017992414\n",
            "iter: 729 , loss: 0.0017984813\n",
            "iter: 730 , loss: 0.0017977224\n",
            "iter: 731 , loss: 0.0017969649\n",
            "iter: 732 , loss: 0.0017962087\n",
            "iter: 733 , loss: 0.0017954537\n",
            "iter: 734 , loss: 0.0017947007\n",
            "iter: 735 , loss: 0.0017939481\n",
            "iter: 736 , loss: 0.0017931974\n",
            "iter: 737 , loss: 0.0017924483\n",
            "iter: 738 , loss: 0.0017917\n",
            "iter: 739 , loss: 0.001790953\n",
            "iter: 740 , loss: 0.0017902079\n",
            "iter: 741 , loss: 0.0017894635\n",
            "iter: 742 , loss: 0.001788721\n",
            "iter: 743 , loss: 0.0017879788\n",
            "iter: 744 , loss: 0.0017872386\n",
            "iter: 745 , loss: 0.0017864999\n",
            "iter: 746 , loss: 0.0017857619\n",
            "iter: 747 , loss: 0.0017850257\n",
            "iter: 748 , loss: 0.0017842904\n",
            "iter: 749 , loss: 0.0017835569\n",
            "iter: 750 , loss: 0.0017828239\n",
            "iter: 751 , loss: 0.0017820928\n",
            "iter: 752 , loss: 0.0017813632\n",
            "iter: 753 , loss: 0.001780634\n",
            "iter: 754 , loss: 0.0017799067\n",
            "iter: 755 , loss: 0.0017791805\n",
            "iter: 756 , loss: 0.001778455\n",
            "iter: 757 , loss: 0.0017777315\n",
            "iter: 758 , loss: 0.0017770092\n",
            "iter: 759 , loss: 0.0017762876\n",
            "iter: 760 , loss: 0.0017755674\n",
            "iter: 761 , loss: 0.001774849\n",
            "iter: 762 , loss: 0.001774131\n",
            "iter: 763 , loss: 0.0017734148\n",
            "iter: 764 , loss: 0.0017727\n",
            "iter: 765 , loss: 0.0017719858\n",
            "iter: 766 , loss: 0.0017712732\n",
            "iter: 767 , loss: 0.0017705614\n",
            "iter: 768 , loss: 0.0017698511\n",
            "iter: 769 , loss: 0.0017691422\n",
            "iter: 770 , loss: 0.001768434\n",
            "iter: 771 , loss: 0.0017677274\n",
            "iter: 772 , loss: 0.001767022\n",
            "iter: 773 , loss: 0.0017663174\n",
            "iter: 774 , loss: 0.0017656141\n",
            "iter: 775 , loss: 0.0017649126\n",
            "iter: 776 , loss: 0.0017642112\n",
            "iter: 777 , loss: 0.0017635118\n",
            "iter: 778 , loss: 0.0017628131\n",
            "iter: 779 , loss: 0.0017621159\n",
            "iter: 780 , loss: 0.0017614198\n",
            "iter: 781 , loss: 0.0017607245\n",
            "iter: 782 , loss: 0.0017600304\n",
            "iter: 783 , loss: 0.0017593381\n",
            "iter: 784 , loss: 0.0017586461\n",
            "iter: 785 , loss: 0.0017579559\n",
            "iter: 786 , loss: 0.0017572668\n",
            "iter: 787 , loss: 0.0017565781\n",
            "iter: 788 , loss: 0.0017558913\n",
            "iter: 789 , loss: 0.0017552051\n",
            "iter: 790 , loss: 0.0017545205\n",
            "iter: 791 , loss: 0.0017538368\n",
            "iter: 792 , loss: 0.0017531536\n",
            "iter: 793 , loss: 0.0017524725\n",
            "iter: 794 , loss: 0.0017517923\n",
            "iter: 795 , loss: 0.001751113\n",
            "iter: 796 , loss: 0.0017504344\n",
            "iter: 797 , loss: 0.0017497577\n",
            "iter: 798 , loss: 0.0017490811\n",
            "iter: 799 , loss: 0.001748407\n",
            "iter: 800 , loss: 0.0017477327\n",
            "iter: 801 , loss: 0.0017470601\n",
            "iter: 802 , loss: 0.0017463887\n",
            "iter: 803 , loss: 0.0017457182\n",
            "iter: 804 , loss: 0.0017450486\n",
            "iter: 805 , loss: 0.0017443799\n",
            "iter: 806 , loss: 0.0017437123\n",
            "iter: 807 , loss: 0.0017430463\n",
            "iter: 808 , loss: 0.0017423809\n",
            "iter: 809 , loss: 0.0017417166\n",
            "iter: 810 , loss: 0.0017410538\n",
            "iter: 811 , loss: 0.0017403916\n",
            "iter: 812 , loss: 0.0017397305\n",
            "iter: 813 , loss: 0.0017390706\n",
            "iter: 814 , loss: 0.0017384117\n",
            "iter: 815 , loss: 0.0017377537\n",
            "iter: 816 , loss: 0.0017370969\n",
            "iter: 817 , loss: 0.001736441\n",
            "iter: 818 , loss: 0.0017357861\n",
            "iter: 819 , loss: 0.0017351325\n",
            "iter: 820 , loss: 0.0017344796\n",
            "iter: 821 , loss: 0.0017338276\n",
            "iter: 822 , loss: 0.0017331769\n",
            "iter: 823 , loss: 0.0017325277\n",
            "iter: 824 , loss: 0.0017318787\n",
            "iter: 825 , loss: 0.0017312312\n",
            "iter: 826 , loss: 0.0017305841\n",
            "iter: 827 , loss: 0.0017299382\n",
            "iter: 828 , loss: 0.0017292934\n",
            "iter: 829 , loss: 0.0017286498\n",
            "iter: 830 , loss: 0.0017280075\n",
            "iter: 831 , loss: 0.0017273654\n",
            "iter: 832 , loss: 0.0017267246\n",
            "iter: 833 , loss: 0.0017260851\n",
            "iter: 834 , loss: 0.0017254462\n",
            "iter: 835 , loss: 0.0017248084\n",
            "iter: 836 , loss: 0.0017241716\n",
            "iter: 837 , loss: 0.0017235358\n",
            "iter: 838 , loss: 0.0017229011\n",
            "iter: 839 , loss: 0.0017222669\n",
            "iter: 840 , loss: 0.0017216341\n",
            "iter: 841 , loss: 0.001721002\n",
            "iter: 842 , loss: 0.001720371\n",
            "iter: 843 , loss: 0.0017197413\n",
            "iter: 844 , loss: 0.0017191115\n",
            "iter: 845 , loss: 0.0017184836\n",
            "iter: 846 , loss: 0.0017178565\n",
            "iter: 847 , loss: 0.0017172298\n",
            "iter: 848 , loss: 0.0017166047\n",
            "iter: 849 , loss: 0.0017159804\n",
            "iter: 850 , loss: 0.0017153565\n",
            "iter: 851 , loss: 0.0017147345\n",
            "iter: 852 , loss: 0.0017141132\n",
            "iter: 853 , loss: 0.001713492\n",
            "iter: 854 , loss: 0.0017128724\n",
            "iter: 855 , loss: 0.0017122534\n",
            "iter: 856 , loss: 0.0017116359\n",
            "iter: 857 , loss: 0.0017110186\n",
            "iter: 858 , loss: 0.0017104029\n",
            "iter: 859 , loss: 0.0017097875\n",
            "iter: 860 , loss: 0.0017091734\n",
            "iter: 861 , loss: 0.0017085599\n",
            "iter: 862 , loss: 0.0017079477\n",
            "iter: 863 , loss: 0.0017073359\n",
            "iter: 864 , loss: 0.0017067256\n",
            "iter: 865 , loss: 0.0017061156\n",
            "iter: 866 , loss: 0.0017055068\n",
            "iter: 867 , loss: 0.001704899\n",
            "iter: 868 , loss: 0.001704292\n",
            "iter: 869 , loss: 0.0017036854\n",
            "iter: 870 , loss: 0.0017030803\n",
            "iter: 871 , loss: 0.001702476\n",
            "iter: 872 , loss: 0.0017018726\n",
            "iter: 873 , loss: 0.0017012695\n",
            "iter: 874 , loss: 0.0017006682\n",
            "iter: 875 , loss: 0.0017000673\n",
            "iter: 876 , loss: 0.001699467\n",
            "iter: 877 , loss: 0.0016988682\n",
            "iter: 878 , loss: 0.0016982697\n",
            "iter: 879 , loss: 0.0016976724\n",
            "iter: 880 , loss: 0.0016970758\n",
            "iter: 881 , loss: 0.00169648\n",
            "iter: 882 , loss: 0.0016958851\n",
            "iter: 883 , loss: 0.0016952913\n",
            "iter: 884 , loss: 0.001694698\n",
            "iter: 885 , loss: 0.0016941063\n",
            "iter: 886 , loss: 0.0016935142\n",
            "iter: 887 , loss: 0.0016929241\n",
            "iter: 888 , loss: 0.0016923341\n",
            "iter: 889 , loss: 0.0016917449\n",
            "iter: 890 , loss: 0.0016911572\n",
            "iter: 891 , loss: 0.0016905696\n",
            "iter: 892 , loss: 0.0016899833\n",
            "iter: 893 , loss: 0.001689398\n",
            "iter: 894 , loss: 0.0016888131\n",
            "iter: 895 , loss: 0.0016882294\n",
            "iter: 896 , loss: 0.0016876463\n",
            "iter: 897 , loss: 0.0016870644\n",
            "iter: 898 , loss: 0.0016864822\n",
            "iter: 899 , loss: 0.0016859018\n",
            "iter: 900 , loss: 0.0016853221\n",
            "iter: 901 , loss: 0.0016847431\n",
            "iter: 902 , loss: 0.0016841649\n",
            "iter: 903 , loss: 0.0016835873\n",
            "iter: 904 , loss: 0.001683011\n",
            "iter: 905 , loss: 0.0016824349\n",
            "iter: 906 , loss: 0.00168186\n",
            "iter: 907 , loss: 0.0016812858\n",
            "iter: 908 , loss: 0.0016807122\n",
            "iter: 909 , loss: 0.0016801404\n",
            "iter: 910 , loss: 0.0016795679\n",
            "iter: 911 , loss: 0.0016789971\n",
            "iter: 912 , loss: 0.0016784266\n",
            "iter: 913 , loss: 0.0016778572\n",
            "iter: 914 , loss: 0.0016772889\n",
            "iter: 915 , loss: 0.0016767209\n",
            "iter: 916 , loss: 0.0016761535\n",
            "iter: 917 , loss: 0.0016755876\n",
            "iter: 918 , loss: 0.0016750218\n",
            "iter: 919 , loss: 0.0016744573\n",
            "iter: 920 , loss: 0.0016738937\n",
            "iter: 921 , loss: 0.00167333\n",
            "iter: 922 , loss: 0.0016727675\n",
            "iter: 923 , loss: 0.0016722062\n",
            "iter: 924 , loss: 0.0016716454\n",
            "iter: 925 , loss: 0.0016710853\n",
            "iter: 926 , loss: 0.0016705262\n",
            "iter: 927 , loss: 0.0016699671\n",
            "iter: 928 , loss: 0.0016694098\n",
            "iter: 929 , loss: 0.0016688523\n",
            "iter: 930 , loss: 0.0016682963\n",
            "iter: 931 , loss: 0.0016677402\n",
            "iter: 932 , loss: 0.0016671853\n",
            "iter: 933 , loss: 0.0016666315\n",
            "iter: 934 , loss: 0.0016660783\n",
            "iter: 935 , loss: 0.0016655257\n",
            "iter: 936 , loss: 0.001664974\n",
            "iter: 937 , loss: 0.0016644227\n",
            "iter: 938 , loss: 0.0016638724\n",
            "iter: 939 , loss: 0.001663323\n",
            "iter: 940 , loss: 0.0016627741\n",
            "iter: 941 , loss: 0.0016622255\n",
            "iter: 942 , loss: 0.0016616784\n",
            "iter: 943 , loss: 0.0016611314\n",
            "iter: 944 , loss: 0.0016605859\n",
            "iter: 945 , loss: 0.0016600403\n",
            "iter: 946 , loss: 0.0016594961\n",
            "iter: 947 , loss: 0.0016589522\n",
            "iter: 948 , loss: 0.0016584089\n",
            "iter: 949 , loss: 0.0016578666\n",
            "iter: 950 , loss: 0.0016573248\n",
            "iter: 951 , loss: 0.0016567836\n",
            "iter: 952 , loss: 0.001656244\n",
            "iter: 953 , loss: 0.0016557041\n",
            "iter: 954 , loss: 0.0016551657\n",
            "iter: 955 , loss: 0.0016546277\n",
            "iter: 956 , loss: 0.00165409\n",
            "iter: 957 , loss: 0.0016535532\n",
            "iter: 958 , loss: 0.0016530174\n",
            "iter: 959 , loss: 0.0016524822\n",
            "iter: 960 , loss: 0.0016519476\n",
            "iter: 961 , loss: 0.0016514142\n",
            "iter: 962 , loss: 0.0016508807\n",
            "iter: 963 , loss: 0.0016503483\n",
            "iter: 964 , loss: 0.0016498164\n",
            "iter: 965 , loss: 0.0016492854\n",
            "iter: 966 , loss: 0.0016487548\n",
            "iter: 967 , loss: 0.001648225\n",
            "iter: 968 , loss: 0.0016476961\n",
            "iter: 969 , loss: 0.0016471673\n",
            "iter: 970 , loss: 0.0016466401\n",
            "iter: 971 , loss: 0.0016461129\n",
            "iter: 972 , loss: 0.0016455867\n",
            "iter: 973 , loss: 0.0016450612\n",
            "iter: 974 , loss: 0.001644536\n",
            "iter: 975 , loss: 0.0016440118\n",
            "iter: 976 , loss: 0.0016434882\n",
            "iter: 977 , loss: 0.0016429655\n",
            "iter: 978 , loss: 0.0016424435\n",
            "iter: 979 , loss: 0.0016419216\n",
            "iter: 980 , loss: 0.0016414003\n",
            "iter: 981 , loss: 0.0016408804\n",
            "iter: 982 , loss: 0.001640361\n",
            "iter: 983 , loss: 0.0016398416\n",
            "iter: 984 , loss: 0.0016393237\n",
            "iter: 985 , loss: 0.0016388053\n",
            "iter: 986 , loss: 0.0016382888\n",
            "iter: 987 , loss: 0.0016377725\n",
            "iter: 988 , loss: 0.0016372568\n",
            "iter: 989 , loss: 0.0016367416\n",
            "iter: 990 , loss: 0.0016362274\n",
            "iter: 991 , loss: 0.0016357137\n",
            "iter: 992 , loss: 0.0016352006\n",
            "iter: 993 , loss: 0.001634688\n",
            "iter: 994 , loss: 0.0016341764\n",
            "iter: 995 , loss: 0.0016336653\n",
            "iter: 996 , loss: 0.0016331547\n",
            "iter: 997 , loss: 0.0016326449\n",
            "iter: 998 , loss: 0.0016321352\n",
            "iter: 999 , loss: 0.0016316274\n",
            "iter: 1000 , loss: 0.0016311188\n",
            "iter: 1001 , loss: 0.0016306121\n",
            "iter: 1002 , loss: 0.0016301046\n",
            "iter: 1003 , loss: 0.0016295991\n",
            "iter: 1004 , loss: 0.0016290938\n",
            "iter: 1005 , loss: 0.001628589\n",
            "iter: 1006 , loss: 0.0016280843\n",
            "iter: 1007 , loss: 0.0016275808\n",
            "iter: 1008 , loss: 0.0016270782\n",
            "iter: 1009 , loss: 0.0016265761\n",
            "iter: 1010 , loss: 0.0016260742\n",
            "iter: 1011 , loss: 0.0016255731\n",
            "iter: 1012 , loss: 0.001625073\n",
            "iter: 1013 , loss: 0.001624573\n",
            "iter: 1014 , loss: 0.0016240738\n",
            "iter: 1015 , loss: 0.0016235758\n",
            "iter: 1016 , loss: 0.0016230772\n",
            "iter: 1017 , loss: 0.0016225801\n",
            "iter: 1018 , loss: 0.0016220834\n",
            "iter: 1019 , loss: 0.0016215867\n",
            "iter: 1020 , loss: 0.0016210916\n",
            "iter: 1021 , loss: 0.001620596\n",
            "iter: 1022 , loss: 0.0016201021\n",
            "iter: 1023 , loss: 0.0016196084\n",
            "iter: 1024 , loss: 0.0016191152\n",
            "iter: 1025 , loss: 0.0016186226\n",
            "iter: 1026 , loss: 0.0016181307\n",
            "iter: 1027 , loss: 0.0016176391\n",
            "iter: 1028 , loss: 0.0016171485\n",
            "iter: 1029 , loss: 0.0016166583\n",
            "iter: 1030 , loss: 0.0016161689\n",
            "iter: 1031 , loss: 0.0016156799\n",
            "iter: 1032 , loss: 0.0016151916\n",
            "iter: 1033 , loss: 0.001614704\n",
            "iter: 1034 , loss: 0.0016142167\n",
            "iter: 1035 , loss: 0.0016137301\n",
            "iter: 1036 , loss: 0.0016132441\n",
            "iter: 1037 , loss: 0.0016127587\n",
            "iter: 1038 , loss: 0.001612274\n",
            "iter: 1039 , loss: 0.0016117893\n",
            "iter: 1040 , loss: 0.0016113059\n",
            "iter: 1041 , loss: 0.001610823\n",
            "iter: 1042 , loss: 0.0016103406\n",
            "iter: 1043 , loss: 0.0016098579\n",
            "iter: 1044 , loss: 0.0016093766\n",
            "iter: 1045 , loss: 0.0016088961\n",
            "iter: 1046 , loss: 0.0016084155\n",
            "iter: 1047 , loss: 0.001607936\n",
            "iter: 1048 , loss: 0.0016074571\n",
            "iter: 1049 , loss: 0.0016069787\n",
            "iter: 1050 , loss: 0.0016065004\n",
            "iter: 1051 , loss: 0.001606023\n",
            "iter: 1052 , loss: 0.0016055462\n",
            "iter: 1053 , loss: 0.0016050698\n",
            "iter: 1054 , loss: 0.0016045943\n",
            "iter: 1055 , loss: 0.001604119\n",
            "iter: 1056 , loss: 0.0016036446\n",
            "iter: 1057 , loss: 0.0016031705\n",
            "iter: 1058 , loss: 0.001602697\n",
            "iter: 1059 , loss: 0.0016022241\n",
            "iter: 1060 , loss: 0.0016017518\n",
            "iter: 1061 , loss: 0.0016012801\n",
            "iter: 1062 , loss: 0.0016008088\n",
            "iter: 1063 , loss: 0.0016003379\n",
            "iter: 1064 , loss: 0.0015998674\n",
            "iter: 1065 , loss: 0.0015993981\n",
            "iter: 1066 , loss: 0.0015989288\n",
            "iter: 1067 , loss: 0.001598461\n",
            "iter: 1068 , loss: 0.0015979925\n",
            "iter: 1069 , loss: 0.0015975252\n",
            "iter: 1070 , loss: 0.0015970583\n",
            "iter: 1071 , loss: 0.0015965923\n",
            "iter: 1072 , loss: 0.0015961264\n",
            "iter: 1073 , loss: 0.0015956607\n",
            "iter: 1074 , loss: 0.0015951962\n",
            "iter: 1075 , loss: 0.0015947318\n",
            "iter: 1076 , loss: 0.001594269\n",
            "iter: 1077 , loss: 0.0015938053\n",
            "iter: 1078 , loss: 0.0015933427\n",
            "iter: 1079 , loss: 0.0015928805\n",
            "iter: 1080 , loss: 0.0015924195\n",
            "iter: 1081 , loss: 0.0015919581\n",
            "iter: 1082 , loss: 0.0015914977\n",
            "iter: 1083 , loss: 0.001591038\n",
            "iter: 1084 , loss: 0.0015905787\n",
            "iter: 1085 , loss: 0.0015901193\n",
            "iter: 1086 , loss: 0.0015896612\n",
            "iter: 1087 , loss: 0.0015892035\n",
            "iter: 1088 , loss: 0.0015887462\n",
            "iter: 1089 , loss: 0.0015882893\n",
            "iter: 1090 , loss: 0.0015878329\n",
            "iter: 1091 , loss: 0.0015873774\n",
            "iter: 1092 , loss: 0.0015869221\n",
            "iter: 1093 , loss: 0.0015864674\n",
            "iter: 1094 , loss: 0.0015860134\n",
            "iter: 1095 , loss: 0.0015855599\n",
            "iter: 1096 , loss: 0.0015851068\n",
            "iter: 1097 , loss: 0.0015846537\n",
            "iter: 1098 , loss: 0.0015842017\n",
            "iter: 1099 , loss: 0.0015837502\n",
            "iter: 1100 , loss: 0.001583299\n",
            "iter: 1101 , loss: 0.0015828485\n",
            "iter: 1102 , loss: 0.0015823983\n",
            "iter: 1103 , loss: 0.0015819492\n",
            "iter: 1104 , loss: 0.0015814999\n",
            "iter: 1105 , loss: 0.0015810509\n",
            "iter: 1106 , loss: 0.0015806032\n",
            "iter: 1107 , loss: 0.0015801552\n",
            "iter: 1108 , loss: 0.0015797084\n",
            "iter: 1109 , loss: 0.0015792617\n",
            "iter: 1110 , loss: 0.0015788159\n",
            "iter: 1111 , loss: 0.0015783701\n",
            "iter: 1112 , loss: 0.0015779252\n",
            "iter: 1113 , loss: 0.0015774803\n",
            "iter: 1114 , loss: 0.0015770366\n",
            "iter: 1115 , loss: 0.0015765929\n",
            "iter: 1116 , loss: 0.0015761501\n",
            "iter: 1117 , loss: 0.0015757078\n",
            "iter: 1118 , loss: 0.001575265\n",
            "iter: 1119 , loss: 0.0015748235\n",
            "iter: 1120 , loss: 0.0015743823\n",
            "iter: 1121 , loss: 0.0015739414\n",
            "iter: 1122 , loss: 0.0015735017\n",
            "iter: 1123 , loss: 0.001573062\n",
            "iter: 1124 , loss: 0.0015726227\n",
            "iter: 1125 , loss: 0.0015721843\n",
            "iter: 1126 , loss: 0.0015717462\n",
            "iter: 1127 , loss: 0.0015713088\n",
            "iter: 1128 , loss: 0.001570871\n",
            "iter: 1129 , loss: 0.0015704345\n",
            "iter: 1130 , loss: 0.0015699981\n",
            "iter: 1131 , loss: 0.0015695624\n",
            "iter: 1132 , loss: 0.0015691272\n",
            "iter: 1133 , loss: 0.001568692\n",
            "iter: 1134 , loss: 0.0015682579\n",
            "iter: 1135 , loss: 0.0015678245\n",
            "iter: 1136 , loss: 0.0015673903\n",
            "iter: 1137 , loss: 0.0015669574\n",
            "iter: 1138 , loss: 0.0015665252\n",
            "iter: 1139 , loss: 0.0015660933\n",
            "iter: 1140 , loss: 0.0015656613\n",
            "iter: 1141 , loss: 0.0015652305\n",
            "iter: 1142 , loss: 0.0015648003\n",
            "iter: 1143 , loss: 0.0015643698\n",
            "iter: 1144 , loss: 0.0015639401\n",
            "iter: 1145 , loss: 0.0015635112\n",
            "iter: 1146 , loss: 0.0015630824\n",
            "iter: 1147 , loss: 0.001562654\n",
            "iter: 1148 , loss: 0.0015622264\n",
            "iter: 1149 , loss: 0.0015617987\n",
            "iter: 1150 , loss: 0.0015613722\n",
            "iter: 1151 , loss: 0.0015609455\n",
            "iter: 1152 , loss: 0.0015605196\n",
            "iter: 1153 , loss: 0.0015600943\n",
            "iter: 1154 , loss: 0.0015596694\n",
            "iter: 1155 , loss: 0.0015592445\n",
            "iter: 1156 , loss: 0.0015588206\n",
            "iter: 1157 , loss: 0.0015583967\n",
            "iter: 1158 , loss: 0.0015579734\n",
            "iter: 1159 , loss: 0.0015575509\n",
            "iter: 1160 , loss: 0.0015571284\n",
            "iter: 1161 , loss: 0.0015567067\n",
            "iter: 1162 , loss: 0.001556285\n",
            "iter: 1163 , loss: 0.0015558643\n",
            "iter: 1164 , loss: 0.0015554435\n",
            "iter: 1165 , loss: 0.0015550236\n",
            "iter: 1166 , loss: 0.0015546043\n",
            "iter: 1167 , loss: 0.0015541854\n",
            "iter: 1168 , loss: 0.0015537662\n",
            "iter: 1169 , loss: 0.0015533481\n",
            "iter: 1170 , loss: 0.0015529303\n",
            "iter: 1171 , loss: 0.0015525131\n",
            "iter: 1172 , loss: 0.0015520962\n",
            "iter: 1173 , loss: 0.0015516792\n",
            "iter: 1174 , loss: 0.0015512636\n",
            "iter: 1175 , loss: 0.0015508479\n",
            "iter: 1176 , loss: 0.0015504326\n",
            "iter: 1177 , loss: 0.0015500179\n",
            "iter: 1178 , loss: 0.0015496037\n",
            "iter: 1179 , loss: 0.0015491897\n",
            "iter: 1180 , loss: 0.0015487764\n",
            "iter: 1181 , loss: 0.0015483634\n",
            "iter: 1182 , loss: 0.0015479508\n",
            "iter: 1183 , loss: 0.0015475389\n",
            "iter: 1184 , loss: 0.001547127\n",
            "iter: 1185 , loss: 0.0015467157\n",
            "iter: 1186 , loss: 0.001546305\n",
            "iter: 1187 , loss: 0.0015458945\n",
            "iter: 1188 , loss: 0.0015454845\n",
            "iter: 1189 , loss: 0.0015450752\n",
            "iter: 1190 , loss: 0.0015446662\n",
            "iter: 1191 , loss: 0.001544257\n",
            "iter: 1192 , loss: 0.0015438489\n",
            "iter: 1193 , loss: 0.001543441\n",
            "iter: 1194 , loss: 0.0015430339\n",
            "iter: 1195 , loss: 0.001542627\n",
            "iter: 1196 , loss: 0.0015422205\n",
            "iter: 1197 , loss: 0.001541814\n",
            "iter: 1198 , loss: 0.0015414082\n",
            "iter: 1199 , loss: 0.0015410032\n",
            "iter: 1200 , loss: 0.0015405981\n",
            "iter: 1201 , loss: 0.0015401939\n",
            "iter: 1202 , loss: 0.0015397898\n",
            "iter: 1203 , loss: 0.0015393861\n",
            "iter: 1204 , loss: 0.0015389828\n",
            "iter: 1205 , loss: 0.0015385799\n",
            "iter: 1206 , loss: 0.001538178\n",
            "iter: 1207 , loss: 0.001537776\n",
            "iter: 1208 , loss: 0.0015373743\n",
            "iter: 1209 , loss: 0.0015369727\n",
            "iter: 1210 , loss: 0.0015365728\n",
            "iter: 1211 , loss: 0.0015361722\n",
            "iter: 1212 , loss: 0.0015357727\n",
            "iter: 1213 , loss: 0.0015353727\n",
            "iter: 1214 , loss: 0.0015349741\n",
            "iter: 1215 , loss: 0.001534575\n",
            "iter: 1216 , loss: 0.0015341769\n",
            "iter: 1217 , loss: 0.0015337792\n",
            "iter: 1218 , loss: 0.0015333819\n",
            "iter: 1219 , loss: 0.0015329848\n",
            "iter: 1220 , loss: 0.001532588\n",
            "iter: 1221 , loss: 0.0015321919\n",
            "iter: 1222 , loss: 0.0015317958\n",
            "iter: 1223 , loss: 0.0015314006\n",
            "iter: 1224 , loss: 0.0015310056\n",
            "iter: 1225 , loss: 0.0015306111\n",
            "iter: 1226 , loss: 0.001530217\n",
            "iter: 1227 , loss: 0.0015298229\n",
            "iter: 1228 , loss: 0.0015294298\n",
            "iter: 1229 , loss: 0.0015290364\n",
            "iter: 1230 , loss: 0.0015286442\n",
            "iter: 1231 , loss: 0.0015282518\n",
            "iter: 1232 , loss: 0.0015278597\n",
            "iter: 1233 , loss: 0.0015274686\n",
            "iter: 1234 , loss: 0.0015270776\n",
            "iter: 1235 , loss: 0.0015266872\n",
            "iter: 1236 , loss: 0.0015262965\n",
            "iter: 1237 , loss: 0.0015259065\n",
            "iter: 1238 , loss: 0.0015255173\n",
            "iter: 1239 , loss: 0.0015251283\n",
            "iter: 1240 , loss: 0.00152474\n",
            "iter: 1241 , loss: 0.0015243513\n",
            "iter: 1242 , loss: 0.0015239638\n",
            "iter: 1243 , loss: 0.0015235761\n",
            "iter: 1244 , loss: 0.0015231889\n",
            "iter: 1245 , loss: 0.0015228022\n",
            "iter: 1246 , loss: 0.0015224161\n",
            "iter: 1247 , loss: 0.0015220299\n",
            "iter: 1248 , loss: 0.0015216442\n",
            "iter: 1249 , loss: 0.001521259\n",
            "iter: 1250 , loss: 0.0015208744\n",
            "iter: 1251 , loss: 0.0015204902\n",
            "iter: 1252 , loss: 0.0015201058\n",
            "iter: 1253 , loss: 0.0015197224\n",
            "iter: 1254 , loss: 0.0015193389\n",
            "iter: 1255 , loss: 0.0015189562\n",
            "iter: 1256 , loss: 0.0015185738\n",
            "iter: 1257 , loss: 0.0015181917\n",
            "iter: 1258 , loss: 0.0015178099\n",
            "iter: 1259 , loss: 0.0015174289\n",
            "iter: 1260 , loss: 0.0015170474\n",
            "iter: 1261 , loss: 0.0015166666\n",
            "iter: 1262 , loss: 0.0015162866\n",
            "iter: 1263 , loss: 0.0015159067\n",
            "iter: 1264 , loss: 0.0015155275\n",
            "iter: 1265 , loss: 0.0015151481\n",
            "iter: 1266 , loss: 0.0015147699\n",
            "iter: 1267 , loss: 0.0015143908\n",
            "iter: 1268 , loss: 0.0015140132\n",
            "iter: 1269 , loss: 0.001513635\n",
            "iter: 1270 , loss: 0.0015132586\n",
            "iter: 1271 , loss: 0.0015128811\n",
            "iter: 1272 , loss: 0.0015125051\n",
            "iter: 1273 , loss: 0.0015121286\n",
            "iter: 1274 , loss: 0.0015117531\n",
            "iter: 1275 , loss: 0.0015113776\n",
            "iter: 1276 , loss: 0.0015110028\n",
            "iter: 1277 , loss: 0.001510628\n",
            "iter: 1278 , loss: 0.0015102532\n",
            "iter: 1279 , loss: 0.0015098795\n",
            "iter: 1280 , loss: 0.0015095057\n",
            "iter: 1281 , loss: 0.0015091331\n",
            "iter: 1282 , loss: 0.0015087604\n",
            "iter: 1283 , loss: 0.0015083876\n",
            "iter: 1284 , loss: 0.0015080161\n",
            "iter: 1285 , loss: 0.0015076435\n",
            "iter: 1286 , loss: 0.0015072722\n",
            "iter: 1287 , loss: 0.0015069013\n",
            "iter: 1288 , loss: 0.0015065308\n",
            "iter: 1289 , loss: 0.00150616\n",
            "iter: 1290 , loss: 0.0015057907\n",
            "iter: 1291 , loss: 0.0015054208\n",
            "iter: 1292 , loss: 0.0015050519\n",
            "iter: 1293 , loss: 0.001504683\n",
            "iter: 1294 , loss: 0.0015043145\n",
            "iter: 1295 , loss: 0.0015039463\n",
            "iter: 1296 , loss: 0.0015035783\n",
            "iter: 1297 , loss: 0.0015032113\n",
            "iter: 1298 , loss: 0.0015028439\n",
            "iter: 1299 , loss: 0.0015024773\n",
            "iter: 1300 , loss: 0.0015021107\n",
            "iter: 1301 , loss: 0.0015017448\n",
            "iter: 1302 , loss: 0.0015013793\n",
            "iter: 1303 , loss: 0.001501014\n",
            "iter: 1304 , loss: 0.0015006487\n",
            "iter: 1305 , loss: 0.0015002841\n",
            "iter: 1306 , loss: 0.0014999198\n",
            "iter: 1307 , loss: 0.0014995559\n",
            "iter: 1308 , loss: 0.0014991926\n",
            "iter: 1309 , loss: 0.0014988293\n",
            "iter: 1310 , loss: 0.0014984667\n",
            "iter: 1311 , loss: 0.001498104\n",
            "iter: 1312 , loss: 0.0014977418\n",
            "iter: 1313 , loss: 0.0014973802\n",
            "iter: 1314 , loss: 0.0014970183\n",
            "iter: 1315 , loss: 0.001496657\n",
            "iter: 1316 , loss: 0.0014962965\n",
            "iter: 1317 , loss: 0.0014959362\n",
            "iter: 1318 , loss: 0.0014955755\n",
            "iter: 1319 , loss: 0.0014952159\n",
            "iter: 1320 , loss: 0.0014948564\n",
            "iter: 1321 , loss: 0.0014944976\n",
            "iter: 1322 , loss: 0.0014941387\n",
            "iter: 1323 , loss: 0.0014937804\n",
            "iter: 1324 , loss: 0.0014934223\n",
            "iter: 1325 , loss: 0.0014930648\n",
            "iter: 1326 , loss: 0.0014927073\n",
            "iter: 1327 , loss: 0.0014923499\n",
            "iter: 1328 , loss: 0.0014919937\n",
            "iter: 1329 , loss: 0.0014916371\n",
            "iter: 1330 , loss: 0.001491281\n",
            "iter: 1331 , loss: 0.0014909254\n",
            "iter: 1332 , loss: 0.0014905699\n",
            "iter: 1333 , loss: 0.0014902151\n",
            "iter: 1334 , loss: 0.0014898604\n",
            "iter: 1335 , loss: 0.0014895059\n",
            "iter: 1336 , loss: 0.0014891514\n",
            "iter: 1337 , loss: 0.0014887978\n",
            "iter: 1338 , loss: 0.0014884446\n",
            "iter: 1339 , loss: 0.0014880917\n",
            "iter: 1340 , loss: 0.0014877392\n",
            "iter: 1341 , loss: 0.0014873869\n",
            "iter: 1342 , loss: 0.0014870347\n",
            "iter: 1343 , loss: 0.0014866829\n",
            "iter: 1344 , loss: 0.0014863313\n",
            "iter: 1345 , loss: 0.0014859807\n",
            "iter: 1346 , loss: 0.0014856297\n",
            "iter: 1347 , loss: 0.0014852794\n",
            "iter: 1348 , loss: 0.0014849292\n",
            "iter: 1349 , loss: 0.0014845794\n",
            "iter: 1350 , loss: 0.00148423\n",
            "iter: 1351 , loss: 0.0014838809\n",
            "iter: 1352 , loss: 0.0014835324\n",
            "iter: 1353 , loss: 0.0014831831\n",
            "iter: 1354 , loss: 0.0014828357\n",
            "iter: 1355 , loss: 0.0014824879\n",
            "iter: 1356 , loss: 0.0014821402\n",
            "iter: 1357 , loss: 0.0014817931\n",
            "iter: 1358 , loss: 0.0014814466\n",
            "iter: 1359 , loss: 0.0014811\n",
            "iter: 1360 , loss: 0.0014807535\n",
            "iter: 1361 , loss: 0.0014804075\n",
            "iter: 1362 , loss: 0.0014800623\n",
            "iter: 1363 , loss: 0.0014797169\n",
            "iter: 1364 , loss: 0.001479372\n",
            "iter: 1365 , loss: 0.0014790276\n",
            "iter: 1366 , loss: 0.0014786832\n",
            "iter: 1367 , loss: 0.0014783391\n",
            "iter: 1368 , loss: 0.0014779958\n",
            "iter: 1369 , loss: 0.0014776523\n",
            "iter: 1370 , loss: 0.0014773093\n",
            "iter: 1371 , loss: 0.0014769662\n",
            "iter: 1372 , loss: 0.0014766245\n",
            "iter: 1373 , loss: 0.0014762823\n",
            "iter: 1374 , loss: 0.0014759405\n",
            "iter: 1375 , loss: 0.0014755991\n",
            "iter: 1376 , loss: 0.0014752579\n",
            "iter: 1377 , loss: 0.0014749174\n",
            "iter: 1378 , loss: 0.0014745767\n",
            "iter: 1379 , loss: 0.0014742365\n",
            "iter: 1380 , loss: 0.0014738962\n",
            "iter: 1381 , loss: 0.0014735572\n",
            "iter: 1382 , loss: 0.001473218\n",
            "iter: 1383 , loss: 0.0014728784\n",
            "iter: 1384 , loss: 0.00147254\n",
            "iter: 1385 , loss: 0.0014722018\n",
            "iter: 1386 , loss: 0.0014718638\n",
            "iter: 1387 , loss: 0.0014715259\n",
            "iter: 1388 , loss: 0.0014711889\n",
            "iter: 1389 , loss: 0.0014708516\n",
            "iter: 1390 , loss: 0.0014705152\n",
            "iter: 1391 , loss: 0.0014701781\n",
            "iter: 1392 , loss: 0.0014698426\n",
            "iter: 1393 , loss: 0.0014695066\n",
            "iter: 1394 , loss: 0.0014691707\n",
            "iter: 1395 , loss: 0.0014688356\n",
            "iter: 1396 , loss: 0.0014685005\n",
            "iter: 1397 , loss: 0.0014681659\n",
            "iter: 1398 , loss: 0.0014678311\n",
            "iter: 1399 , loss: 0.0014674977\n",
            "iter: 1400 , loss: 0.0014671634\n",
            "iter: 1401 , loss: 0.0014668303\n",
            "iter: 1402 , loss: 0.001466497\n",
            "iter: 1403 , loss: 0.0014661647\n",
            "iter: 1404 , loss: 0.0014658319\n",
            "iter: 1405 , loss: 0.0014654996\n",
            "iter: 1406 , loss: 0.0014651679\n",
            "iter: 1407 , loss: 0.0014648361\n",
            "iter: 1408 , loss: 0.0014645046\n",
            "iter: 1409 , loss: 0.0014641738\n",
            "iter: 1410 , loss: 0.0014638432\n",
            "iter: 1411 , loss: 0.0014635122\n",
            "iter: 1412 , loss: 0.0014631822\n",
            "iter: 1413 , loss: 0.0014628522\n",
            "iter: 1414 , loss: 0.0014625229\n",
            "iter: 1415 , loss: 0.0014621938\n",
            "iter: 1416 , loss: 0.0014618644\n",
            "iter: 1417 , loss: 0.0014615357\n",
            "iter: 1418 , loss: 0.0014612076\n",
            "iter: 1419 , loss: 0.0014608798\n",
            "iter: 1420 , loss: 0.0014605513\n",
            "iter: 1421 , loss: 0.0014602239\n",
            "iter: 1422 , loss: 0.0014598971\n",
            "iter: 1423 , loss: 0.0014595699\n",
            "iter: 1424 , loss: 0.0014592431\n",
            "iter: 1425 , loss: 0.0014589172\n",
            "iter: 1426 , loss: 0.0014585909\n",
            "iter: 1427 , loss: 0.0014582651\n",
            "iter: 1428 , loss: 0.0014579396\n",
            "iter: 1429 , loss: 0.0014576146\n",
            "iter: 1430 , loss: 0.0014572896\n",
            "iter: 1431 , loss: 0.0014569651\n",
            "iter: 1432 , loss: 0.001456641\n",
            "iter: 1433 , loss: 0.0014563167\n",
            "iter: 1434 , loss: 0.0014559929\n",
            "iter: 1435 , loss: 0.0014556694\n",
            "iter: 1436 , loss: 0.0014553466\n",
            "iter: 1437 , loss: 0.0014550234\n",
            "iter: 1438 , loss: 0.001454701\n",
            "iter: 1439 , loss: 0.0014543787\n",
            "iter: 1440 , loss: 0.0014540566\n",
            "iter: 1441 , loss: 0.0014537348\n",
            "iter: 1442 , loss: 0.0014534132\n",
            "iter: 1443 , loss: 0.001453092\n",
            "iter: 1444 , loss: 0.0014527711\n",
            "iter: 1445 , loss: 0.0014524509\n",
            "iter: 1446 , loss: 0.0014521304\n",
            "iter: 1447 , loss: 0.0014518106\n",
            "iter: 1448 , loss: 0.0014514906\n",
            "iter: 1449 , loss: 0.0014511709\n",
            "iter: 1450 , loss: 0.0014508517\n",
            "iter: 1451 , loss: 0.0014505329\n",
            "iter: 1452 , loss: 0.0014502144\n",
            "iter: 1453 , loss: 0.0014498959\n",
            "iter: 1454 , loss: 0.0014495781\n",
            "iter: 1455 , loss: 0.0014492603\n",
            "iter: 1456 , loss: 0.001448943\n",
            "iter: 1457 , loss: 0.0014486255\n",
            "iter: 1458 , loss: 0.0014483086\n",
            "iter: 1459 , loss: 0.0014479918\n",
            "iter: 1460 , loss: 0.0014476754\n",
            "iter: 1461 , loss: 0.0014473591\n",
            "iter: 1462 , loss: 0.0014470433\n",
            "iter: 1463 , loss: 0.0014467274\n",
            "iter: 1464 , loss: 0.0014464122\n",
            "iter: 1465 , loss: 0.0014460973\n",
            "iter: 1466 , loss: 0.0014457825\n",
            "iter: 1467 , loss: 0.001445468\n",
            "iter: 1468 , loss: 0.0014451541\n",
            "iter: 1469 , loss: 0.0014448401\n",
            "iter: 1470 , loss: 0.0014445267\n",
            "iter: 1471 , loss: 0.0014442132\n",
            "iter: 1472 , loss: 0.0014438999\n",
            "iter: 1473 , loss: 0.0014435874\n",
            "iter: 1474 , loss: 0.0014432748\n",
            "iter: 1475 , loss: 0.0014429621\n",
            "iter: 1476 , loss: 0.0014426502\n",
            "iter: 1477 , loss: 0.001442339\n",
            "iter: 1478 , loss: 0.0014420269\n",
            "iter: 1479 , loss: 0.0014417159\n",
            "iter: 1480 , loss: 0.001441405\n",
            "iter: 1481 , loss: 0.0014410942\n",
            "iter: 1482 , loss: 0.0014407835\n",
            "iter: 1483 , loss: 0.001440473\n",
            "iter: 1484 , loss: 0.0014401635\n",
            "iter: 1485 , loss: 0.0014398538\n",
            "iter: 1486 , loss: 0.0014395445\n",
            "iter: 1487 , loss: 0.0014392352\n",
            "iter: 1488 , loss: 0.0014389264\n",
            "iter: 1489 , loss: 0.0014386177\n",
            "iter: 1490 , loss: 0.00143831\n",
            "iter: 1491 , loss: 0.001438002\n",
            "iter: 1492 , loss: 0.0014376941\n",
            "iter: 1493 , loss: 0.0014373867\n",
            "iter: 1494 , loss: 0.0014370794\n",
            "iter: 1495 , loss: 0.0014367723\n",
            "iter: 1496 , loss: 0.0014364658\n",
            "iter: 1497 , loss: 0.0014361589\n",
            "iter: 1498 , loss: 0.0014358527\n",
            "iter: 1499 , loss: 0.0014355468\n",
            "iter: 1500 , loss: 0.0014352413\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EP_aPA6vNvb"
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oX45HVuNSgIK"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "4tSr4Qj6vNyG",
        "outputId": "fb751654-4d21-4d50-81b9-4c623ef6a5df"
      },
      "source": [
        "iter = np.arange(1,len(test_losses)+1)\n",
        "plt.title('Linear Regresion Mean Squerd Error minimization')\n",
        "plt.plot(iter, test_losses, label='Testing loss')\n",
        "plt.plot(iter, train_losses, label='Training loss')\n",
        "plt.legend()\n",
        "plt.xlabel('Number of iterations')\n",
        "plt.ylabel('Mean Squerd Error')\n",
        "plt.show()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cClVf6nHvN15",
        "outputId": "68a481f5-3112-4ca0-bd47-6f4d1cd1abc6"
      },
      "source": [
        "temp = np.mean(np.power(np.subtract(sess.run(predict, feed_dict = {x:data_x_test}), data_y_test),2))\n",
        "temp"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0014352412815531784"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dat1eNqD38l6"
      },
      "source": [
        ""
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jz5-xEvd38pQ"
      },
      "source": [
        ""
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcwH3YPEc9RQ"
      },
      "source": [
        "# https://stackoverflow.com/questions/40994583/how-to-implement-tensorflows-next-batch-for-own-data\n",
        "def next_batch(num, data, labels):\n",
        "    data['y'] = labels\n",
        "    data = data.sample(num)\n",
        "    data_shuffle = data.sample(num)\n",
        "    labels_shuffle = np.array(data['y'], ndmin=2).T\n",
        "    return np.asarray(data_shuffle), labels_shuffle.reshape((data.shape[0], 1))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASRzbReOZBV3"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(X_df, Y_df, test_size=0.3, random_state=1)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJNEFUoZ_UoW"
      },
      "source": [
        "train_size = x_train.shape[0]\n",
        "test_size = x_test.shape[0]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wc7uHRWYKorM"
      },
      "source": [
        "y_train = np.array(y_train,ndmin=2).T\n",
        "y_test = np.array(y_test,ndmin=2).T"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAo33rgzRctw"
      },
      "source": [
        "step_size = 0.01\n",
        "(hidden1_size, hidden2_size, hidden3_size, hidden4_size, hidden5_size) = (1228, 1228, 612, 1, 1 )"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzNP11ydE5ck",
        "outputId": "e2bf1ca5-d7ee-4613-cb64-f37000b0ce51"
      },
      "source": [
        " x_train.shape[0]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1351"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5vzlppZRKzV",
        "outputId": "a576ea16-7650-4a07-f4c2-8018dfde08e4"
      },
      "source": [
        "for _ in [1,2]:\n",
        "  minVal=-0.0001\n",
        "  maxVal=0.0001\n",
        "\n",
        "  x = tf.placeholder(tf.float32, shape=[None, x_train.shape[1]])\n",
        "  y_ = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "\n",
        "  W1 = tf.Variable(tf.random.uniform(shape=[x_train.shape[1], hidden1_size], minval=minVal, maxval=maxVal))\n",
        "  b1 = tf.Variable(tf.random.uniform(shape=[hidden1_size], minval=minVal, maxval=maxVal))\n",
        "  z1 = tf.nn.relu(tf.matmul(x,W1)+b1)\n",
        "\n",
        "  W2 = tf.Variable(tf.random.uniform([hidden1_size, hidden2_size], minval=minVal, maxval=maxVal))\n",
        "  b2 = tf.Variable(tf.random.uniform(shape=[hidden2_size], minval=minVal, maxval=maxVal))\n",
        "  z2 = tf.nn.relu(tf.matmul(z1,W2)+b2)\n",
        "\n",
        "  W3 = tf.Variable(tf.random.uniform([hidden2_size, hidden3_size], minval=minVal, maxval=maxVal))\n",
        "  b3 = tf.Variable(tf.random.uniform(shape=[hidden3_size], minval=minVal, maxval=maxVal))\n",
        "  z3 = tf.nn.relu(tf.matmul(z2,W3)+b3)\n",
        "\n",
        "  W4 = tf.Variable(tf.random.uniform([hidden3_size, hidden4_size], minval=minVal, maxval=maxVal))\n",
        "  b4 = tf.Variable(tf.random.uniform(shape=[hidden4_size], minval=minVal, maxval=maxVal))\n",
        "  z4 = tf.nn.relu(tf.matmul(z3,W4)+b4)\n",
        "\n",
        "  W5 = tf.Variable(tf.random.uniform([hidden4_size, hidden5_size], minval=minVal, maxval=maxVal))\n",
        "  b5 = tf.Variable(tf.random.uniform(shape=[hidden5_size], minval=minVal, maxval=maxVal))\n",
        "  # z5 = tf.nn.relu(tf.matmul(z4,W5)+b5)\n",
        "\n",
        "  # W6 = tf.Variable(tf.random.uniform([hidden5_size, 1], minval=minVal, maxval=maxVal))\n",
        "  # b6 = tf.Variable(tf.random.uniform(shape=[1], minval=minVal, maxval=maxVal))\n",
        "\n",
        "  predict = tf.matmul(z4,W5) + b5\n",
        "  # predict = tf.matmul(z5,W6) + b6\n",
        "\n",
        "  loss = tf.reduce_mean(tf.pow(predict - y_, 2))\n",
        "  train_step = tf.train.GradientDescentOptimizer(step_size).minimize(loss)\n",
        "\n",
        "  init = tf.global_variables_initializer()\n",
        "  sess = tf.Session()\n",
        "  train_losses = []\n",
        "  test_losses = []\n",
        "  batch_xn, batch_yn = next_batch(train_size, x_train, y_train)\n",
        "  batch_xt, batch_yt = next_batch(test_size, x_test, y_test)\n",
        "  sess.run(init)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h78yXovxc9VT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab87a576-27ed-4983-8657-5813489cdd05"
      },
      "source": [
        "for i in range (10):\n",
        "    for _ in range(10):\n",
        "      sess.run(train_step, feed_dict={x: x_train, y_: y_train})\n",
        "    temp_loss = sess.run(loss, feed_dict={x: x_train, y_: y_train})\n",
        "    train_losses.append(temp_loss)\n",
        "    temp_loss = sess.run(loss, feed_dict={x: x_test, y_: y_test})\n",
        "    test_losses.append(temp_loss)\n",
        "    print('iter:', i+1,', loss:', temp_loss)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter: 1 , loss: 0.3014876\n",
            "iter: 2 , loss: 0.29410812\n",
            "iter: 3 , loss: 0.28937268\n",
            "iter: 4 , loss: 0.28636748\n",
            "iter: 5 , loss: 0.28448877\n",
            "iter: 6 , loss: 0.28333881\n",
            "iter: 7 , loss: 0.28265628\n",
            "iter: 8 , loss: 0.28227028\n",
            "iter: 9 , loss: 0.2820694\n",
            "iter: 10 , loss: 0.2819818\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMqmSbKNq3jk"
      },
      "source": [
        "step_size = step_size/30\n",
        "train_step = tf.train.GradientDescentOptimizer(step_size).minimize(loss)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psXg4_b8q6vk",
        "outputId": "4bf443f3-d0c1-4099-e491-9b7425a450db"
      },
      "source": [
        "for i in range (10):\n",
        "    for _ in range(10):\n",
        "      sess.run(train_step, feed_dict={x: batch_xn, y_: batch_yn})\n",
        "    temp_loss = sess.run(loss, feed_dict={x: batch_xn, y_: batch_yn})\n",
        "    train_losses.append(temp_loss)\n",
        "    temp_loss = sess.run(loss, feed_dict={x: batch_xt, y_: batch_yt})\n",
        "    test_losses.append(temp_loss)\n",
        "    print('iter:', i+1,', loss:', temp_loss)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter: 1 , loss: 0.28198025\n",
            "iter: 2 , loss: 0.2819788\n",
            "iter: 3 , loss: 0.28197736\n",
            "iter: 4 , loss: 0.28197607\n",
            "iter: 5 , loss: 0.28197476\n",
            "iter: 6 , loss: 0.28197357\n",
            "iter: 7 , loss: 0.2819724\n",
            "iter: 8 , loss: 0.28197134\n",
            "iter: 9 , loss: 0.2819703\n",
            "iter: 10 , loss: 0.28196934\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRsC5OC-SgLF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "7216e90b-1bb5-4549-b2c1-5f5d8b43e3d5"
      },
      "source": [
        "iter = np.arange(1,len(test_losses)+1)\n",
        "plt.title('NN Mean Squerd Error minimization')\n",
        "plt.plot(iter, test_losses, label='Testing loss')\n",
        "plt.plot(iter, train_losses, label='Training loss')\n",
        "plt.legend()\n",
        "plt.xlabel('Number of iterations')\n",
        "plt.ylabel('Mean Squerd Error')\n",
        "plt.show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0uQG8nzLzN_"
      },
      "source": [
        ""
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HFYlKQKL5OB"
      },
      "source": [
        ""
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pz8Xc5nD_Uye"
      },
      "source": [
        ""
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6DGe1xp_U1V"
      },
      "source": [
        ""
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoXEQ4bs_U4M"
      },
      "source": [
        ""
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWo9rxNu_U8K"
      },
      "source": [
        ""
      ],
      "execution_count": 30,
      "outputs": []
    }
  ]
}